{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando arquivo para NER-NestedClinBr\n",
    "Deixar sempre o arquivo de test para fazer o teste completo (com o pipeline todo)\n",
    "1o. -> passar pelo modelo de NER normal (binario), pegar possiveis entidades\n",
    "2o. -> combinacao de palavras, modelo de span\n",
    "\n",
    "Descontinuas: treinar duplicando as frases, tirando os termos do meio.. na hora da predicao: se duas entidades estão mito proximas (ou no mesmo chunk), tirar o espaço e ver se são (ou tentar achar uma regra, ex sempre q tem tratamento e uma anotmia perto, usar uma janela.. ex duas palavras de distancia)...\n",
    "\n",
    "Para tese: testar tbm sem as descontinuas.. pra pode comparar com outros metodos...\n",
    "\n",
    "v2 - com corpus original Tempclinbr (arq .txt), contendo as quebras de linha.. aqui quebrar cada frase do texto em uma entrada no dicionario.\n",
    "\n",
    "v3 - split de train em 75% para dev.. conforme feito no TempClinBr.. e tbm tirar os 41 primeiros tokens dos textos: \n",
    "'Data de Criação do Documento: 22/04/2014' como codigo Joao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['RAREDISEASE','DISEASE','SYMPTOM','SIGN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('../obj-rare')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj-rare')\n",
    "    with open('../obj-rare/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('../obj-rare')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj-rare')\n",
    "    try:\n",
    "        with open('../obj-rare/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastasCorpus = [r'corpus\\dev',r'corpus\\train']\n",
    "#pastasCorpus = [r'corpus\\test']\n",
    "#pastasCorpus = [r'corpus\\TESTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make the World a 2y4Better Place2y0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'Make the World a 2.4Better Place2.0'\n",
    "pattern = r'([0-9])\\.([0-9])'\n",
    "replacement = r'\\1y\\2'\n",
    "html = re.sub(pattern, replacement, s)\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicSentences(pastaCorpus):\n",
    "    entidadesConsiderar=getTiposEntidade()\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('. ') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                #print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                #print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal=numIndiceAnterior+1\n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                for entidade_linha in linha:\n",
    "                    #print('entidade_linha:', entidade_linha)\n",
    "                    if entidade_linha[0].strip()!='T': # só queremos entidades, relação não\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "                        #print('else, linha[0][0]:', linha[0][0].strip())\n",
    "                        #print('entidade_linha:', entidade_linha)\n",
    "                    if ';' not in entidade_linha: # tem descontinua?\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        if tipo_entidade=='SKINRAREDISEASE':\n",
    "                            tipo_entidade='RAREDISEASE'\n",
    "                        #print(tipo_entidade)\n",
    "                        if tipo_entidade not in entidadesConsiderar:\n",
    "                            continue\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua')\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        if tipo_entidade_string=='SKINRAREDISEASE':\n",
    "                            tipo_entidade_string='RAREDISEASE'\n",
    "                        #print(tipo_entidade)\n",
    "                        if tipo_entidade_string not in entidadesConsiderar:\n",
    "                            continue\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "\n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "\n",
    "\n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print(indicesDic)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('key:', key)\n",
    "                        #print('value:', value)\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        dic_sentences[num]=[value, listaEntidades]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>3:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Treinamento:-- corpus\\train\n",
      "numMaxTokensPorFrase: 94\n",
      "len(sentences): 6321\n",
      "len(Descontinuas): 664\n",
      "len(frasesComDescontinuas): 294\n",
      "--Teste:-- corpus\\dev\n",
      "numMaxTokensPorFrase: 79\n",
      "len(sentences): 897\n",
      "len(Descontinuas): 103\n",
      "len(frasesComDescontinuas): 35\n"
     ]
    }
   ],
   "source": [
    "print('--Treinamento:--', pastasCorpus[1])\n",
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n",
    "print('len(sentences):', len(dic_sentencesTrain))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTrain))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTrain)))\n",
    "\n",
    "\n",
    "print('--Teste:--', pastasCorpus[0])\n",
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n",
    "print('len(sentences):', len(dic_sentencesTest))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTest))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The', 0, 0],\n",
       "  ['congenital', 1, 4],\n",
       "  ['form', 2, 15],\n",
       "  ['of', 3, 20],\n",
       "  ['arodermatitis', 4, 23],\n",
       "  ['enteropathica', 5, 37],\n",
       "  ['is', 6, 51],\n",
       "  ['a', 7, 54],\n",
       "  ['rare', 8, 56],\n",
       "  ['disorder', 9, 61],\n",
       "  ['beginning', 10, 70],\n",
       "  ['during', 11, 80],\n",
       "  ['infancy', 12, 87],\n",
       "  ['.', 13, 94]],\n",
       " [['congenital', [1], 'RAREDISEASE'],\n",
       "  ['arodermatitis enteropathica', [4, 5], 'RAREDISEASE']]]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Agenesis', 0, 0],\n",
       "  ['of', 1, 9],\n",
       "  ['corpus', 2, 12],\n",
       "  ['callosum', 3, 19],\n",
       "  ['(', 4, 28],\n",
       "  ['ACC', 5, 29],\n",
       "  [')', 6, 32],\n",
       "  ['is', 7, 34],\n",
       "  ['a', 8, 37],\n",
       "  ['rare', 9, 39],\n",
       "  ['disorder', 10, 44],\n",
       "  ['that', 11, 53],\n",
       "  ['is', 12, 58],\n",
       "  ['present', 13, 61],\n",
       "  ['at', 14, 69],\n",
       "  ['birth', 15, 72],\n",
       "  ['(', 16, 78],\n",
       "  ['congenital', 17, 79],\n",
       "  [')', 18, 89],\n",
       "  ['.', 19, 90]],\n",
       " [['Agenesis of corpus callosum', [0, 1, 2, 3], 'RAREDISEASE'],\n",
       "  ['ACC', [5], 'RAREDISEASE']]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 12, 52, 58, 58, 58]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasesComDescontinuasTrain[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "# frases com entidades descontinuas\n",
    "print(len(set(frasesComDescontinuasTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['In', 0, 545], ['other', 1, 548], ['instances', 2, 554], [',', 3, 563], ['AN', 4, 565], ['may', 5, 568], ['occur', 6, 572], ['in', 7, 578], ['association', 8, 581], ['with', 9, 593], ['an', 10, 598], ['underlying', 11, 601], ['cancerous', 12, 612], ['tumor', 13, 622], ['(', 14, 628], ['i', 15, 629], ['.', 16, 630], ['e', 17, 631], ['.', 18, 632], [',', 19, 633], ['malignant', 20, 635], ['AN', 21, 645], [')', 22, 647], ['.', 23, 648], ['.', 24, 649]], [['AN', [4], 'DISEASE'], ['malignant AN', [20, 21], 'DISEASE']]]\n"
     ]
    }
   ],
   "source": [
    "print(dic_sentencesTest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhoTrain 4803\n",
      "tamanhoDev 1518\n",
      "len(dic_sentencesTrain): 4803\n",
      "len(dic_sentencesDev): 1517\n",
      "[[['Molecular', 0, 613], ['genetic', 1, 623], ['testing', 2, 631], ['is', 3, 639], ['available', 4, 642], ['for', 5, 652], ['genes', 6, 656], ['associated', 7, 662], ['with', 8, 673], ['EPM1', 9, 678], [',', 10, 682], ['EPM2A', 11, 684], [',', 12, 689], ['and', 13, 691], ['for', 14, 695], ['some', 15, 699], ['of', 16, 704], ['the', 17, 707], ['genes', 18, 711], ['associated', 19, 717], ['with', 20, 728], ['other', 21, 733], ['types', 22, 739], ['of', 23, 745], ['PME', 24, 748], ['.', 25, 751], ['.', 26, 752]], [['PME', [24], 'RAREDISEASE']]]\n",
      "[[['PSP', 0, 0], ['is', 1, 4], ['under', 2, 7], ['-', 3, 12], ['diagnosed', 4, 13], [',', 5, 22], ['so', 6, 24], ['it', 7, 27], ['is', 8, 30], ['difficult', 9, 33], ['to', 10, 43], ['know', 11, 46], ['how', 12, 51], ['many', 13, 55], ['people', 14, 60], ['are', 15, 67], ['affected', 16, 71], ['.', 17, 79]], [['PSP', [0], 'RAREDISEASE']]]\n"
     ]
    }
   ],
   "source": [
    "save_obj('dic_sentencesTrainDev',dic_sentencesTrain)\n",
    "porc=0.76\n",
    "tamanhoTotal = len(dic_sentencesTrain)\n",
    "tamanhoTrain = int(tamanhoTotal*porc)\n",
    "print('tamanhoTrain', tamanhoTrain)\n",
    "tamanhoDev = tamanhoTotal - tamanhoTrain\n",
    "print('tamanhoDev', tamanhoDev)\n",
    "dic_sentencesDev_temp = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[tamanhoTrain:-1]}\n",
    "dic_sentencesTrain = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[:tamanhoTrain]}\n",
    "num=0\n",
    "dic_sentencesDev = {}\n",
    "for key, value in dic_sentencesDev_temp.items():\n",
    "    dic_sentencesDev[num] = value\n",
    "    num=num+1\n",
    "\n",
    "print('len(dic_sentencesTrain):', len(dic_sentencesTrain))\n",
    "print('len(dic_sentencesDev):', len(dic_sentencesDev))\n",
    "print(dic_sentencesTrain[tamanhoTrain-1])\n",
    "print(dic_sentencesDev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['PSP', 0, 0],\n",
       "  ['is', 1, 4],\n",
       "  ['under', 2, 7],\n",
       "  ['-', 3, 12],\n",
       "  ['diagnosed', 4, 13],\n",
       "  [',', 5, 22],\n",
       "  ['so', 6, 24],\n",
       "  ['it', 7, 27],\n",
       "  ['is', 8, 30],\n",
       "  ['difficult', 9, 33],\n",
       "  ['to', 10, 43],\n",
       "  ['know', 11, 46],\n",
       "  ['how', 12, 51],\n",
       "  ['many', 13, 55],\n",
       "  ['people', 14, 60],\n",
       "  ['are', 15, 67],\n",
       "  ['affected', 16, 71],\n",
       "  ['.', 17, 79]],\n",
       " [['PSP', [0], 'RAREDISEASE']]]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['This', 0, 81],\n",
       "  ['disorder', 1, 86],\n",
       "  ['is', 2, 95],\n",
       "  ['believed', 3, 98],\n",
       "  ['to', 4, 107],\n",
       "  ['affect', 5, 110],\n",
       "  ['approximately', 6, 117],\n",
       "  ['20', 7, 131],\n",
       "  [',', 8, 133],\n",
       "  ['000', 9, 134],\n",
       "  ['people', 10, 138],\n",
       "  ['in', 11, 145],\n",
       "  ['the', 12, 148],\n",
       "  ['United', 13, 152],\n",
       "  ['States', 14, 159],\n",
       "  ['.', 15, 165]],\n",
       " []]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravando em  data-ner\n",
      "num_entidade_train 102995\n",
      "num_entidade_dev 33658\n",
      "num_entidade_test: 18729\n",
      "num_entidade_total: 155382\n"
     ]
    }
   ],
   "source": [
    "# gerar arquivo treinamento\n",
    "path='data-ner'\n",
    "f_train = open(path+r'\\nested_train.conll', 'w', encoding='utf-8')\n",
    "print('Gravando em ', path)\n",
    "num_entidade_total=0\n",
    "num_entidade_train=0\n",
    "num_entidade_dev=0\n",
    "num_entidade_test=0\n",
    "\n",
    "\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_train.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_train=num_entidade_train+1\n",
    "    f_train.write('\\n')\n",
    "        \n",
    "f_train.close()\n",
    "\n",
    "f_dev = open(path+r'\\nested_dev.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesDev)):\n",
    "    tokens = dic_sentencesDev[i][0]\n",
    "    ents = dic_sentencesDev[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_dev.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_dev=num_entidade_dev+1\n",
    "    f_dev.write('\\n')\n",
    "f_dev.close()\n",
    "\n",
    "\n",
    "f_test = open(path+r'\\nested_test.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_test.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_test=num_entidade_test+1\n",
    "    f_test.write('\\n')\n",
    "f_test.close()\n",
    "\n",
    "print('num_entidade_train', num_entidade_train)\n",
    "print('num_entidade_dev', num_entidade_dev)\n",
    "print('num_entidade_test:', num_entidade_test)\n",
    "num_entidade_total=num_entidade_train+num_entidade_dev+num_entidade_test\n",
    "print('num_entidade_total:', num_entidade_total)\n",
    "\n",
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The', 0, 0],\n",
       "  ['exact', 1, 4],\n",
       "  ['prevalence', 2, 10],\n",
       "  ['and', 3, 21],\n",
       "  ['incidence', 4, 25],\n",
       "  ['of', 5, 35],\n",
       "  ['abetalipoproteinemia', 6, 38],\n",
       "  ['is', 7, 59],\n",
       "  ['unknown', 8, 62],\n",
       "  [',', 9, 69],\n",
       "  ['but', 10, 71],\n",
       "  ['it', 11, 75],\n",
       "  ['is', 12, 78],\n",
       "  ['estimated', 13, 81],\n",
       "  ['to', 14, 91],\n",
       "  ['affect', 15, 94],\n",
       "  ['less', 16, 101],\n",
       "  ['than', 17, 106],\n",
       "  ['1', 18, 111],\n",
       "  ['in', 19, 113],\n",
       "  ['1', 20, 116],\n",
       "  [',', 21, 117],\n",
       "  ['000', 22, 118],\n",
       "  [',', 23, 121],\n",
       "  ['000', 24, 122],\n",
       "  ['people', 25, 126],\n",
       "  ['in', 26, 133],\n",
       "  ['the', 27, 136],\n",
       "  ['general', 28, 140],\n",
       "  ['population', 29, 148],\n",
       "  ['.', 30, 158]],\n",
       " [['abetalipoproteinemia', [6], 'RAREDISEASE']]]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar modelos binarios para baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosBinarios(entidade, dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'binarios/nested_'+tipo+'_'+entidade+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "    num_entidade=0\n",
    "\n",
    "    # TODO - refazer.. qdo vem entidade isolada, nao está gravando...\n",
    "    print('\\nGravando arquivo de {} para entidade {}'.format(tipo, entidade))\n",
    "\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        indiceEnts=[]\n",
    "        for token in tokens:\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==entidade:\n",
    "                    tag = ent[2]\n",
    "                    num_entidade=num_entidade+1\n",
    "                    break\n",
    "            if tag != entidade:\n",
    "                tag='O'\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            f_entidade.write(tokenGravar+' '+tag+'\\n')\n",
    "            num_entidade_total=num_entidade_total+1\n",
    "        f_entidade.write('\\n')\n",
    "\n",
    "    f_entidade.close()\n",
    "\n",
    "    print('num_entidade:', num_entidade)\n",
    "    print('num_entidade_total:', num_entidade_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo de train para entidade RAREDISEASE\n",
      "num_entidade: 5695\n",
      "num_entidade_total: 102995\n",
      "\n",
      "Gravando arquivo de dev para entidade RAREDISEASE\n",
      "num_entidade: 1818\n",
      "num_entidade_total: 33658\n",
      "\n",
      "Gravando arquivo de test para entidade RAREDISEASE\n",
      "num_entidade: 1142\n",
      "num_entidade_total: 18729\n",
      "\n",
      "Gravando arquivo de train para entidade DISEASE\n",
      "num_entidade: 2068\n",
      "num_entidade_total: 102995\n",
      "\n",
      "Gravando arquivo de dev para entidade DISEASE\n",
      "num_entidade: 696\n",
      "num_entidade_total: 33658\n",
      "\n",
      "Gravando arquivo de test para entidade DISEASE\n",
      "num_entidade: 378\n",
      "num_entidade_total: 18729\n",
      "\n",
      "Gravando arquivo de train para entidade SYMPTOM\n",
      "num_entidade: 479\n",
      "num_entidade_total: 102995\n",
      "\n",
      "Gravando arquivo de dev para entidade SYMPTOM\n",
      "num_entidade: 153\n",
      "num_entidade_total: 33658\n",
      "\n",
      "Gravando arquivo de test para entidade SYMPTOM\n",
      "num_entidade: 45\n",
      "num_entidade_total: 18729\n",
      "\n",
      "Gravando arquivo de train para entidade SIGN\n",
      "num_entidade: 8129\n",
      "num_entidade_total: 102995\n",
      "\n",
      "Gravando arquivo de dev para entidade SIGN\n",
      "num_entidade: 3277\n",
      "num_entidade_total: 33658\n",
      "\n",
      "Gravando arquivo de test para entidade SIGN\n",
      "num_entidade: 1849\n",
      "num_entidade_total: 18729\n"
     ]
    }
   ],
   "source": [
    "entidades = getTiposEntidade()# ['Problema','Teste','Tratamento','Anatomia']\n",
    "#entidades = ['Anatomia']\n",
    "for entidade in entidades:\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesTrain, 'train')\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesDev, 'dev')\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosMultilabel(dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'multilabel/nested_'+tipo+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "\n",
    "    print('\\nGravando arquivo multilabel de {}'.format(tipo))\n",
    "\n",
    "    entidades = getTiposEntidade() #['Problema','Teste','Tratamento','Anatomia']\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        #print('tokens:', tokens)\n",
    "        #print('ents:', ents)\n",
    "        for token in tokens:\n",
    "            listaTokensLabels = list()\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            #print('indiceToken:', indiceToken)\n",
    "            labels = list()\n",
    "            for ent in ents:\n",
    "                #print('ent:', ent)\n",
    "                #print('indiceToken:', indiceToken)\n",
    "                if indiceToken in ent[1]:\n",
    "                    for entidade in entidades:\n",
    "                        tag = ent[2]\n",
    "                        if tag==entidade:\n",
    "                            labels.append(tag)\n",
    "                            num_entidade_total=num_entidade_total+1\n",
    "                        else:\n",
    "                            labels.append('O')\n",
    "            if len(labels)==0:\n",
    "                labels = ['O','O','O','O']\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            f_entidade.write(tokenGravar+'\\t'+'\\t'.join(labels)+'\\n')\n",
    "            #print(tokenGravar+'\\t'+'\\t'.join(labels))\n",
    "\n",
    "        \n",
    "        f_entidade.write('\\n')\n",
    "\n",
    "    f_entidade.close()\n",
    "\n",
    "    print('num_entidade_total:', num_entidade_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo multilabel de train\n",
      "num_entidade_total: 16797\n",
      "\n",
      "Gravando arquivo multilabel de dev\n",
      "num_entidade_total: 6113\n",
      "\n",
      "Gravando arquivo multilabel de test\n",
      "num_entidade_total: 3490\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosMultilabel(dic_sentencesTrain, 'train')\n",
    "gravarArquivosMultilabel(dic_sentencesDev, 'dev')\n",
    "gravarArquivosMultilabel(dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte 2- Gerar arquivo treinamento para SpanClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areConsecutive(arr):\n",
    "    # Sort the array\n",
    "    arr.sort()\n",
    "    n = len(arr)\n",
    "    # checking the adjacent elements\n",
    "    for i in range (1,n):\n",
    "        if(arr[i]!=arr[i-1]+1):\n",
    "            return False;\n",
    "             \n",
    "    return True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTest = load_obj('dic_sentencesTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidades(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sem taxa de Downsampling')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        #termo = frase[indice:i+2]\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #print('--termo--:', termo)\n",
    "                        frase.insert(indice, '<e1>')\n",
    "                        frase.insert(i+2, '</e1>')\n",
    "                        frase_string=' '.join(frase).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '. </e1>' in frase_string or ', </e1>' in frase_string  or '; </e1>' in frase_string or '- </e1>' in frase_string  or ': </e1>' in frase_string  or '= </e1>' in frase_string  or '/ </e1>' in frase_string  or '( </e1>' in frase_string  or ') </e1>' in frase_string  or '[ </e1>' in frase_string  or '] </e1>' in frase_string  or ': </e1>' in frase_string or 'and </e1>' in frase_string or 'or </e1>' in frase_string:\n",
    "                            devePular=1\n",
    "                        if '<e1> .' in frase_string or '<e1> ,' in frase_string  or '<e1> ;' in frase_string or '<e1> -' in frase_string  or '<e1> :' in frase_string  or '<e1> =' in frase_string  or '<e1> /' in frase_string  or '<e1> (' in frase_string  or '<e1> )' in frase_string  or '<e1> [' in frase_string  or '<e1> ]' in frase_string  or '<e1> :' in frase_string  or '<e1> and' in frase_string  or '<e1> or' in frase_string:\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> [0-9]* </e1>\", frase_string):\n",
    "                            devePular=1\n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase in combinacaoEntidades:\n",
    "                            if frase[0] == frase_string:\n",
    "                                tem_frase=''\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                            combinacaoEntidades.append([frase_string, 'O'])\n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidades = combinacaoEntidades[:(num_positivas*taxaDownsampling)+num_positivas]\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    print('len(pulando_termos_postagger):', len(pulando_termos_postagger))\n",
    "    \n",
    "    return combinacaoEntidadesAll, pulando_termos_postagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels sempre tem que começar com zero, senao da erro no treinamento\n",
    "# RuntimeError: CUDA error: device-side assert triggered\n",
    "def getCombinacaoEntidadesSoPositivos(dic_predictions):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            #print('tipo_entidade:', tipo_entidade)\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    \n",
    "    return combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Só positivos\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidadesAll:) 4803\n",
      "\n",
      "--Dev--\n",
      "Só positivos\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidadesAll:) 1517\n",
      "\n",
      "--Test--\n",
      "Só positivos\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidadesAll:) 897\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainPos= getCombinacaoEntidadesSoPositivos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevPos = getCombinacaoEntidadesSoPositivos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestPos = getCombinacaoEntidadesSoPositivos(dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       " [['Various benign ( non - cancerous ) forms of <e1> AN </e1> have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       "   'DISEASE'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , <e1> drug - induced AN </e1> ) .',\n",
       "   'DISEASE'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( <e1> obesity </e1> ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       "   'DISEASE']]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTestPos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidadesAll:) 4803\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Dev--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidadesAll:) 1517\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Test--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidadesAll:) 897\n",
      "len(pulando_termos_postagger): 0\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, _ = getCombinacaoEntidades(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, _ = getCombinacaoEntidades(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, _ = getCombinacaoEntidades(dic_sentencesTest, False, '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       " [['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - induced <e1> AN </e1> ) .',\n",
       "   'O'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - <e1> induced </e1> AN ) .',\n",
       "   'O'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , <e1> drug - induced </e1> AN ) .',\n",
       "   'O'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , <e1> drug - induced AN </e1> ) .',\n",
       "   'DISEASE'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - <e1> induced AN </e1> ) .',\n",
       "   'O'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( <e1> obesity </e1> ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       "   'DISEASE'],\n",
       "  ['Various benign ( non - cancerous ) forms of <e1> AN </e1> have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , drug - induced AN ) .',\n",
       "   'DISEASE'],\n",
       "  ['Various benign ( non - cancerous ) forms of AN have been identified in which the disorder may be inherited as a primary condition or associated with various underlying syndromes , an excess accumulation of body fat ( obesity ) , or the use of certain medications ( i . e . , <e1> drug </e1> - induced AN ) .',\n",
       "   'O']]]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['People', 0, 673],\n",
       "  ['with', 1, 680],\n",
       "  ['Brown', 2, 685],\n",
       "  ['Syndrome', 3, 691],\n",
       "  ['have', 4, 700],\n",
       "  ['limited', 5, 705],\n",
       "  ['eye', 6, 713],\n",
       "  ['movement', 7, 717],\n",
       "  ['in', 8, 726],\n",
       "  ['the', 9, 729],\n",
       "  ['affected', 10, 733],\n",
       "  ['eye', 11, 742],\n",
       "  ['.', 12, 745]],\n",
       " [['Brown Syndrome', [2, 3], 'RAREDISEASE'],\n",
       "  ['limited eye movement in the affected eye',\n",
       "   [5, 6, 7, 8, 9, 10, 11],\n",
       "   'SIGN']]]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o erro é que o ponto faz com que quebre a frase, mas a entidade continua...\n",
    "dic_sentencesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       " [['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects <e1> the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete </e1> absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis </e1> ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of </e1> an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects <e1> the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of </e1> an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of </e1> an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence </e1> ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the <e1> two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that <e1> connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the two <e1> cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete </e1> absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that <e1> connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial </e1> or complete absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the <e1> two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'SIGN'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of </e1> an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the two <e1> cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of </e1> an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an </e1> area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis </e1> ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that <e1> connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of the brain that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area of </e1> the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the <e1> brain that connects the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that connects the two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that connects the two cerebral hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area <e1> of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects <e1> the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that <e1> connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis </e1> ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the two cerebral <e1> hemispheres </e1> .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain <e1> that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of <e1> the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the </e1> brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence </e1> ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an <e1> area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain </e1> that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence </e1> ( agenesis ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) <e1> of an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis </e1> ) of an area of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects the <e1> two </e1> cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area </e1> of the brain that connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of <e1> an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that <e1> connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( agenesis ) of an area of the brain that connects <e1> the </e1> two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a <e1> partial or complete absence ( agenesis ) of an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete <e1> absence ( agenesis ) of an area of the brain that </e1> connects the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that connects the two cerebral </e1> hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or complete absence ( <e1> agenesis ) of an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O'],\n",
       "  ['It is characterized by a partial or <e1> complete absence ( agenesis ) of an area of the brain that connects </e1> the two cerebral hemispheres .',\n",
       "   'O']]]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People with Brown Syndrome have limited eye movement in the affected eye .',\n",
       " [['People with Brown Syndrome have <e1> limited eye movement in the </e1> affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited eye movement in the affected eye </e1> .',\n",
       "   'SIGN'],\n",
       "  ['People with Brown Syndrome have limited eye movement <e1> in </e1> the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye <e1> movement in the </e1> affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited eye movement in the affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in <e1> the </e1> affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye </e1> movement in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited </e1> eye movement in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in <e1> the affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in the <e1> affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in the <e1> affected eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in the affected <e1> eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement <e1> in the </e1> affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye movement in the affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited eye </e1> movement in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye movement in the affected eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye <e1> movement </e1> in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with <e1> Brown Syndrome </e1> have limited eye movement in the affected eye .',\n",
       "   'RAREDISEASE'],\n",
       "  ['People with <e1> Brown </e1> Syndrome have limited eye movement in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye <e1> movement in the affected eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement <e1> in the affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye <e1> movement in </e1> the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement <e1> in the affected eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited eye movement in </e1> the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye movement in </e1> the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye <e1> movement in the affected </e1> eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye movement </e1> in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited eye movement in <e1> the affected eye </e1> .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have <e1> limited eye movement </e1> in the affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown Syndrome have limited <e1> eye movement in the </e1> affected eye .',\n",
       "   'O'],\n",
       "  ['People with Brown <e1> Syndrome </e1> have limited eye movement in the affected eye .',\n",
       "   'O']]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PSP is under - diagnosed , so it is difficult to know how many people are affected .',\n",
       " [['<e1> PSP </e1> is under - diagnosed , so it is difficult to know how many people are affected .',\n",
       "   'RAREDISEASE']]]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamento(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\span.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_train.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_train.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\span.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_dev.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_dev.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\span.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_test.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_test.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 56035\n",
      "numTotalEntidadesTrain: 34724\n",
      "numTotalEntidadesDev: 13237\n",
      "numTotalEntidadesTest: 8074\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('sem_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 10484\n",
      "numTotalEntidadesTrain: 6788\n",
      "numTotalEntidadesDev: 2398\n",
      "numTotalEntidadesTest: 1298\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('so_positivos',combinacaoEntidadesTrainPos, combinacaoEntidadesDevPos, combinacaoEntidadesTestPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    #labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    labels = {'O':0, 'RAREDISEASE':1, 'DISEASE':2, 'SYMPTOM':3, 'SIGN':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Sentence Pairs - Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "    random.shuffle(combinacaoEntidades)\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='12mg'>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"^[0-9]*mg\", '12mg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    #labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    labels = {'O':0, 'RAREDISEASE':1, 'DISEASE':2, 'SYMPTOM':3, 'SIGN':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidadesNeg = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Sentence Pairs - Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Sentence Pairs - Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem taxa de Downsampling')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "        # agora, os negativos\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #termo=entidade[0].strip()\n",
    "                        #print('--termo--:', termo)\n",
    "                        #frase.insert(indice, '<e1>')\n",
    "                        #frase.insert(i+2, '</e1>')\n",
    "                        #frase_string=' '.join(frase).strip()\n",
    "                        #frase_string=texto_entidade\n",
    "                        #frase_string=termo[0]\n",
    "                        frase_string=' '.join(termo).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '.' in frase_string[-1:] or ',' in frase_string[-1:]  or ';' in frase_string[-1:] or '-' in frase_string[-1:]  or ':' in frase_string[-1:]  or '=' in frase_string[-1:]  or '/' in frase_string[-1:]  or '(' in frase_string[-1:]  or ')' in frase_string[-1:]  or '[' in frase_string[-1:]  or ']' in frase_string[-1:]  or ':' in frase_string[-1:]:\n",
    "                            devePular=1\n",
    "                        if '.' in frase_string[:1] or ',' in frase_string[:1]  or ';' in frase_string[:1] or '-' in frase_string[:1]  or ':' in frase_string[:1]  or '=' in frase_string[:1] or '/' in frase_string[:1]  or '(' in frase_string[:1]  or ')' in frase_string[:1] or '[' in frase_string[:1]  or ']' in frase_string[:1]  or ':' in frase_string[:1]:\n",
    "                            devePular=1\n",
    "                        if re.search(\"^[0-9]*mg\", frase_string):\n",
    "                            devePular=1\n",
    "                            \n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase_l in combinacaoEntidadesPos:\n",
    "                            if frase_l[0] == frase_string:\n",
    "                                tem_frase='1'\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                        #print('tem_frase:', tem_frase)\n",
    "                        #if tem_frase==0:\n",
    "                            #print('aaaaaaaaaaaa, frase_string:', frase_string)\n",
    "                            combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        #combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidadesNeg = combinacaoEntidadesNeg[:(num_positivas*taxaDownsampling)]\n",
    "            random.shuffle(combinacaoEntidadesNeg)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos+combinacaoEntidadesNeg\n",
    "        combinacaoEntidadesPos=list()\n",
    "        combinacaoEntidadesNeg=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentencePos(dic_predictions):\n",
    "    #labels = {'Problema':0, 'Tratamento':1, 'Teste':2, 'Anatomia':3}\n",
    "    labels = {'O':0, 'RAREDISEASE':1, 'DISEASE':2, 'SYMPTOM':3, 'SIGN':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    print('Sentence Pairs - So positivos')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            random.shuffle(combinacaoEntidadesPos)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos\n",
    "        combinacaoEntidadesPos=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidades:) 37260\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidades:) 14360\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidades:) 8781\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, False, '', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The', 0, 0],\n",
       "  ['congenital', 1, 4],\n",
       "  ['form', 2, 15],\n",
       "  ['of', 3, 20],\n",
       "  ['arodermatitis', 4, 23],\n",
       "  ['enteropathica', 5, 37],\n",
       "  ['is', 6, 51],\n",
       "  ['a', 7, 54],\n",
       "  ['rare', 8, 56],\n",
       "  ['disorder', 9, 61],\n",
       "  ['beginning', 10, 70],\n",
       "  ['during', 11, 80],\n",
       "  ['infancy', 12, 87],\n",
       "  ['.', 13, 94]],\n",
       " [['congenital', [1], 'RAREDISEASE'],\n",
       "  ['arodermatitis enteropathica', [4, 5], 'RAREDISEASE']]]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamentoSentence(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\sentence_pairs.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_train.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\sentence_pairs.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_dev.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\sentence_pairs.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_test.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 60401\n",
      "numTotalEntidadesTrain: 37260\n",
      "numTotalEntidadesDev: 14360\n",
      "numTotalEntidadesTest: 8781\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamentoSentence('sentence-pairs-all',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - So positivos\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidades:) 6788\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - So positivos\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidades:) 2398\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - So positivos\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidades:) 1298\n",
      "numTotalEntidades: 10484\n",
      "numTotalEntidadesTrain: 6788\n",
      "numTotalEntidadesDev: 2398\n",
      "numTotalEntidadesTest: 1298\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentencePos= getCombinacaoEntidadesSentencePos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-positivos',combinacaoEntidadesTrainSentencePos, combinacaoEntidadesDevSentencePos, combinacaoEntidadesTestSentencePos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10247"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"pucpr-br/postagger-bio-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('pucpr-br/postagger-bio-english')\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='max')\n",
    "#doc = nlp_token_class(['These differences in gene expression have not been molecularly defined '])\n",
    "#print(doc)\n",
    "\n",
    "def getDicPosTagger(dic_sentencesTrainDev):\n",
    "    dicPostagger = load_obj('dic_postagger')\n",
    "    allFrases = load_obj('allFrases')\n",
    "    if dicPostagger==None or allFrases==None:\n",
    "        dicPostagger = {}\n",
    "        allFrases=[]\n",
    "        for key, value in dic_sentencesTrainDev.items():\n",
    "            tokens=value[0]\n",
    "            frase = [t[0] for t in tokens]\n",
    "            frase = ' '.join(frase)\n",
    "            allFrases.append(frase)\n",
    "            #print(frase)\n",
    "\n",
    "        #print(allFrases)\n",
    "        doc = nlp_token_class(allFrases)\n",
    "        #print(doc)\n",
    "        for frase in doc:\n",
    "            for d in frase:\n",
    "                #print(d)\n",
    "                pos = d['entity_group']\n",
    "                #print(pos)\n",
    "                token=d['word']\n",
    "                if 'WP' in pos: \n",
    "                    pos='WP'\n",
    "                if 'PRP' in pos: \n",
    "                    pos='PRP'\n",
    "                if 'VB' in pos: \n",
    "                    pos='VB'\n",
    "                if 'RB' in pos: \n",
    "                    pos='RB'\n",
    "                if 'JJ' in pos: \n",
    "                    pos='JJ'\n",
    "                if 'NN' in pos: \n",
    "                    pos='NN'\n",
    "                dicPostagger[token] = pos    \n",
    "\n",
    "        save_obj('dicPostagger', dicPostagger)\n",
    "        save_obj('allFrases', allFrases)\n",
    "    return dicPostagger, allFrases\n",
    "\n",
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')\n",
    "\n",
    "dicPosTagger, _ = getDicPosTagger(dic_sentencesTrainDev)\n",
    "#dicPosTagger, _ = getDicPosTagger(dic_sentencesTest)\n",
    "len(dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPosTagger['gene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-N-VB-NN-NN-RB-N'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tipoPostaggerTokens(entidade_token, dicPostagger):\n",
    "    postagger = ''\n",
    "    for p in entidade_token:\n",
    "        #print('p:', p)\n",
    "        if p.lower() in dicPostagger.keys():\n",
    "            postagger = postagger + '-' + dicPostagger.get(p.lower())\n",
    "        else:\n",
    "            #print('nao tem:', p)\n",
    "            # se nao tem, considera N\n",
    "            postagger = postagger + '-' + 'N'\n",
    "    return postagger\n",
    "tipoPostaggerTokens(['hello','is','gene','expression','yet', '?'], dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-NN',\n",
       " '-NN-PUNCT-N-NN',\n",
       " '-VB-NN',\n",
       " '-JJ-NN-VB-DT-NN-CC-JJ-PUNCT-JJ-PUNCT-NN',\n",
       " '-JJ-NN-VB',\n",
       " '-DT-NN',\n",
       " '-NN-IN-DT-NN',\n",
       " '-JJ-NN',\n",
       " '-NN-IN-NN-NN',\n",
       " '-JJ']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger):\n",
    "    lista_postaggers_entidades = []\n",
    "    for key, value in dic_sentencesTrainDev.items():\n",
    "        entidades = value[1]\n",
    "        for entidade in entidades:\n",
    "            #print(entidade[0])\n",
    "            pos_tagger=tipoPostaggerTokens(entidade[0].split(), dicPosTagger)\n",
    "            if pos_tagger not in lista_postaggers_entidades:\n",
    "                lista_postaggers_entidades.append(pos_tagger)\n",
    "    return lista_postaggers_entidades\n",
    "\n",
    "lista_postaggers_entidades = getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger)\n",
    "lista_postaggers_entidades[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidades:) 12395\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidades:) 4396\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidades:) 2397\n",
      "numTotalEntidades: 19188\n",
      "numTotalEntidadesTrain: 12395\n",
      "numTotalEntidadesDev: 4396\n",
      "numTotalEntidadesTest: 2397\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-downsampling',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidades:) 28998\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidades:) 10621\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidades:) 5972\n",
      "numTotalEntidades: 45591\n",
      "numTotalEntidadesTrain: 28998\n",
      "numTotalEntidadesDev: 10621\n",
      "numTotalEntidadesTest: 5972\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 0)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-filtro',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTestSentence[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parte 2.2 - tentar tirar verbos, CC, etc do O para nao ficar com mto FP\n",
    "### depois, com nosso pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-NN-CC-NN-TO-VB-DT-NN',\n",
       " '-NN-IN-DT-NN-IN-NN-CC-NN-NN',\n",
       " '-JJ-NN-CC-NN-IN-CD',\n",
       " '-NN-CC-NN-IN-JJ-N-NN-NN',\n",
       " '-N-CC-N-N-IN-DT-NN-IN-DT-NN-NN-PUNCT-NN-PUNCT-IN-VB-TO-VB-N',\n",
       " '-JJ-NN-IN-NN-IN-DT-JJ-NN',\n",
       " '-JJ-NN-IN-VB-RB-VB-JJ',\n",
       " '-NN-IN-NN-IN-DT-JJ-NN-IN-NN',\n",
       " '-NN-PUNCT-NN-PUNCT-JJ']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_postaggers_entidades[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('-KC' in lista_postaggers_entidades) # e\n",
    "print('-PREP+ART' in lista_postaggers_entidades) \n",
    "print('-PREP' in lista_postaggers_entidades)\n",
    "print('-ART' in lista_postaggers_entidades)\n",
    "print('-V' in lista_postaggers_entidades)\n",
    "print('-ADJ' in lista_postaggers_entidades)\n",
    "print('-PU' in lista_postaggers_entidades)\n",
    "print('-PCP' in lista_postaggers_entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidadesAll:) 4803\n",
      "len(pulando_termos_postagger): 11375\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidadesAll:) 1517\n",
      "len(pulando_termos_postagger): 5009\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidadesAll:) 897\n",
      "len(pulando_termos_postagger): 3672\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agenesis of corpus callosum can occur in conjunction with spina bifida .',\n",
       " [['<e1> Agenesis of corpus callosum </e1> can occur in conjunction with spina bifida .',\n",
       "   'RAREDISEASE'],\n",
       "  ['Agenesis of <e1> corpus callosum </e1> can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['Agenesis of corpus callosum can occur in conjunction with spina <e1> bifida </e1> .',\n",
       "   'O'],\n",
       "  ['Agenesis of corpus callosum can occur in conjunction with <e1> spina </e1> bifida .',\n",
       "   'O'],\n",
       "  ['Agenesis <e1> of corpus callosum </e1> can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['Agenesis of corpus callosum can occur in conjunction with <e1> spina bifida </e1> .',\n",
       "   'DISEASE'],\n",
       "  ['Agenesis <e1> of </e1> corpus callosum can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['<e1> Agenesis </e1> of corpus callosum can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['Agenesis of corpus <e1> callosum </e1> can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['<e1> Agenesis of </e1> corpus callosum can occur in conjunction with spina bifida .',\n",
       "   'O'],\n",
       "  ['Agenesis of <e1> corpus </e1> callosum can occur in conjunction with spina bifida .',\n",
       "   'O']]]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['drug', '-', 'induced'], '-NN-PUNCT-JJ'],\n",
       " [['drug', '-', 'induced', 'AN'], '-NN-PUNCT-JJ-DT'],\n",
       " [['-', 'induced'], '-PUNCT-JJ'],\n",
       " [['-', 'induced', 'AN'], '-PUNCT-JJ-DT'],\n",
       " [['induced', 'AN'], '-JJ-DT'],\n",
       " [['malignant', 'AN'], '-JJ-DT'],\n",
       " [['Agenesis', 'of', 'corpus'], '-NN-IN-N'],\n",
       " [['of', 'corpus'], '-IN-N'],\n",
       " [['partial', 'or'], '-JJ-CC'],\n",
       " [['partial', 'or', 'complete', 'absence', '('], '-JJ-CC-JJ-NN-PUNCT'],\n",
       " [['partial', 'or', 'complete', 'absence', '(', 'agenesis'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN'],\n",
       " [['partial', 'or', 'complete', 'absence', '(', 'agenesis', ')'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT'],\n",
       " [['partial', 'or', 'complete', 'absence', '(', 'agenesis', ')', 'of'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN'],\n",
       " [['partial', 'or', 'complete', 'absence', '(', 'agenesis', ')', 'of', 'an'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two',\n",
       "   'cerebral'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD-JJ'],\n",
       " [['partial',\n",
       "   'or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two',\n",
       "   'cerebral',\n",
       "   'hemispheres'],\n",
       "  '-JJ-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD-JJ-NN'],\n",
       " [['or', 'complete'], '-CC-JJ'],\n",
       " [['or', 'complete', 'absence', '('], '-CC-JJ-NN-PUNCT'],\n",
       " [['or', 'complete', 'absence', '(', 'agenesis'], '-CC-JJ-NN-PUNCT-NN'],\n",
       " [['or', 'complete', 'absence', '(', 'agenesis', ')'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT'],\n",
       " [['or', 'complete', 'absence', '(', 'agenesis', ')', 'of'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN'],\n",
       " [['or', 'complete', 'absence', '(', 'agenesis', ')', 'of', 'an'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT'],\n",
       " [['or', 'complete', 'absence', '(', 'agenesis', ')', 'of', 'an', 'area'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two',\n",
       "   'cerebral'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD-JJ'],\n",
       " [['or',\n",
       "   'complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the',\n",
       "   'two',\n",
       "   'cerebral',\n",
       "   'hemispheres'],\n",
       "  '-CC-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT-CD-JJ-NN'],\n",
       " [['complete', 'absence', '(', 'agenesis', ')'], '-JJ-NN-PUNCT-NN-PUNCT'],\n",
       " [['complete', 'absence', '(', 'agenesis', ')', 'of'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN'],\n",
       " [['complete', 'absence', '(', 'agenesis', ')', 'of', 'an'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT'],\n",
       " [['complete', 'absence', '(', 'agenesis', ')', 'of', 'an', 'area'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN'],\n",
       " [['complete', 'absence', '(', 'agenesis', ')', 'of', 'an', 'area', 'of'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN'],\n",
       " [['complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT'],\n",
       " [['complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN'],\n",
       " [['complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN'],\n",
       " [['complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB'],\n",
       " [['complete',\n",
       "   'absence',\n",
       "   '(',\n",
       "   'agenesis',\n",
       "   ')',\n",
       "   'of',\n",
       "   'an',\n",
       "   'area',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brain',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'the'],\n",
       "  '-JJ-NN-PUNCT-NN-PUNCT-IN-DT-NN-IN-DT-NN-IN-VB-DT']]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTest[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['-', 'Macrostomia', 'Syndrome'], '-PUNCT-N-NN'],\n",
       " [['physical', 'abnormalities', 'affecting', 'the'], '-JJ-NN-VB-DT'],\n",
       " [['physical', 'abnormalities', 'affecting', 'the', 'head', 'and'],\n",
       "  '-JJ-NN-VB-DT-NN-CC'],\n",
       " [['physical', 'abnormalities', 'affecting', 'the', 'head', 'and', 'facial'],\n",
       "  '-JJ-NN-VB-DT-NN-CC-JJ'],\n",
       " [['physical',\n",
       "   'abnormalities',\n",
       "   'affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '('],\n",
       "  '-JJ-NN-VB-DT-NN-CC-JJ-PUNCT'],\n",
       " [['physical',\n",
       "   'abnormalities',\n",
       "   'affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '(',\n",
       "   'craniofacial'],\n",
       "  '-JJ-NN-VB-DT-NN-CC-JJ-PUNCT-JJ'],\n",
       " [['physical',\n",
       "   'abnormalities',\n",
       "   'affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '(',\n",
       "   'craniofacial',\n",
       "   ')'],\n",
       "  '-JJ-NN-VB-DT-NN-CC-JJ-PUNCT-JJ-PUNCT'],\n",
       " [['abnormalities', 'affecting', 'the', 'head', 'and'], '-NN-VB-DT-NN-CC'],\n",
       " [['abnormalities', 'affecting', 'the', 'head', 'and', 'facial'],\n",
       "  '-NN-VB-DT-NN-CC-JJ'],\n",
       " [['abnormalities', 'affecting', 'the', 'head', 'and', 'facial', '('],\n",
       "  '-NN-VB-DT-NN-CC-JJ-PUNCT'],\n",
       " [['abnormalities',\n",
       "   'affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '(',\n",
       "   'craniofacial'],\n",
       "  '-NN-VB-DT-NN-CC-JJ-PUNCT-JJ'],\n",
       " [['abnormalities',\n",
       "   'affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '(',\n",
       "   'craniofacial',\n",
       "   ')'],\n",
       "  '-NN-VB-DT-NN-CC-JJ-PUNCT-JJ-PUNCT'],\n",
       " [['affecting', 'the'], '-VB-DT'],\n",
       " [['affecting', 'the', 'head', 'and'], '-VB-DT-NN-CC'],\n",
       " [['affecting', 'the', 'head', 'and', 'facial'], '-VB-DT-NN-CC-JJ'],\n",
       " [['affecting', 'the', 'head', 'and', 'facial', '('], '-VB-DT-NN-CC-JJ-PUNCT'],\n",
       " [['affecting', 'the', 'head', 'and', 'facial', '(', 'craniofacial'],\n",
       "  '-VB-DT-NN-CC-JJ-PUNCT-JJ'],\n",
       " [['affecting', 'the', 'head', 'and', 'facial', '(', 'craniofacial', ')'],\n",
       "  '-VB-DT-NN-CC-JJ-PUNCT-JJ-PUNCT'],\n",
       " [['affecting',\n",
       "   'the',\n",
       "   'head',\n",
       "   'and',\n",
       "   'facial',\n",
       "   '(',\n",
       "   'craniofacial',\n",
       "   ')',\n",
       "   'area'],\n",
       "  '-VB-DT-NN-CC-JJ-PUNCT-JJ-PUNCT-NN'],\n",
       " [['the', 'head', 'and'], '-DT-NN-CC']]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTrain[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 43556\n",
      "numTotalEntidadesTrain: 27773\n",
      "numTotalEntidadesDev: 10096\n",
      "numTotalEntidadesTest: 5687\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('com_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro, key: 12\n",
      "erro, key: 63\n",
      "erro, key: 75\n",
      "erro, key: 107\n",
      "erro, key: 152\n",
      "erro, key: 152\n",
      "erro, key: 215\n",
      "erro, key: 364\n",
      "erro, key: 418\n",
      "erro, key: 420\n",
      "erro, key: 524\n",
      "erro, key: 631\n",
      "erro, key: 745\n",
      "erro, key: 784\n",
      "erro, key: 844\n",
      "erro, key: 953\n",
      "key: 999\n",
      "erro, key: 1064\n",
      "erro, key: 1157\n",
      "erro, key: 1226\n",
      "erro, key: 1245\n",
      "erro, key: 1271\n",
      "erro, key: 1537\n",
      "erro, key: 1697\n",
      "erro, key: 1777\n",
      "erro, key: 1961\n",
      "key: 1999\n",
      "erro, key: 2011\n",
      "erro, key: 2033\n",
      "erro, key: 2059\n",
      "erro, key: 2150\n",
      "erro, key: 2185\n",
      "erro, key: 2440\n",
      "erro, key: 2441\n",
      "erro, key: 2504\n",
      "erro, key: 2536\n",
      "erro, key: 2672\n",
      "erro, key: 2686\n",
      "erro, key: 2693\n",
      "erro, key: 2751\n",
      "erro, key: 2812\n",
      "erro, key: 2903\n",
      "erro, key: 2930\n",
      "key: 2999\n",
      "erro, key: 3108\n",
      "erro, key: 3126\n",
      "erro, key: 3161\n",
      "erro, key: 3375\n",
      "erro, key: 3410\n",
      "erro, key: 3410\n",
      "erro, key: 3440\n",
      "erro, key: 3454\n",
      "erro, key: 3533\n",
      "erro, key: 3536\n",
      "erro, key: 3622\n",
      "erro, key: 3634\n",
      "erro, key: 3716\n",
      "erro, key: 3719\n",
      "erro, key: 3875\n",
      "erro, key: 3877\n",
      "erro, key: 3897\n",
      "erro, key: 3908\n",
      "erro, key: 3954\n",
      "key: 3999\n",
      "erro, key: 4110\n",
      "erro, key: 4131\n",
      "erro, key: 4234\n",
      "erro, key: 4324\n",
      "erro, key: 4473\n",
      "erro, key: 4515\n",
      "erro, key: 4613\n",
      "erro_corpus: 67\n",
      "num_frases_sem_entidade: 1589\n",
      "len(combinacaoEntidadesAll:) 4803\n",
      "len(pulando_termos_postagger): 11375\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro, key: 37\n",
      "erro, key: 79\n",
      "erro, key: 104\n",
      "erro, key: 172\n",
      "erro, key: 255\n",
      "erro, key: 255\n",
      "erro, key: 275\n",
      "erro, key: 357\n",
      "erro, key: 358\n",
      "erro, key: 361\n",
      "erro, key: 381\n",
      "erro, key: 418\n",
      "erro, key: 419\n",
      "erro, key: 419\n",
      "erro, key: 425\n",
      "erro, key: 459\n",
      "erro, key: 488\n",
      "erro, key: 499\n",
      "erro, key: 771\n",
      "erro, key: 817\n",
      "erro, key: 843\n",
      "erro, key: 909\n",
      "erro, key: 956\n",
      "key: 999\n",
      "erro, key: 1069\n",
      "erro, key: 1092\n",
      "erro, key: 1116\n",
      "erro, key: 1173\n",
      "erro, key: 1241\n",
      "erro, key: 1290\n",
      "erro, key: 1301\n",
      "erro, key: 1365\n",
      "erro, key: 1481\n",
      "erro_corpus: 32\n",
      "num_frases_sem_entidade: 472\n",
      "len(combinacaoEntidadesAll:) 1517\n",
      "len(pulando_termos_postagger): 5009\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro, key: 69\n",
      "erro, key: 170\n",
      "erro, key: 304\n",
      "erro, key: 304\n",
      "erro, key: 316\n",
      "erro, key: 332\n",
      "erro, key: 511\n",
      "erro, key: 608\n",
      "erro, key: 609\n",
      "erro, key: 613\n",
      "erro, key: 613\n",
      "erro, key: 669\n",
      "erro, key: 676\n",
      "erro, key: 712\n",
      "erro, key: 722\n",
      "erro, key: 723\n",
      "erro, key: 726\n",
      "erro, key: 741\n",
      "erro, key: 787\n",
      "erro, key: 887\n",
      "erro_corpus: 20\n",
      "num_frases_sem_entidade: 285\n",
      "len(combinacaoEntidadesAll:) 897\n",
      "len(pulando_termos_postagger): 3672\n",
      "numTotalEntidades: 19197\n",
      "numTotalEntidadesTrain: 12391\n",
      "numTotalEntidadesDev: 4407\n",
      "numTotalEntidadesTest: 2399\n"
     ]
    }
   ],
   "source": [
    "# com downsampling\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "\n",
    "gravarArquivosTreinamento('com-filtro-downsampling',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, tratar descontinuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
