{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando arquivo para NER-QA-NestedClinBr\n",
    "Deixar sempre o arquivo de test para fazer o teste completo (com o pipeline todo)\n",
    "1o. -> passar pelo modelo de NER normal (binario), pegar possiveis entidades\n",
    "2o. -> combinacao de palavras, modelo de span\n",
    "\n",
    "Descontinuas: treinar duplicando as frases, tirando os termos do meio.. na hora da predicao: se duas entidades estão mito proximas (ou no mesmo chunk), tirar o espaço e ver se são (ou tentar achar uma regra, ex sempre q tem tratamento e uma anotmia perto, usar uma janela.. ex duas palavras de distancia)...\n",
    "\n",
    "Para tese: testar tbm sem as descontinuas.. pra pode comparar com outros metodos...\n",
    "\n",
    "v2 - com corpus original Tempclinbr (arq .txt), contendo as quebras de linha.. aqui quebrar cada frase do texto em uma entrada no dicionario.\n",
    "\n",
    "v3 - split de train em 75% para dev.. conforme feito no TempClinBr.. e tbm tirar os 41 primeiros tokens dos textos: \n",
    "'Data de Criação do Documento: 22/04/2014' como codigo Joao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    with open('../obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    try:\n",
    "        with open('../obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastasCorpus = [r'corpus\\test',r'corpus\\train']\n",
    "#pastasCorpus = [r'corpus\\test']\n",
    "#pastasCorpus = [r'corpus\\TESTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make the World a 2y4Better Place2y0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'Make the World a 2.4Better Place2.0'\n",
    "pattern = r'([0-9])\\.([0-9])'\n",
    "replacement = r'\\1y\\2'\n",
    "html = re.sub(pattern, replacement, s)\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicSentences(pastaCorpus):\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if numL==1: # descarta primeira frase, que é data de criação do doc\n",
    "                        #print('descartando frase:', l)\n",
    "                        continue\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('.') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                #print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                #print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal=numIndiceAnterior+1\n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                for entidade_linha in linha:\n",
    "                    if ';' not in entidade_linha: # tem descontinua?\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua, linha: {}, num: {}'.format(linha, num))\n",
    "                        #print('descontinua, file: {}, num: {}'.format(filename, num))\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        #print('aaaaaaaaaaaaaaaaa')\n",
    "                        dicEntidades[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        \n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "\n",
    "\n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print(indicesDic)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('key:', key)\n",
    "                        #print('value:', value)\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                                #print(\"[termos_entidade, novos_indices, tipo_entidade]:\", [termos_entidade, novos_indices, tipo_entidade])\n",
    "                            else:\n",
    "                                #print('else, i[0]:',i[0])\n",
    "                                pass\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        dic_sentences[num]=[value, listaEntidades]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>3:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 146\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0, 43],\n",
       "  ['em', 1, 52],\n",
       "  ['tratamento', 2, 55],\n",
       "  ['para', 3, 66],\n",
       "  ['ICC', 4, 71],\n",
       "  ['diastólica', 5, 75],\n",
       "  ['há', 6, 86],\n",
       "  ['cerca', 7, 89],\n",
       "  ['de', 8, 95],\n",
       "  ['5', 9, 98],\n",
       "  ['a', 10, 100],\n",
       "  ['com', 11, 102],\n",
       "  ['melhora', 12, 106],\n",
       "  ['importante', 13, 114],\n",
       "  ['dos', 14, 125],\n",
       "  ['sintomas', 15, 129],\n",
       "  ['após', 16, 138],\n",
       "  ['início', 17, 143],\n",
       "  ['do', 18, 150],\n",
       "  ['tratamento', 19, 153],\n",
       "  ['.', 20, 163]],\n",
       " [['tratamento', [2], 'Tratamento'],\n",
       "  ['ICC diastólica', [4, 5], 'Problema'],\n",
       "  ['melhora importante dos sintomas após início do tratamento',\n",
       "   [12, 13, 14, 15, 16, 17, 18, 19],\n",
       "   'Problema'],\n",
       "  ['tratamento', [19], 'Tratamento']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 192\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Treinamento:-- corpus\\train\n",
      "numMaxTokensPorFrase: 192\n",
      "len(sentences): 1736\n",
      "len(Descontinuas): 91\n",
      "len(frasesComDescontinuas): 51\n",
      "--Teste:-- corpus\\test\n",
      "numMaxTokensPorFrase: 146\n",
      "len(sentences): 506\n",
      "len(Descontinuas): 44\n",
      "len(frasesComDescontinuas): 17\n"
     ]
    }
   ],
   "source": [
    "print('--Treinamento:--', pastasCorpus[1])\n",
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n",
    "print('len(sentences):', len(dic_sentencesTrain))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTrain))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTrain)))\n",
    "\n",
    "\n",
    "print('--Teste:--', pastasCorpus[0])\n",
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n",
    "print('len(sentences):', len(dic_sentencesTest))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTest))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Abd', 0, 644],\n",
       "  ['globoso', 1, 648],\n",
       "  [',', 2, 655],\n",
       "  ['flacido', 3, 657],\n",
       "  [',', 4, 664],\n",
       "  ['indolor', 5, 666],\n",
       "  ['a', 6, 674],\n",
       "  ['palpacao', 7, 676],\n",
       "  [',', 8, 684],\n",
       "  ['sem', 9, 686],\n",
       "  ['VCM', 10, 690],\n",
       "  ['.', 11, 693]],\n",
       " [['Abd', [0], 'Anatomia'], ['VCM', [10], 'Problema']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['CONTRAÇÃO', 0, 507],\n",
       "  ['SEGMENTAR', 1, 517],\n",
       "  ['DO', 2, 527],\n",
       "  ['VE', 3, 530],\n",
       "  ['ALTERADA', 4, 533],\n",
       "  ['.', 5, 541]],\n",
       " [['CONTRAÇÃO SEGMENTAR DO VE ALTERADA', [0, 1, 2, 3, 4], 'Problema'],\n",
       "  ['VE', [3], 'Anatomia']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 15, 30, 30, 30, 30, 55, 55, 65, 65]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasesComDescontinuasTrain[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# frases com entidades descontinuas\n",
    "print(len(set(frasesComDescontinuasTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 49,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 91,\n",
       " 131,\n",
       " 147,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 227,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 263,\n",
       " 291,\n",
       " 291,\n",
       " 291,\n",
       " 316,\n",
       " 329,\n",
       " 367,\n",
       " 367,\n",
       " 367,\n",
       " 367,\n",
       " 406,\n",
       " 468]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasesComDescontinuasTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Comorbidades', 0, 142], [':', 1, 154], ['DM', 2, 156], ['há', 3, 159], ['10', 4, 162], ['anos', 5, 165], ['em', 6, 170], ['uso', 7, 173], ['de', 8, 177], ['metformina', 9, 180], ['850mg', 10, 191], ['3', 11, 197], ['cp', 12, 199], ['/', 13, 201], ['dia', 14, 202], [',', 15, 205], ['acarbose', 16, 207], ['1', 17, 216], ['cp', 18, 218], ['/', 19, 220], ['dia', 20, 221], ['e', 21, 225], ['glicazida', 22, 227], ['60mg', 23, 237], ['2', 24, 242], ['cp', 25, 244], ['/', 26, 246], ['dia', 27, 247], ['e', 28, 251], ['insulina', 29, 253], ['(', 30, 262], ['24', 31, 263], ['-', 32, 266], ['0', 33, 268], ['-', 34, 270], ['24', 35, 272], [')', 36, 274], ['.', 37, 275]], [['Comorbidades', [0], 'Problema'], ['DM', [2], 'Problema'], ['metformina 850mg', [9, 10], 'Tratamento'], ['acarbose', [16], 'Tratamento'], ['glicazida 60mg', [22, 23], 'Tratamento'], ['insulina', [29], 'Tratamento']]]\n"
     ]
    }
   ],
   "source": [
    "print(dic_sentencesTest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhoTrain 1319\n",
      "tamanhoDev 417\n",
      "len(dic_sentencesTrain): 1319\n",
      "len(dic_sentencesDev): 416\n",
      "[[['Paciente', 0, 151], ['relata', 1, 160], ['apenas', 2, 167], ['um', 3, 174], ['episodio', 4, 177], ['no', 5, 186], ['momento', 6, 189], ['de', 7, 197], ['gripe', 8, 200], ['.', 9, 205]], [['gripe', [8], 'Problema']]]\n",
      "[[['HAS', 0, 207], [',', 1, 210], ['ICC', 2, 212], [',', 3, 215], ['nega', 4, 217], ['DM', 5, 222], ['.', 6, 224]], [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]\n"
     ]
    }
   ],
   "source": [
    "#save_obj('dic_sentencesTrainDev',dic_sentencesTrain)\n",
    "porc=0.76\n",
    "tamanhoTotal = len(dic_sentencesTrain)\n",
    "tamanhoTrain = int(tamanhoTotal*porc)\n",
    "print('tamanhoTrain', tamanhoTrain)\n",
    "tamanhoDev = tamanhoTotal - tamanhoTrain\n",
    "print('tamanhoDev', tamanhoDev)\n",
    "dic_sentencesDev_temp = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[tamanhoTrain:-1]}\n",
    "dic_sentencesTrain = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[:tamanhoTrain]}\n",
    "num=0\n",
    "dic_sentencesDev = {}\n",
    "for key, value in dic_sentencesDev_temp.items():\n",
    "    dic_sentencesDev[num] = value\n",
    "    num=num+1\n",
    "\n",
    "print('len(dic_sentencesTrain):', len(dic_sentencesTrain))\n",
    "print('len(dic_sentencesDev):', len(dic_sentencesDev))\n",
    "print(dic_sentencesTrain[tamanhoTrain-1])\n",
    "print(dic_sentencesDev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0, 207],\n",
       "  [',', 1, 210],\n",
       "  ['ICC', 2, 212],\n",
       "  [',', 3, 215],\n",
       "  ['nega', 4, 217],\n",
       "  ['DM', 5, 222],\n",
       "  ['.', 6, 224]],\n",
       " [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['carvedilol', 0, 226],\n",
       "  ['1', 1, 237],\n",
       "  ['cp', 2, 239],\n",
       "  ['12', 3, 242],\n",
       "  ['/', 4, 244],\n",
       "  ['12', 5, 245],\n",
       "  [',', 6, 247],\n",
       "  ['furosemida', 7, 249],\n",
       "  ['20mg', 8, 260],\n",
       "  ['2', 9, 265],\n",
       "  ['cp', 10, 267],\n",
       "  ['de', 11, 270],\n",
       "  ['12', 12, 273],\n",
       "  ['/', 13, 275],\n",
       "  ['12', 14, 276],\n",
       "  [',', 15, 278],\n",
       "  ['sinvastatina', 16, 280],\n",
       "  ['1cp', 17, 293],\n",
       "  ['a', 18, 297],\n",
       "  ['noite', 19, 299],\n",
       "  [',', 20, 304],\n",
       "  ['AAS', 21, 306],\n",
       "  ['100mg', 22, 310],\n",
       "  ['apos', 23, 316],\n",
       "  ['almoço', 24, 321],\n",
       "  ['e', 25, 328],\n",
       "  ['Omeprazol', 26, 330],\n",
       "  ['20mg', 27, 340],\n",
       "  ['1', 28, 345],\n",
       "  ['xdia', 29, 347],\n",
       "  ['.', 30, 351]],\n",
       " [['carvedilol', [0], 'Tratamento'],\n",
       "  ['furosemida 20mg', [7, 8], 'Tratamento'],\n",
       "  ['sinvastatina', [16], 'Tratamento'],\n",
       "  ['AAS 100mg', [21, 22], 'Tratamento'],\n",
       "  ['Omeprazol 20mg', [26, 27], 'Tratamento']]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 0, 43],\n",
       "  ['importante', 1, 52],\n",
       "  ['aos', 2, 63],\n",
       "  ['esforços', 3, 67],\n",
       "  ['+', 4, 76],\n",
       "  ['dor', 5, 78],\n",
       "  ['tipo', 6, 82],\n",
       "  ['peso', 7, 87],\n",
       "  ['no', 8, 92],\n",
       "  ['peito', 9, 95],\n",
       "  ['no', 10, 101],\n",
       "  ['esforço', 11, 104],\n",
       "  ['.', 12, 111]],\n",
       " [['Dispneia importante aos esforços', [0, 1, 2, 3], 'Problema'],\n",
       "  ['dor tipo peso no peito no esforço', [5, 6, 7, 8, 9, 10, 11], 'Problema'],\n",
       "  ['peito', [9], 'Anatomia']]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipoEntidade: Problema\n",
      "token: ['Dispneia', 0, 43]\n",
      "Problema\n",
      "token: ['importante', 1, 52]\n",
      "Problema\n",
      "token: ['aos', 2, 63]\n",
      "Problema\n",
      "token: ['esforços', 3, 67]\n",
      "Problema\n",
      "token: ['+', 4, 76]\n",
      "O\n",
      "token: ['dor', 5, 78]\n",
      "Problema\n",
      "token: ['tipo', 6, 82]\n",
      "Problema\n",
      "token: ['peso', 7, 87]\n",
      "Problema\n",
      "token: ['no', 8, 92]\n",
      "Problema\n",
      "token: ['peito', 9, 95]\n",
      "Problema\n",
      "token: ['no', 10, 101]\n",
      "Problema\n",
      "token: ['esforço', 11, 104]\n",
      "Problema\n",
      "token: ['.', 12, 111]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['Dispneia', 0, 43]\n",
      "O\n",
      "token: ['importante', 1, 52]\n",
      "O\n",
      "token: ['aos', 2, 63]\n",
      "O\n",
      "token: ['esforços', 3, 67]\n",
      "O\n",
      "token: ['+', 4, 76]\n",
      "O\n",
      "token: ['dor', 5, 78]\n",
      "O\n",
      "token: ['tipo', 6, 82]\n",
      "O\n",
      "token: ['peso', 7, 87]\n",
      "O\n",
      "token: ['no', 8, 92]\n",
      "O\n",
      "token: ['peito', 9, 95]\n",
      "O\n",
      "token: ['no', 10, 101]\n",
      "O\n",
      "token: ['esforço', 11, 104]\n",
      "O\n",
      "token: ['.', 12, 111]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['Dispneia', 0, 43]\n",
      "O\n",
      "token: ['importante', 1, 52]\n",
      "O\n",
      "token: ['aos', 2, 63]\n",
      "O\n",
      "token: ['esforços', 3, 67]\n",
      "O\n",
      "token: ['+', 4, 76]\n",
      "O\n",
      "token: ['dor', 5, 78]\n",
      "O\n",
      "token: ['tipo', 6, 82]\n",
      "O\n",
      "token: ['peso', 7, 87]\n",
      "O\n",
      "token: ['no', 8, 92]\n",
      "O\n",
      "token: ['peito', 9, 95]\n",
      "O\n",
      "token: ['no', 10, 101]\n",
      "O\n",
      "token: ['esforço', 11, 104]\n",
      "O\n",
      "token: ['.', 12, 111]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['Dispneia', 0, 43]\n",
      "O\n",
      "token: ['importante', 1, 52]\n",
      "O\n",
      "token: ['aos', 2, 63]\n",
      "O\n",
      "token: ['esforços', 3, 67]\n",
      "O\n",
      "token: ['+', 4, 76]\n",
      "O\n",
      "token: ['dor', 5, 78]\n",
      "O\n",
      "token: ['tipo', 6, 82]\n",
      "O\n",
      "token: ['peso', 7, 87]\n",
      "O\n",
      "token: ['no', 8, 92]\n",
      "O\n",
      "token: ['peito', 9, 95]\n",
      "Anatomia\n",
      "token: ['no', 10, 101]\n",
      "O\n",
      "token: ['esforço', 11, 104]\n",
      "O\n",
      "token: ['.', 12, 111]\n",
      "O\n",
      "tipoEntidade: Problema\n",
      "token: ['Obeso', 0, 113]\n",
      "Problema\n",
      "token: [',', 1, 118]\n",
      "O\n",
      "token: ['has', 2, 120]\n",
      "Problema\n",
      "token: [',', 3, 123]\n",
      "O\n",
      "token: ['icc', 4, 125]\n",
      "Problema\n",
      "token: ['.', 5, 128]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['Obeso', 0, 113]\n",
      "O\n",
      "token: [',', 1, 118]\n",
      "O\n",
      "token: ['has', 2, 120]\n",
      "O\n",
      "token: [',', 3, 123]\n",
      "O\n",
      "token: ['icc', 4, 125]\n",
      "O\n",
      "token: ['.', 5, 128]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['Obeso', 0, 113]\n",
      "O\n",
      "token: [',', 1, 118]\n",
      "O\n",
      "token: ['has', 2, 120]\n",
      "O\n",
      "token: [',', 3, 123]\n",
      "O\n",
      "token: ['icc', 4, 125]\n",
      "O\n",
      "token: ['.', 5, 128]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['Obeso', 0, 113]\n",
      "O\n",
      "token: [',', 1, 118]\n",
      "O\n",
      "token: ['has', 2, 120]\n",
      "O\n",
      "token: [',', 3, 123]\n",
      "O\n",
      "token: ['icc', 4, 125]\n",
      "O\n",
      "token: ['.', 5, 128]\n",
      "O\n",
      "tipoEntidade: Problema\n",
      "token: ['c', 0, 130]\n",
      "O\n",
      "token: ['#', 1, 132]\n",
      "O\n",
      "token: ['cintilografia', 2, 134]\n",
      "O\n",
      "token: ['miocardica', 3, 148]\n",
      "O\n",
      "token: ['para', 4, 159]\n",
      "O\n",
      "token: ['avaliar', 5, 164]\n",
      "O\n",
      "token: ['angina', 6, 172]\n",
      "Problema\n",
      "token: ['.', 7, 178]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['c', 0, 130]\n",
      "O\n",
      "token: ['#', 1, 132]\n",
      "O\n",
      "token: ['cintilografia', 2, 134]\n",
      "Teste\n",
      "token: ['miocardica', 3, 148]\n",
      "Teste\n",
      "token: ['para', 4, 159]\n",
      "O\n",
      "token: ['avaliar', 5, 164]\n",
      "Teste\n",
      "token: ['angina', 6, 172]\n",
      "O\n",
      "token: ['.', 7, 178]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['c', 0, 130]\n",
      "O\n",
      "token: ['#', 1, 132]\n",
      "O\n",
      "token: ['cintilografia', 2, 134]\n",
      "O\n",
      "token: ['miocardica', 3, 148]\n",
      "O\n",
      "token: ['para', 4, 159]\n",
      "O\n",
      "token: ['avaliar', 5, 164]\n",
      "O\n",
      "token: ['angina', 6, 172]\n",
      "O\n",
      "token: ['.', 7, 178]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['c', 0, 130]\n",
      "O\n",
      "token: ['#', 1, 132]\n",
      "O\n",
      "token: ['cintilografia', 2, 134]\n",
      "O\n",
      "token: ['miocardica', 3, 148]\n",
      "Anatomia\n",
      "token: ['para', 4, 159]\n",
      "O\n",
      "token: ['avaliar', 5, 164]\n",
      "O\n",
      "token: ['angina', 6, 172]\n",
      "O\n",
      "token: ['.', 7, 178]\n",
      "O\n",
      "tipoEntidade: Problema\n",
      "token: ['Plastia', 0, 43]\n",
      "O\n",
      "token: ['Mitral', 1, 51]\n",
      "O\n",
      "token: ['(', 2, 58]\n",
      "O\n",
      "token: ['Insuficiencia', 3, 60]\n",
      "Problema\n",
      "token: [')', 4, 74]\n",
      "O\n",
      "token: [',', 5, 75]\n",
      "O\n",
      "token: ['CRM', 6, 77]\n",
      "O\n",
      "token: ['Saf', 7, 81]\n",
      "O\n",
      "token: ['-', 8, 84]\n",
      "O\n",
      "token: ['2Mg', 9, 85]\n",
      "O\n",
      "token: ['e', 10, 89]\n",
      "O\n",
      "token: ['e', 11, 89]\n",
      "O\n",
      "token: ['Saf', 12, 93]\n",
      "O\n",
      "token: ['-', 13, 96]\n",
      "O\n",
      "token: ['3MG', 14, 97]\n",
      "O\n",
      "token: [')', 15, 101]\n",
      "O\n",
      "token: ['.', 16, 102]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['Plastia', 0, 43]\n",
      "O\n",
      "token: ['Mitral', 1, 51]\n",
      "O\n",
      "token: ['(', 2, 58]\n",
      "O\n",
      "token: ['Insuficiencia', 3, 60]\n",
      "O\n",
      "token: [')', 4, 74]\n",
      "O\n",
      "token: [',', 5, 75]\n",
      "O\n",
      "token: ['CRM', 6, 77]\n",
      "O\n",
      "token: ['Saf', 7, 81]\n",
      "O\n",
      "token: ['-', 8, 84]\n",
      "O\n",
      "token: ['2Mg', 9, 85]\n",
      "O\n",
      "token: ['e', 10, 89]\n",
      "O\n",
      "token: ['e', 11, 89]\n",
      "O\n",
      "token: ['Saf', 12, 93]\n",
      "O\n",
      "token: ['-', 13, 96]\n",
      "O\n",
      "token: ['3MG', 14, 97]\n",
      "O\n",
      "token: [')', 15, 101]\n",
      "O\n",
      "token: ['.', 16, 102]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['Plastia', 0, 43]\n",
      "Tratamento\n",
      "token: ['Mitral', 1, 51]\n",
      "Tratamento\n",
      "token: ['(', 2, 58]\n",
      "O\n",
      "token: ['Insuficiencia', 3, 60]\n",
      "O\n",
      "token: [')', 4, 74]\n",
      "O\n",
      "token: [',', 5, 75]\n",
      "O\n",
      "token: ['CRM', 6, 77]\n",
      "Tratamento\n",
      "token: ['Saf', 7, 81]\n",
      "O\n",
      "token: ['-', 8, 84]\n",
      "O\n",
      "token: ['2Mg', 9, 85]\n",
      "O\n",
      "token: ['e', 10, 89]\n",
      "O\n",
      "token: ['e', 11, 89]\n",
      "O\n",
      "token: ['Saf', 12, 93]\n",
      "O\n",
      "token: ['-', 13, 96]\n",
      "O\n",
      "token: ['3MG', 14, 97]\n",
      "O\n",
      "token: [')', 15, 101]\n",
      "O\n",
      "token: ['.', 16, 102]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['Plastia', 0, 43]\n",
      "O\n",
      "token: ['Mitral', 1, 51]\n",
      "Anatomia\n",
      "token: ['(', 2, 58]\n",
      "O\n",
      "token: ['Insuficiencia', 3, 60]\n",
      "O\n",
      "token: [')', 4, 74]\n",
      "O\n",
      "token: [',', 5, 75]\n",
      "O\n",
      "token: ['CRM', 6, 77]\n",
      "O\n",
      "token: ['Saf', 7, 81]\n",
      "O\n",
      "token: ['-', 8, 84]\n",
      "O\n",
      "token: ['2Mg', 9, 85]\n",
      "O\n",
      "token: ['e', 10, 89]\n",
      "O\n",
      "token: ['e', 11, 89]\n",
      "O\n",
      "token: ['Saf', 12, 93]\n",
      "O\n",
      "token: ['-', 13, 96]\n",
      "O\n",
      "token: ['3MG', 14, 97]\n",
      "O\n",
      "token: [')', 15, 101]\n",
      "O\n",
      "token: ['.', 16, 102]\n",
      "O\n",
      "tipoEntidade: Problema\n",
      "token: ['(', 0, 103]\n",
      "O\n",
      "token: ['09', 1, 104]\n",
      "O\n",
      "token: ['/', 2, 106]\n",
      "O\n",
      "token: ['03', 3, 107]\n",
      "O\n",
      "token: ['/', 4, 109]\n",
      "O\n",
      "token: ['16', 5, 110]\n",
      "O\n",
      "token: [')', 6, 112]\n",
      "O\n",
      "token: ['.', 7, 113]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['(', 0, 103]\n",
      "O\n",
      "token: ['09', 1, 104]\n",
      "O\n",
      "token: ['/', 2, 106]\n",
      "O\n",
      "token: ['03', 3, 107]\n",
      "O\n",
      "token: ['/', 4, 109]\n",
      "O\n",
      "token: ['16', 5, 110]\n",
      "O\n",
      "token: [')', 6, 112]\n",
      "O\n",
      "token: ['.', 7, 113]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['(', 0, 103]\n",
      "O\n",
      "token: ['09', 1, 104]\n",
      "O\n",
      "token: ['/', 2, 106]\n",
      "O\n",
      "token: ['03', 3, 107]\n",
      "O\n",
      "token: ['/', 4, 109]\n",
      "O\n",
      "token: ['16', 5, 110]\n",
      "O\n",
      "token: [')', 6, 112]\n",
      "O\n",
      "token: ['.', 7, 113]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['(', 0, 103]\n",
      "O\n",
      "token: ['09', 1, 104]\n",
      "O\n",
      "token: ['/', 2, 106]\n",
      "O\n",
      "token: ['03', 3, 107]\n",
      "O\n",
      "token: ['/', 4, 109]\n",
      "O\n",
      "token: ['16', 5, 110]\n",
      "O\n",
      "token: [')', 6, 112]\n",
      "O\n",
      "token: ['.', 7, 113]\n",
      "O\n",
      "tipoEntidade: Problema\n",
      "token: ['Uso', 0, 115]\n",
      "O\n",
      "token: [':', 1, 118]\n",
      "O\n",
      "token: ['AAS', 2, 120]\n",
      "O\n",
      "token: ['100', 3, 124]\n",
      "O\n",
      "token: ['-', 4, 129]\n",
      "O\n",
      "token: ['1xd', 5, 130]\n",
      "O\n",
      "token: [';', 6, 133]\n",
      "O\n",
      "token: ['Metoprolol', 7, 135]\n",
      "O\n",
      "token: ['25', 8, 147]\n",
      "O\n",
      "token: ['-', 9, 150]\n",
      "O\n",
      "token: ['1xd', 10, 151]\n",
      "O\n",
      "token: [';', 11, 154]\n",
      "O\n",
      "token: ['FSM', 12, 156]\n",
      "O\n",
      "token: ['-', 13, 160]\n",
      "O\n",
      "token: ['1xd', 14, 161]\n",
      "O\n",
      "token: [';', 15, 165]\n",
      "O\n",
      "token: ['Levotiroxina', 16, 167]\n",
      "O\n",
      "token: ['175', 17, 180]\n",
      "O\n",
      "token: ['-', 18, 185]\n",
      "O\n",
      "token: ['1xd', 19, 186]\n",
      "O\n",
      "token: [';', 20, 189]\n",
      "O\n",
      "token: ['Sinva', 21, 191]\n",
      "O\n",
      "token: ['40', 22, 197]\n",
      "O\n",
      "token: ['-', 23, 200]\n",
      "O\n",
      "token: ['1xd', 24, 201]\n",
      "O\n",
      "token: [';', 25, 204]\n",
      "O\n",
      "token: ['Fluoxetina', 26, 206]\n",
      "O\n",
      "token: ['20', 27, 217]\n",
      "O\n",
      "token: ['-', 28, 219]\n",
      "O\n",
      "token: ['1xd', 29, 220]\n",
      "O\n",
      "token: ['.', 30, 223]\n",
      "O\n",
      "tipoEntidade: Teste\n",
      "token: ['Uso', 0, 115]\n",
      "O\n",
      "token: [':', 1, 118]\n",
      "O\n",
      "token: ['AAS', 2, 120]\n",
      "O\n",
      "token: ['100', 3, 124]\n",
      "O\n",
      "token: ['-', 4, 129]\n",
      "O\n",
      "token: ['1xd', 5, 130]\n",
      "O\n",
      "token: [';', 6, 133]\n",
      "O\n",
      "token: ['Metoprolol', 7, 135]\n",
      "O\n",
      "token: ['25', 8, 147]\n",
      "O\n",
      "token: ['-', 9, 150]\n",
      "O\n",
      "token: ['1xd', 10, 151]\n",
      "O\n",
      "token: [';', 11, 154]\n",
      "O\n",
      "token: ['FSM', 12, 156]\n",
      "O\n",
      "token: ['-', 13, 160]\n",
      "O\n",
      "token: ['1xd', 14, 161]\n",
      "O\n",
      "token: [';', 15, 165]\n",
      "O\n",
      "token: ['Levotiroxina', 16, 167]\n",
      "O\n",
      "token: ['175', 17, 180]\n",
      "O\n",
      "token: ['-', 18, 185]\n",
      "O\n",
      "token: ['1xd', 19, 186]\n",
      "O\n",
      "token: [';', 20, 189]\n",
      "O\n",
      "token: ['Sinva', 21, 191]\n",
      "O\n",
      "token: ['40', 22, 197]\n",
      "O\n",
      "token: ['-', 23, 200]\n",
      "O\n",
      "token: ['1xd', 24, 201]\n",
      "O\n",
      "token: [';', 25, 204]\n",
      "O\n",
      "token: ['Fluoxetina', 26, 206]\n",
      "O\n",
      "token: ['20', 27, 217]\n",
      "O\n",
      "token: ['-', 28, 219]\n",
      "O\n",
      "token: ['1xd', 29, 220]\n",
      "O\n",
      "token: ['.', 30, 223]\n",
      "O\n",
      "tipoEntidade: Tratamento\n",
      "token: ['Uso', 0, 115]\n",
      "O\n",
      "token: [':', 1, 118]\n",
      "O\n",
      "token: ['AAS', 2, 120]\n",
      "Tratamento\n",
      "token: ['100', 3, 124]\n",
      "Tratamento\n",
      "token: ['-', 4, 129]\n",
      "O\n",
      "token: ['1xd', 5, 130]\n",
      "O\n",
      "token: [';', 6, 133]\n",
      "O\n",
      "token: ['Metoprolol', 7, 135]\n",
      "Tratamento\n",
      "token: ['25', 8, 147]\n",
      "Tratamento\n",
      "token: ['-', 9, 150]\n",
      "O\n",
      "token: ['1xd', 10, 151]\n",
      "O\n",
      "token: [';', 11, 154]\n",
      "O\n",
      "token: ['FSM', 12, 156]\n",
      "Tratamento\n",
      "token: ['-', 13, 160]\n",
      "O\n",
      "token: ['1xd', 14, 161]\n",
      "O\n",
      "token: [';', 15, 165]\n",
      "O\n",
      "token: ['Levotiroxina', 16, 167]\n",
      "Tratamento\n",
      "token: ['175', 17, 180]\n",
      "Tratamento\n",
      "token: ['-', 18, 185]\n",
      "O\n",
      "token: ['1xd', 19, 186]\n",
      "O\n",
      "token: [';', 20, 189]\n",
      "O\n",
      "token: ['Sinva', 21, 191]\n",
      "Tratamento\n",
      "token: ['40', 22, 197]\n",
      "Tratamento\n",
      "token: ['-', 23, 200]\n",
      "O\n",
      "token: ['1xd', 24, 201]\n",
      "O\n",
      "token: [';', 25, 204]\n",
      "O\n",
      "token: ['Fluoxetina', 26, 206]\n",
      "Tratamento\n",
      "token: ['20', 27, 217]\n",
      "Tratamento\n",
      "token: ['-', 28, 219]\n",
      "O\n",
      "token: ['1xd', 29, 220]\n",
      "O\n",
      "token: ['.', 30, 223]\n",
      "O\n",
      "tipoEntidade: Anatomia\n",
      "token: ['Uso', 0, 115]\n",
      "O\n",
      "token: [':', 1, 118]\n",
      "O\n",
      "token: ['AAS', 2, 120]\n",
      "O\n",
      "token: ['100', 3, 124]\n",
      "O\n",
      "token: ['-', 4, 129]\n",
      "O\n",
      "token: ['1xd', 5, 130]\n",
      "O\n",
      "token: [';', 6, 133]\n",
      "O\n",
      "token: ['Metoprolol', 7, 135]\n",
      "O\n",
      "token: ['25', 8, 147]\n",
      "O\n",
      "token: ['-', 9, 150]\n",
      "O\n",
      "token: ['1xd', 10, 151]\n",
      "O\n",
      "token: [';', 11, 154]\n",
      "O\n",
      "token: ['FSM', 12, 156]\n",
      "O\n",
      "token: ['-', 13, 160]\n",
      "O\n",
      "token: ['1xd', 14, 161]\n",
      "O\n",
      "token: [';', 15, 165]\n",
      "O\n",
      "token: ['Levotiroxina', 16, 167]\n",
      "O\n",
      "token: ['175', 17, 180]\n",
      "O\n",
      "token: ['-', 18, 185]\n",
      "O\n",
      "token: ['1xd', 19, 186]\n",
      "O\n",
      "token: [';', 20, 189]\n",
      "O\n",
      "token: ['Sinva', 21, 191]\n",
      "O\n",
      "token: ['40', 22, 197]\n",
      "O\n",
      "token: ['-', 23, 200]\n",
      "O\n",
      "token: ['1xd', 24, 201]\n",
      "O\n",
      "token: [';', 25, 204]\n",
      "O\n",
      "token: ['Fluoxetina', 26, 206]\n",
      "O\n",
      "token: ['20', 27, 217]\n",
      "O\n",
      "token: ['-', 28, 219]\n",
      "O\n",
      "token: ['1xd', 29, 220]\n",
      "O\n",
      "token: ['.', 30, 223]\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "listaEntidades=getTiposEntidade()\n",
    "\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    if i>5:\n",
    "        break\n",
    "    for tipoEntidade in listaEntidades:\n",
    "        print('tipoEntidade:', tipoEntidade)\n",
    "        indiceEnts=[]\n",
    "        for token in tokens:\n",
    "            print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==tipoEntidade:\n",
    "                    tag = ent[2]\n",
    "                    break\n",
    "            print(tag)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravando em  data_ner_qa\n",
      "num_entidade_train 55204\n",
      "num_entidade_dev 16036\n",
      "num_entidade_test: 21812\n",
      "num_entidade_total: 93052\n"
     ]
    }
   ],
   "source": [
    "# gerar arquivo treinamento\n",
    "path='data_ner_qa'\n",
    "f_train = open(path+r'\\nested_ner_qa_train.conll', 'w', encoding='utf-8')\n",
    "listaEntidades=getTiposEntidade()\n",
    "\n",
    "print('Gravando em ', path)\n",
    "num_entidade_total=0\n",
    "num_entidade_train=0\n",
    "num_entidade_dev=0\n",
    "num_entidade_test=0\n",
    "\n",
    "\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    for tipoEntidade in listaEntidades:\n",
    "        indiceEnts=[]\n",
    "        #f_train.write(tipoEntidade+' O\\n[SEP]\\n')\n",
    "        f_train.write(tipoEntidade+' <sep>\\n')\n",
    "        for token in tokens:\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==tipoEntidade:\n",
    "                    tag = ent[2]\n",
    "                    break\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            if tag!=tipoEntidade:\n",
    "                tag='O'\n",
    "            else:\n",
    "                tag='ENT'\n",
    "            f_train.write(tokenGravar+' '+tag+'\\n')\n",
    "            num_entidade_train=num_entidade_train+1\n",
    "        f_train.write('\\n')        \n",
    "f_train.close()\n",
    "\n",
    "f_dev = open(path+r'\\nested_ner_qa_dev.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesDev)):\n",
    "    tokens = dic_sentencesDev[i][0]\n",
    "    ents = dic_sentencesDev[i][1]\n",
    "    for tipoEntidade in listaEntidades:\n",
    "        indiceEnts=[]\n",
    "        f_dev.write(tipoEntidade+' <sep>\\n')\n",
    "        for token in tokens:\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==tipoEntidade:\n",
    "                    tag = ent[2]\n",
    "                    break\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            if tag!=tipoEntidade:\n",
    "                tag='O'\n",
    "            else:\n",
    "                tag='ENT'\n",
    "            f_dev.write(tokenGravar+' '+tag+'\\n')\n",
    "            num_entidade_dev=num_entidade_dev+1\n",
    "        f_dev.write('\\n')\n",
    "f_dev.close()\n",
    "\n",
    "\n",
    "f_test = open(path+r'\\nested_ner_qa_test.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for tipoEntidade in listaEntidades:\n",
    "        f_test.write(tipoEntidade+' <sep>\\n')\n",
    "        for token in tokens:\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==tipoEntidade:\n",
    "                    tag = ent[2]\n",
    "                    break\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            if tag!=tipoEntidade:\n",
    "                tag='O'\n",
    "            else:\n",
    "                tag='ENT'\n",
    "            f_test.write(tokenGravar+' '+tag+'\\n')\n",
    "            num_entidade_test=num_entidade_test+1\n",
    "        f_test.write('\\n')\n",
    "f_test.close()\n",
    "\n",
    "print('num_entidade_train', num_entidade_train)\n",
    "print('num_entidade_dev', num_entidade_dev)\n",
    "print('num_entidade_test:', num_entidade_test)\n",
    "num_entidade_total=num_entidade_train+num_entidade_dev+num_entidade_test\n",
    "print('num_entidade_total:', num_entidade_total)\n",
    "\n",
    "#save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "#save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "#save_obj('dic_sentencesTest',dic_sentencesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
