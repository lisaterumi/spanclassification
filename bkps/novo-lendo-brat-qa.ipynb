{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando arquivo para NER-NestedClinBr\n",
    "\n",
    "Teste com formato QA.. tem como retornar as entidades descontinuas tbm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    with open('../obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    try:\n",
    "        with open('../obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastasCorpus = [r'corpus\\test',r'corpus\\train']\n",
    "#pastasCorpus = [r'corpus\\test']\n",
    "#pastasCorpus = [r'corpus\\TESTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make the World a 2y4Better Place2y0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'Make the World a 2.4Better Place2.0'\n",
    "pattern = r'([0-9])\\.([0-9])'\n",
    "replacement = r'\\1y\\2'\n",
    "html = re.sub(pattern, replacement, s)\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef getDicSentences(pastaCorpus):\\n    #devePrintar=True\\n    devePrintar=False\\n    dic_sentences = {}\\n    numMaxTokensPorFrase=0 # numero tokens da maior frase\\n    num=0\\n    listaEntidades=[]\\n    frasesComDescontinuas=[]\\n    for filename in os.listdir(pastaCorpus):\\n        f = os.path.join(pastaCorpus, filename)\\n        if os.path.isfile(f):\\n            fileNameSemExtensao=os.path.splitext(filename)[0]\\n            if devePrintar:\\n                print(\\'\\n\\n--fileName (sem extensao):--\\', fileNameSemExtensao)\\n                pass\\n            extension = os.path.splitext(filename)[1][1:]\\n            if extension==\\'ann\\':\\n                # tokens da frase\\n                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+\\'.txt\\', \"r\", encoding=\\'utf-8\\')\\n                linha=fileTxt.readlines()\\n                fileTxt.close()\\n                if devePrintar:\\n                    print(\\'linha:\\', linha)\\n                    pass\\n                frases=[]\\n                allFrasesString=\\'\\'\\n                numL=0\\n                for l in linha:\\n                    numL=numL+1\\n                    allFrasesString = allFrasesString+l\\n                    if numL==1: # descarta primeira frase, que é data de criação do doc\\n                        #print(\\'descartando frase:\\', l)\\n                        continue\\n                    if l.strip() and l.strip()!=\\'\\n\\':\\n                        #print(\\'l:\\', l)\\n                        pattern = r\\'([0-9])\\\\.([0-9])\\'\\n                        replacement = r\\'\\x01==\\x02\\'\\n                        novoL = re.sub(pattern, replacement, l.strip())\\n                        l2 = novoL.split(\\'.\\') # quebrando frases\\n                        for l3 in l2:\\n                            if l3.strip() and l3.strip()!=\\'\\n\\':\\n                                novaFrase = l3.replace(\\'\\n\\',\\'\\').replace(\\'==\\',\\'.\\').strip()+\\'.\\'\\n                                frases.append(novaFrase)\\n                print(\\'frases:\\', frases)\\n                # para cada frase\\n                # para tokenizar nesses tokens\\n                print(\\'allFrasesString:\\', allFrasesString)\\n                frasesTokens={}\\n                #numCaracteresTotal=42 # primeira frase ignorada\\n                numIndiceAnterior=0\\n                for frase in frases:\\n                    tokens=[]\\n                    frase2 = frase.strip().replace(\\'/\\',\\' / \\').replace(\\')\\',\\' ) \\').replace(\\'(\\',\\' ( \\').replace(\\']\\',\\' ] \\').replace(\\'[\\',\\' [ \\').replace(\\',\\',\\' , \\').replace(\\'.\\',\\' . \\').replace(\\';\\',\\' ; \\').replace(\\'-\\',\\' - \\').replace(\\'+\\',\\' + \\').replace(\"\\'\",\" \\' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\\n                    frase2 = replaceWhiteSpaces(frase2)\\n                    frase2 = frase2.split()\\n                    for numtoken, token in enumerate(frase2):\\n                        if devePrintar:\\n                            print(\\'token:\\', token)\\n                            print(\\'numIndiceAnterior:\\', numIndiceAnterior)\\n                        if token!=\\'.\\':\\n                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\\n                        else:\\n                            numCaracteresTotal=numIndiceAnterior+1\\n                        #tokens.append([token,numtoken])\\n                        tokens.append([token,numtoken, numCaracteresTotal])\\n                        numIndiceAnterior = numCaracteresTotal+len(token)-1\\n                        if numMaxTokensPorFrase<len(tokens):\\n                            numMaxTokensPorFrase = len(tokens)\\n                    frasesTokens[frase]=tokens\\n                    #linhaTokens=linha.copy()    \\n                if devePrintar:\\n                    print(\\'frasesTokens:\\', frasesTokens)\\n                # agora, as entidades\\n                fileAnn = open(f, \"r\", encoding=\\'utf-8\\')\\n                linha=fileAnn.readlines()\\n                fileAnn.close()\\n\\n                dicEntidades={}\\n                for entidade_linha in linha:\\n                    if \\';\\' not in entidade_linha: # tem descontinua?\\n                        entidade = entidade_linha.split(\\'\\t\\')\\n                        tipo_entidade = entidade[1]\\n                        inicio, fim = tipo_entidade.split()[1:3]\\n                        tipo_entidade = tipo_entidade.split()[0]\\n                        termos_entidade = entidade[2].replace(\\'\\n\\',\\'\\')\\n                        termos_entidade = termos_entidade.strip().replace(\\'/\\',\\' / \\').replace(\\')\\',\\' ) \\').replace(\\'(\\',\\' ( \\').replace(\\']\\',\\' ] \\').replace(\\'[\\',\\' [ \\').replace(\\',\\',\\' , \\').replace(\\'.\\',\\' . \\').replace(\\';\\',\\' ; \\').replace(\\'-\\',\\' - \\').replace(\\'+\\',\\' + \\').replace(\"\\'\",\" \\' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\\n                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\\n                    else:\\n                        frasesComDescontinuas.append(num)\\n                        #print(\\'descontinua, linha: {}, num: {}\\'.format(linha, num))\\n                        #print(\\'descontinua, file: {}, num: {}\\'.format(filename, num))\\n                        entidade = entidade_linha.split(\\'\\t\\')\\n                        # ex T10\\tProblema 244 252;279 306\\tdispneia aos mdoeardos-leves esforço\\n                        #Problema 244 252;279 306\\n                        entidade_temp=entidade[1].split(\\';\\')\\n                        entidade1=entidade_temp[0]\\n                        tipo_entidade = entidade1\\n                        inicio1, fim1 = tipo_entidade.split()[1:3]\\n                        tipo_entidade_string = tipo_entidade.split()[0]\\n                        # mandar só os termos referentes...\\n                        tamTermo1=int(fim1)-int(inicio1)\\n                        termos_entidade = entidade[2].replace(\\'\\n\\',\\'\\')\\n                        termos_entidade=termos_entidade[:tamTermo1]\\n                        termos_entidade = termos_entidade.strip().replace(\\'/\\',\\' / \\').replace(\\')\\',\\' ) \\').replace(\\'(\\',\\' ( \\').replace(\\']\\',\\' ] \\').replace(\\'[\\',\\' [ \\').replace(\\',\\',\\' , \\').replace(\\'.\\',\\' . \\').replace(\\';\\',\\' ; \\').replace(\\'-\\',\\' - \\').replace(\\'+\\',\\' + \\').replace(\"\\'\",\" \\' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\\n                        #print(\\'aaaaaaaaaaaaaaaaa\\')\\n                        dicEntidades[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\\n                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\\n                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\\n                        \\n                        entidade2=entidade_temp[1]\\n                        inicio2, fim2 = entidade2.split()[0:2]\\n                        termos_entidade = entidade[2].replace(\\'\\n\\',\\'\\')\\n                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\\n                        termos_entidade = termos_entidade.strip().replace(\\'/\\',\\' / \\').replace(\\')\\',\\' ) \\').replace(\\'(\\',\\' ( \\').replace(\\']\\',\\' ] \\').replace(\\'[\\',\\' [ \\').replace(\\',\\',\\' , \\').replace(\\'.\\',\\' . \\').replace(\\';\\',\\' ; \\').replace(\\'-\\',\\' - \\').replace(\\'+\\',\\' + \\').replace(\"\\'\",\" \\' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\\n                        dicEntidades[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\\n                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\\n                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\\n\\n\\n                #print(\\'frasesTokens:\\', frasesTokens)\\n                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\\n                listaIndicesJaUsados = []\\n                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\\n                #print(indicesDic)\\n                #print(\\'dicEntidades:\\', dicEntidades)\\n                \\n                for key, value in frasesTokens.items():\\n                    for i in indicesDic:\\n                        tipo_entidade,termos_entidade = dicEntidades[i]\\n                        ##key (i) = indices old a comparar\\n                        #print(\\'key:\\', key)\\n                        #print(\\'value:\\', value)\\n                        #print(\\'i:\\', i)\\n                        for token in value:\\n                            #print(\\'token:\\', token)\\n                            if i[0]==token[2]:\\n                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\\n                                novos_indices=[]\\n                                for k in range(novo_inicio,novo_fim):\\n                                    novos_indices.append(k)\\n                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\\n                                #print(\"[termos_entidade, novos_indices, tipo_entidade]:\", [termos_entidade, novos_indices, tipo_entidade])\\n                            else:\\n                                #print(\\'else, i[0]:\\',i[0])\\n                                pass\\n                        \\n                    if len(value)>0:\\n                        #print(\\'incluindo:\\', key)\\n                        dic_sentences[num]=[value, listaEntidades]\\n                        listaEntidades=[]\\n                        num=num+1 \\n\\n        #print(num)\\n        #if num>318:\\n        #    break\\n\\n        #if num>3:\\n        #    break\\n    print(\\'numMaxTokensPorFrase:\\', numMaxTokensPorFrase)\\n    return dic_sentences, frasesComDescontinuas\\n            \\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def getDicSentences(pastaCorpus):\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if numL==1: # descarta primeira frase, que é data de criação do doc\n",
    "                        #print('descartando frase:', l)\n",
    "                        continue\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('.') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal=numIndiceAnterior+1\n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                for entidade_linha in linha:\n",
    "                    if ';' not in entidade_linha: # tem descontinua?\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua, linha: {}, num: {}'.format(linha, num))\n",
    "                        #print('descontinua, file: {}, num: {}'.format(filename, num))\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        #print('aaaaaaaaaaaaaaaaa')\n",
    "                        dicEntidades[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        \n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "\n",
    "\n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print(indicesDic)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('key:', key)\n",
    "                        #print('value:', value)\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                                #print(\"[termos_entidade, novos_indices, tipo_entidade]:\", [termos_entidade, novos_indices, tipo_entidade])\n",
    "                            else:\n",
    "                                #print('else, i[0]:',i[0])\n",
    "                                pass\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        dic_sentences[num]=[value, listaEntidades]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>3:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicSentences(pastaCorpus):\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if numL==1: # descarta primeira frase, que é data de criação do doc\n",
    "                        #print('descartando frase:', l)\n",
    "                        continue\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('.') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                #print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                #print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal = numIndiceAnterior+1\n",
    "                           \n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                for entidade_linha in linha:\n",
    "                    if ';' not in entidade_linha: # tem descontinua?\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua, linha: {}, num: {}'.format(linha, num))\n",
    "                        #print('descontinua, file: {}, num: {}'.format(filename, num))\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        #print('aaaaaaaaaaaaaaaaa')\n",
    "                        dicEntidades[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        \n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "\n",
    "\n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print(indicesDic)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('key:', key)\n",
    "                        #print('value:', value)\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                                #print(\"[termos_entidade, novos_indices, tipo_entidade]:\", [termos_entidade, novos_indices, tipo_entidade])\n",
    "                            else:\n",
    "                                #print('else, i[0]:',i[0])\n",
    "                                pass\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        dic_sentences[num]=[value, listaEntidades]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>3:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 146\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0, 59],\n",
       "  ['acompanhamento', 1, 62],\n",
       "  ['no', 2, 77],\n",
       "  ['ambualtorio', 3, 80],\n",
       "  ['há', 4, 92],\n",
       "  ['5', 5, 95],\n",
       "  ['anos', 6, 97],\n",
       "  ['por', 7, 102],\n",
       "  ['FA', 8, 106],\n",
       "  [',', 9, 108],\n",
       "  ['uso', 10, 110],\n",
       "  ['de', 11, 114],\n",
       "  ['marevan', 12, 117],\n",
       "  ['5mg', 13, 125],\n",
       "  ['1', 14, 129],\n",
       "  ['x', 15, 131],\n",
       "  ['ao', 16, 133],\n",
       "  ['dia', 17, 136],\n",
       "  ['.', 18, 139]],\n",
       " [['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 192\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Treinamento:-- corpus\\train\n",
      "numMaxTokensPorFrase: 192\n",
      "len(sentences): 1736\n",
      "len(Descontinuas): 91\n",
      "len(frasesComDescontinuas): 51\n",
      "--Teste:-- corpus\\test\n",
      "numMaxTokensPorFrase: 146\n",
      "len(sentences): 506\n",
      "len(Descontinuas): 44\n",
      "len(frasesComDescontinuas): 17\n"
     ]
    }
   ],
   "source": [
    "print('--Treinamento:--', pastasCorpus[1])\n",
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n",
    "print('len(sentences):', len(dic_sentencesTrain))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTrain))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTrain)))\n",
    "\n",
    "\n",
    "print('--Teste:--', pastasCorpus[0])\n",
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n",
    "print('len(sentences):', len(dic_sentencesTest))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTest))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Abd', 0, 644],\n",
       "  ['globoso', 1, 648],\n",
       "  [',', 2, 655],\n",
       "  ['flacido', 3, 657],\n",
       "  [',', 4, 664],\n",
       "  ['indolor', 5, 666],\n",
       "  ['a', 6, 674],\n",
       "  ['palpacao', 7, 676],\n",
       "  [',', 8, 684],\n",
       "  ['sem', 9, 686],\n",
       "  ['VCM', 10, 690],\n",
       "  ['.', 11, 693]],\n",
       " [['Abd', [0], 'Anatomia'], ['VCM', [10], 'Problema']]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['CONTRAÇÃO', 0, 507],\n",
       "  ['SEGMENTAR', 1, 517],\n",
       "  ['DO', 2, 527],\n",
       "  ['VE', 3, 530],\n",
       "  ['ALTERADA', 4, 533],\n",
       "  ['.', 5, 541]],\n",
       " [['CONTRAÇÃO SEGMENTAR DO VE ALTERADA', [0, 1, 2, 3, 4], 'Problema'],\n",
       "  ['VE', [3], 'Anatomia']]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 15, 30, 30, 30, 30, 55, 55, 65, 65]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasesComDescontinuasTrain[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# frases com entidades descontinuas\n",
    "print(len(set(frasesComDescontinuasTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 49,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 91,\n",
       " 131,\n",
       " 147,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 227,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 263,\n",
       " 291,\n",
       " 291,\n",
       " 291,\n",
       " 316,\n",
       " 329,\n",
       " 367,\n",
       " 367,\n",
       " 367,\n",
       " 367,\n",
       " 406,\n",
       " 468]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frasesComDescontinuasTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Comorbidades', 0, 142], [':', 1, 154], ['DM', 2, 156], ['há', 3, 159], ['10', 4, 162], ['anos', 5, 165], ['em', 6, 170], ['uso', 7, 173], ['de', 8, 177], ['metformina', 9, 180], ['850mg', 10, 191], ['3', 11, 197], ['cp', 12, 199], ['/', 13, 201], ['dia', 14, 202], [',', 15, 205], ['acarbose', 16, 207], ['1', 17, 216], ['cp', 18, 218], ['/', 19, 220], ['dia', 20, 221], ['e', 21, 225], ['glicazida', 22, 227], ['60mg', 23, 237], ['2', 24, 242], ['cp', 25, 244], ['/', 26, 246], ['dia', 27, 247], ['e', 28, 251], ['insulina', 29, 253], ['(', 30, 262], ['24', 31, 263], ['-', 32, 266], ['0', 33, 268], ['-', 34, 270], ['24', 35, 272], [')', 36, 274], ['.', 37, 275]], [['Comorbidades', [0], 'Problema'], ['DM', [2], 'Problema'], ['metformina 850mg', [9, 10], 'Tratamento'], ['acarbose', [16], 'Tratamento'], ['glicazida 60mg', [22, 23], 'Tratamento'], ['insulina', [29], 'Tratamento']]]\n"
     ]
    }
   ],
   "source": [
    "print(dic_sentencesTest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhoTrain 1319\n",
      "tamanhoDev 417\n",
      "len(dic_sentencesTrain): 1319\n",
      "len(dic_sentencesDev): 416\n",
      "[[['Paciente', 0, 151], ['relata', 1, 160], ['apenas', 2, 167], ['um', 3, 174], ['episodio', 4, 177], ['no', 5, 186], ['momento', 6, 189], ['de', 7, 197], ['gripe', 8, 200], ['.', 9, 205]], [['gripe', [8], 'Problema']]]\n",
      "[[['HAS', 0, 207], [',', 1, 210], ['ICC', 2, 212], [',', 3, 215], ['nega', 4, 217], ['DM', 5, 222], ['.', 6, 224]], [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]\n"
     ]
    }
   ],
   "source": [
    "save_obj('dic_sentencesTrainDev',dic_sentencesTrain)\n",
    "porc=0.76\n",
    "tamanhoTotal = len(dic_sentencesTrain)\n",
    "tamanhoTrain = int(tamanhoTotal*porc)\n",
    "print('tamanhoTrain', tamanhoTrain)\n",
    "tamanhoDev = tamanhoTotal - tamanhoTrain\n",
    "print('tamanhoDev', tamanhoDev)\n",
    "dic_sentencesDev_temp = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[tamanhoTrain:-1]}\n",
    "dic_sentencesTrain = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[:tamanhoTrain]}\n",
    "num=0\n",
    "dic_sentencesDev = {}\n",
    "for key, value in dic_sentencesDev_temp.items():\n",
    "    dic_sentencesDev[num] = value\n",
    "    num=num+1\n",
    "\n",
    "print('len(dic_sentencesTrain):', len(dic_sentencesTrain))\n",
    "print('len(dic_sentencesDev):', len(dic_sentencesDev))\n",
    "print(dic_sentencesTrain[tamanhoTrain-1])\n",
    "print(dic_sentencesDev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0, 207],\n",
       "  [',', 1, 210],\n",
       "  ['ICC', 2, 212],\n",
       "  [',', 3, 215],\n",
       "  ['nega', 4, 217],\n",
       "  ['DM', 5, 222],\n",
       "  ['.', 6, 224]],\n",
       " [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[280]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [i[2] for i in dic_sentencesDev[1][0] if i[1]==16]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i[0]) for i in dic_sentencesDev[1][0] if i[1]==1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto='123'\n",
    "texto[:len(texto)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0, 59],\n",
       "  ['acompanhamento', 1, 62],\n",
       "  ['no', 2, 77],\n",
       "  ['ambualtorio', 3, 80],\n",
       "  ['há', 4, 92],\n",
       "  ['5', 5, 95],\n",
       "  ['anos', 6, 97],\n",
       "  ['por', 7, 102],\n",
       "  ['FA', 8, 106],\n",
       "  [',', 9, 108],\n",
       "  ['uso', 10, 110],\n",
       "  ['de', 11, 114],\n",
       "  ['marevan', 12, 117],\n",
       "  ['5mg', 13, 125],\n",
       "  ['1', 14, 129],\n",
       "  ['x', 15, 131],\n",
       "  ['ao', 16, 133],\n",
       "  ['dia', 17, 136],\n",
       "  ['.', 18, 139]],\n",
       " [['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef corrigeIndices(dic_sentencesTest):\\n    dic_sentencesTest2 = {}\\n    for key, value in dic_sentencesTest.items():\\n        tokens =value[0] \\n        tokens2=tokens.copy()\\n        valorDesconto=0\\n        listaTokens = []\\n        for i, token in enumerate(tokens2):\\n            #print(token)\\n            ultimoIndice=token[2]\\n            if i==0:\\n                valorDesconto = ultimoIndice\\n            ultimoIndiceCorrigido = ultimoIndice - valorDesconto\\n            listaTokens.append([token[0], token[1], ultimoIndiceCorrigido])\\n            #print(ultimoIndiceCorrigido)\\n        #print(tokens2)\\n        dic_sentencesTest2[key] = [listaTokens, value[1]]\\n    return dic_sentencesTest2\\n\\ndic_sentencesTest2 = corrigeIndices(dic_sentencesTest)\\ndic_sentencesTest2[3]\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def corrigeIndices(dic_sentencesTest):\n",
    "    dic_sentencesTest2 = {}\n",
    "    for key, value in dic_sentencesTest.items():\n",
    "        tokens =value[0] \n",
    "        tokens2=tokens.copy()\n",
    "        valorDesconto=0\n",
    "        listaTokens = []\n",
    "        for i, token in enumerate(tokens2):\n",
    "            #print(token)\n",
    "            ultimoIndice=token[2]\n",
    "            if i==0:\n",
    "                valorDesconto = ultimoIndice\n",
    "            ultimoIndiceCorrigido = ultimoIndice - valorDesconto\n",
    "            listaTokens.append([token[0], token[1], ultimoIndiceCorrigido])\n",
    "            #print(ultimoIndiceCorrigido)\n",
    "        #print(tokens2)\n",
    "        dic_sentencesTest2[key] = [listaTokens, value[1]]\n",
    "    return dic_sentencesTest2\n",
    "\n",
    "dic_sentencesTest2 = corrigeIndices(dic_sentencesTest)\n",
    "dic_sentencesTest2[3]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0, 0],\n",
       "  ['há', 1, 4],\n",
       "  ['15', 2, 7],\n",
       "  ['anos', 3, 10],\n",
       "  ['em', 4, 15],\n",
       "  ['uso', 5, 18],\n",
       "  ['de', 6, 22],\n",
       "  ['losartana', 7, 25],\n",
       "  ['50mg', 8, 35],\n",
       "  ['/', 9, 40],\n",
       "  ['dia', 10, 42],\n",
       "  ['e', 11, 46],\n",
       "  ['digoxina', 12, 48],\n",
       "  ['1', 13, 57],\n",
       "  ['/', 14, 59],\n",
       "  ['2', 15, 61],\n",
       "  ['cp', 16, 63],\n",
       "  ['/', 17, 66],\n",
       "  ['dia', 18, 68],\n",
       "  [',', 19, 72],\n",
       "  ['carvedilol', 20, 74],\n",
       "  ['25', 21, 85],\n",
       "  ['12', 22, 88],\n",
       "  ['/', 23, 91],\n",
       "  ['12', 24, 93],\n",
       "  [',', 25, 96],\n",
       "  ['HCTZ', 26, 98],\n",
       "  ['.', 27, 103]],\n",
       " [['HAS', [0], 'Problema'],\n",
       "  ['losartana 50mg', [7, 8], 'Tratamento'],\n",
       "  ['digoxina', [12], 'Tratamento'],\n",
       "  ['carvedilol 25', [20, 21], 'Tratamento'],\n",
       "  ['HCTZ', [26], 'Tratamento']]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corrigeIndices(dic_sentencesTest):\n",
    "    dic_sentencesTest2 = {}\n",
    "    for key, value in dic_sentencesTest.items():\n",
    "        tokens =value[0] \n",
    "        tokens2=tokens.copy()\n",
    "        listaTokens = []\n",
    "        indiceAnterior=0\n",
    "        tamanhoAnterior=0\n",
    "        for i, token in enumerate(tokens2):\n",
    "            #print(token)\n",
    "            if i==0:\n",
    "                indice=0\n",
    "            else:\n",
    "                indice = indiceAnterior+tamanhoAnterior+1\n",
    "            #indice=token[2]\n",
    "            listaTokens.append([token[0], token[1], indice])\n",
    "            indiceAnterior = indice\n",
    "            tamanhoAnterior = len(token[0])\n",
    "        dic_sentencesTest2[key]=[listaTokens, value[1]]\n",
    "    return dic_sentencesTest2\n",
    "\n",
    "dic_sentencesTest2 = corrigeIndices(dic_sentencesTest)\n",
    "dic_sentencesTest2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Comorbidades', 0, 0],\n",
       "  [':', 1, 13],\n",
       "  ['DM', 2, 15],\n",
       "  ['há', 3, 18],\n",
       "  ['10', 4, 21],\n",
       "  ['anos', 5, 24],\n",
       "  ['em', 6, 29],\n",
       "  ['uso', 7, 32],\n",
       "  ['de', 8, 36],\n",
       "  ['metformina', 9, 39],\n",
       "  ['850mg', 10, 50],\n",
       "  ['3', 11, 56],\n",
       "  ['cp', 12, 58],\n",
       "  ['/', 13, 61],\n",
       "  ['dia', 14, 63],\n",
       "  [',', 15, 67],\n",
       "  ['acarbose', 16, 69],\n",
       "  ['1', 17, 78],\n",
       "  ['cp', 18, 80],\n",
       "  ['/', 19, 83],\n",
       "  ['dia', 20, 85],\n",
       "  ['e', 21, 89],\n",
       "  ['glicazida', 22, 91],\n",
       "  ['60mg', 23, 101],\n",
       "  ['2', 24, 106],\n",
       "  ['cp', 25, 108],\n",
       "  ['/', 26, 111],\n",
       "  ['dia', 27, 113],\n",
       "  ['e', 28, 117],\n",
       "  ['insulina', 29, 119],\n",
       "  ['(', 30, 128],\n",
       "  ['24', 31, 130],\n",
       "  ['-', 32, 133],\n",
       "  ['0', 33, 135],\n",
       "  ['-', 34, 137],\n",
       "  ['24', 35, 139],\n",
       "  [')', 36, 142],\n",
       "  ['.', 37, 144]],\n",
       " [['Comorbidades', [0], 'Problema'],\n",
       "  ['DM', [2], 'Problema'],\n",
       "  ['metformina 850mg', [9, 10], 'Tratamento'],\n",
       "  ['acarbose', [16], 'Tratamento'],\n",
       "  ['glicazida 60mg', [22, 23], 'Tratamento'],\n",
       "  ['insulina', [29], 'Tratamento']]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTrain2 = corrigeIndices(dic_sentencesTrain)\n",
    "dic_sentencesTest2 = corrigeIndices(dic_sentencesTest)\n",
    "dic_sentencesDev2 = corrigeIndices(dic_sentencesDev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravaArquivoQA(dic_sentencesTest, name):\n",
    "    tipos= ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "    texto=r'{\"data\": ['\n",
    "    # title\n",
    "    # paragraphs\n",
    "        #context, qas (answers (answer_start_original, text_original)), question (id, question_original)\n",
    "    num=0\n",
    "    idQa=1000000\n",
    "\n",
    "    for i in range(len(dic_sentencesTest)):\n",
    "        ents = dic_sentencesTest[i][1]\n",
    "\n",
    "        listaEntidadesProblema=list()\n",
    "        listaEntidadesTeste=list()\n",
    "        listaEntidadesTratamento=list()\n",
    "        listaEntidadesAnatomia=list()\n",
    "\n",
    "\n",
    "        #print(contexto)\n",
    "        for entidade in ents:\n",
    "            if entidade[2] == 'Problema':\n",
    "                listaEntidadesProblema.append(entidade)\n",
    "            elif entidade[2] == 'Teste':\n",
    "                listaEntidadesTeste.append(entidade)\n",
    "            elif entidade[2] == 'Tratamento':\n",
    "                listaEntidadesTratamento.append(entidade)\n",
    "            elif entidade[2] == 'Anatomia':\n",
    "                listaEntidadesAnatomia.append(entidade)\n",
    "\n",
    "        if len(listaEntidadesProblema)==0 and len(listaEntidadesTeste)==0 and len(listaEntidadesTratamento)==0 and len(listaEntidadesAnatomia)==0:\n",
    "            continue\n",
    "\n",
    "        title='texto_'+str(i)\n",
    "        if num==0:\n",
    "            texto = texto+'{\"title\":\"'+title+'\",\"paragraphs\": [{\"context\": \"'\n",
    "        else:\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+']}]}, {\"title\":\"'+title+'\",\"paragraphs\": [{\"context\": \"'\n",
    "        \n",
    "        num=num+1\n",
    "\n",
    "        tokens = dic_sentencesTest[i][0]\n",
    "        frase=[t[0] for t in tokens]\n",
    "        contexto = ' '.join(frase)\n",
    "        #contexto = contexto.replace(' , ', ', ').replace(' : ', ': ').replace(' . ', '. ').replace(' / ', '/ ')\n",
    "        if '\"' in contexto:\n",
    "            print('tem aspassssss')\n",
    "        texto = texto+contexto+'\", \"qas\": ['\n",
    "\n",
    "\n",
    "        if len(listaEntidadesProblema)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspassssss22')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"answers\": ['\n",
    "            for entidade in listaEntidadesProblema:\n",
    "                texto = texto+'{\"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+', \"text\": \"'+entidade[0]+'\"},'\n",
    "                \n",
    "\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+'], \"question\": \"Problema\", \"id\":\"'+str(idQa)+'\"},'\n",
    "\n",
    "        if len(listaEntidadesTeste)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspassssss22')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"answers\": ['\n",
    "            for entidade in listaEntidadesTeste:\n",
    "                texto = texto+'{\"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                try:\n",
    "                    start_string = [j[2] for j in dic_sentencesTest[i][0] if j[1]==start][0]\n",
    "                except:\n",
    "                    print()\n",
    "                    raise\n",
    "                fim_string = [j[2] for j in dic_sentencesTest[i][0] if j[1]==fim][0]\n",
    "                fim_string = fim_string+[len(j[0]) for j in dic_sentencesTest[i][0] if j[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+', \"text\": \"'+entidade[0]+'\"},'\n",
    "\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+'], \"question\": \"Teste\", \"id\":\"'+str(idQa)+'\"},'\n",
    "\n",
    "        if len(listaEntidadesTratamento)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspassssss22')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"answers\": ['\n",
    "            for entidade in listaEntidadesTratamento:\n",
    "                texto = texto+'{\"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+', \"text\": \"'+entidade[0]+'\"},'\n",
    "\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+'], \"question\": \"Tratamento\", \"id\":\"'+str(idQa)+'\"},'\n",
    "\n",
    "        if len(listaEntidadesAnatomia)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspassssss22')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"answers\": ['\n",
    "            for entidade in listaEntidadesAnatomia:\n",
    "                texto = texto+'{\"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentencesTest[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+', \"text\": \"'+entidade[0]+'\"},'\n",
    "\n",
    "            # tirando ultima virgula\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+'], \"question\": \"Anatomia\", \"id\":\"'+str(idQa)+'\"},'\n",
    "\n",
    "    texto = texto[:len(texto)-1]\n",
    "    texto = texto+']}]}], \"version\": \"1.1\"}'   \n",
    "    arq = open(name, 'w', encoding='utf8')\n",
    "    arq.write(texto)\n",
    "    arq.close()\n",
    "    #print(texto)\n",
    "  \n",
    "gravaArquivoQA(dic_sentencesTest2, 'data_qa/nested_test.json')\n",
    "gravaArquivoQA(dic_sentencesTrain2, 'data_qa/nested_train.json')\n",
    "gravaArquivoQA(dic_sentencesDev2, 'data_qa/nested_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "try:\n",
    "  file = r'data_qa/nested_test.json'\n",
    "  # Opening JSON file & returns JSON object as a dictionary \n",
    "  #f = open(file, encoding=\"utf8\") \n",
    "  f = open(file, encoding=\"utf8\") \n",
    "  data = json.load(f) \n",
    "except:\n",
    "  print('nao abriu arquivo')\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 0, 0],\n",
       "  ['importante', 1, 9],\n",
       "  ['aos', 2, 20],\n",
       "  ['esforços', 3, 24],\n",
       "  ['+', 4, 33],\n",
       "  ['dor', 5, 35],\n",
       "  ['tipo', 6, 39],\n",
       "  ['peso', 7, 44],\n",
       "  ['no', 8, 49],\n",
       "  ['peito', 9, 52],\n",
       "  ['no', 10, 58],\n",
       "  ['esforço', 11, 61],\n",
       "  ['.', 12, 69]],\n",
       " [['Dispneia importante aos esforços', [0, 1, 2, 3], 'Problema'],\n",
       "  ['dor tipo peso no peito no esforço', [5, 6, 7, 8, 9, 10, 11], 'Problema'],\n",
       "  ['peito', [9], 'Anatomia']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squad v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: data_qa_impossible/nested_test_impossible.json\n",
      "name: data_qa_impossible/nested_train_impossible.json\n",
      "name: data_qa_impossible/nested_dev_impossible.json\n"
     ]
    }
   ],
   "source": [
    "# versão 2 do squad\n",
    "def gravaArquivoQA_impossible(dic_sentences, name):\n",
    "    print('name:', name)\n",
    "    tipos= ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "    texto=r'{\"version\": \"v2.0\", \"data\": ['\n",
    "    # title\n",
    "    # paragraphs\n",
    "        #context, qas (answers (answer_start_original, text_original)), question (id, question_original)\n",
    "    num=0\n",
    "    idQa=1000000\n",
    "\n",
    "    for i in range(len(dic_sentences)):\n",
    "        ents = dic_sentences[i][1]\n",
    "\n",
    "        listaEntidadesProblema=list()\n",
    "        listaEntidadesTeste=list()\n",
    "        listaEntidadesTratamento=list()\n",
    "        listaEntidadesAnatomia=list()\n",
    "\n",
    "        #print(contexto)\n",
    "        for entidade in ents:\n",
    "            if entidade[2] == 'Problema':#problema\n",
    "                listaEntidadesProblema.append(entidade)\n",
    "            elif entidade[2] == 'Teste': #teste\n",
    "                listaEntidadesTeste.append(entidade)\n",
    "            elif entidade[2] == 'Tratamento':#tratamento\n",
    "                listaEntidadesTratamento.append(entidade)\n",
    "            elif entidade[2] == 'Anatomia':#anatomia\n",
    "                listaEntidadesAnatomia.append(entidade)\n",
    "\n",
    "        if len(listaEntidadesProblema)==0 and len(listaEntidadesTeste)==0 and len(listaEntidadesTratamento)==0 and len(listaEntidadesAnatomia)==0:\n",
    "            continue\n",
    "\n",
    "        title='texto_'+str(i)\n",
    "        if num==0:\n",
    "            texto = texto+'{\"title\":\"'+title+'\",\"paragraphs\": [{\"qas\": ['\n",
    "        else:\n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto+'}]}, {\"title\":\"'+title+'\",\"paragraphs\": [{\"qas\": ['\n",
    "        \n",
    "        num=num+1\n",
    "\n",
    "        tokens = dic_sentences[i][0]\n",
    "        frase=[t[0] for t in tokens]\n",
    "        contexto = ' '.join(frase)\n",
    "        \n",
    "        \n",
    "        if len(listaEntidadesProblema)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspas em Problema')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Problema\", \"id\":\"'+str(idQa)+'\", \"answers\": ['\n",
    "            \n",
    "            for entidade in listaEntidadesProblema:\n",
    "                #print(dic_sentencesTest[i][0])\n",
    "                texto = texto+'{\"text\":\"'+entidade[0]+'\", \"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentences[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+'},'\n",
    "                #\"is_impossible\": false\n",
    "                \n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto + '], \"is_impossible\": false},'\n",
    "        \n",
    "        else:\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Problema\", \"id\":\"'+str(idQa)+'\", \"answers\": [], \"is_impossible\": true},'\n",
    "\n",
    "        if len(listaEntidadesTeste)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspas em Teste')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Teste\", \"id\":\"'+str(idQa)+'\", \"answers\": ['\n",
    "            \n",
    "            for entidade in listaEntidadesTeste:\n",
    "                #print(dic_sentencesTest[i][0])\n",
    "                texto = texto+'{\"text\":\"'+entidade[0]+'\", \"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentences[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+'},'\n",
    "                #\"is_impossible\": false\n",
    "                \n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto + '], \"is_impossible\": false},'\n",
    "        \n",
    "        else:\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Teste\", \"id\":\"'+str(idQa)+'\", \"answers\": [], \"is_impossible\": true},'\n",
    "\n",
    "        if len(listaEntidadesTratamento)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspas Tratamento')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Tratamento\", \"id\":\"'+str(idQa)+'\", \"answers\": ['\n",
    "            \n",
    "            for entidade in listaEntidadesTratamento:\n",
    "                #print(dic_sentencesTest[i][0])\n",
    "                texto = texto+'{\"text\":\"'+entidade[0]+'\", \"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentences[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+'},'\n",
    "                #\"is_impossible\": false\n",
    "                \n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto + '], \"is_impossible\": false},'\n",
    "        \n",
    "        else:\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Tratamento\", \"id\":\"'+str(idQa)+'\", \"answers\": [], \"is_impossible\": true},'\n",
    "\n",
    "        if len(listaEntidadesAnatomia)>0:\n",
    "            if '\"' in entidade[0]:\n",
    "                print('tem aspas em Anatomia')\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Anatomia\", \"id\":\"'+str(idQa)+'\", \"answers\": ['\n",
    "            \n",
    "            for entidade in listaEntidadesAnatomia:\n",
    "                #print(dic_sentencesTest[i][0])\n",
    "                texto = texto+'{\"text\":\"'+entidade[0]+'\", \"answer_start\":'\n",
    "                indices = entidade[1]\n",
    "                start =indices[0]\n",
    "                fim = indices[-1]\n",
    "                start_string = [i[2] for i in dic_sentences[i][0] if i[1]==start][0]\n",
    "                fim_string = [i[2] for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "                fim_string = fim_string+[len(i[0]) for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "\n",
    "                texto = texto+str(start_string)+'},'\n",
    "                #\"is_impossible\": false\n",
    "                \n",
    "            texto = texto[:len(texto)-1]\n",
    "            texto = texto + '], \"is_impossible\": false},'\n",
    "        \n",
    "        else:\n",
    "            idQa=idQa+1\n",
    "            texto = texto+'{\"question\": \"Anatomia\", \"id\":\"'+str(idQa)+'\", \"answers\": [], \"is_impossible\": true},'   \n",
    "                    \n",
    "        texto = texto[:len(texto)-1]\n",
    "        texto = texto+'], \"context\":\"'+contexto+'\"}' \n",
    "        \n",
    "    texto = texto+']}]}'\n",
    "\n",
    "    arq = open(name, 'w', encoding='utf8')\n",
    "    arq.write(texto)\n",
    "    arq.close()\n",
    "    #print(texto)\n",
    "  \n",
    "gravaArquivoQA_impossible(dic_sentencesTest2, 'data_qa_impossible/nested_test_impossible.json')\n",
    "gravaArquivoQA_impossible(dic_sentencesTrain2, 'data_qa_impossible/nested_train_impossible.json')\n",
    "gravaArquivoQA_impossible(dic_sentencesDev2, 'data_qa_impossible/nested_dev_impossible.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gerando agora no formato do MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: data_qa_mrc/nested/mrc-ner.test\n"
     ]
    }
   ],
   "source": [
    "# versão MRC\n",
    "\n",
    "def gravaArquivoQA_mrc(dic_sentences, name):\n",
    "    print('name:', name)\n",
    "    tipos= ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "    texto=r'['\n",
    "    # title\n",
    "    # paragraphs\n",
    "        #context, qas (answers (answer_start_original, text_original)), question (id, question_original)\n",
    "    num=0\n",
    "    idQa=1000000\n",
    "\n",
    "    for i in range(len(dic_sentences)):\n",
    "        ents = dic_sentences[i][1]\n",
    "\n",
    "        listaEntidadesProblema=list()\n",
    "        listaEntidadesTeste=list()\n",
    "        listaEntidadesTratamento=list()\n",
    "        listaEntidadesAnatomia=list()\n",
    "\n",
    "        #print(contexto)\n",
    "        for entidade in ents:\n",
    "            if entidade[2] == 'Problema':#problema\n",
    "                listaEntidadesProblema.append(entidade)\n",
    "            elif entidade[2] == 'Teste': #teste\n",
    "                listaEntidadesTeste.append(entidade)\n",
    "            elif entidade[2] == 'Tratamento':#tratamento\n",
    "                listaEntidadesTratamento.append(entidade)\n",
    "            elif entidade[2] == 'Anatomia':#anatomia\n",
    "                listaEntidadesAnatomia.append(entidade)\n",
    "\n",
    "        if len(listaEntidadesProblema)==0 and len(listaEntidadesTeste)==0 and len(listaEntidadesTratamento)==0 and len(listaEntidadesAnatomia)==0:\n",
    "            continue\n",
    "\n",
    "        tokens = dic_sentences[i][0]\n",
    "        frase=[t[0] for t in tokens]\n",
    "        contexto = ' '.join(frase)\n",
    "        \n",
    "        texto = texto+'{\"context\":\"'+contexto+'\",' \n",
    "        \n",
    "        #if len(listaEntidadesProblema)>0:\n",
    "        tipo='Problema'\n",
    "        if '\"' in entidade[0]:\n",
    "            print('tem aspas em ', tipo)\n",
    "        idQa=idQa+1\n",
    "\n",
    "        ends=[]\n",
    "        starts=[]\n",
    "\n",
    "        for entidade in listaEntidadesProblema:\n",
    "            indices = entidade[1]\n",
    "            start =indices[0]\n",
    "            fim = indices[-1]\n",
    "            #start_string = [i[2] for i in dic_sentences[i][0] if i[1]==start][0]\n",
    "            #fim_string = [i[2] for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "            #fim_string = fim_string+[len(i[0]) for i in dic_sentences[i][0] if i[1]==fim][0]\n",
    "            #starts = [6,13]\n",
    "            starts.append(start)\n",
    "            ends.append(fim)\n",
    "\n",
    "        spans_pos=''\n",
    "        impossible=\"true\"\n",
    "        spans_pos = '[]'\n",
    "        if len(starts)>0:\n",
    "            impossible=\"false\"\n",
    "            spans_pos = '['\n",
    "            for a, b in zip(starts, ends):\n",
    "                spans_pos = spans_pos +'\"'+ str(a)+';'+str(b)+'\",'\n",
    "            spans_pos =spans_pos[0:-1]+ ']'\n",
    "\n",
    "\n",
    "        texto = texto+'\"end_position\": '+str(ends)+', \"entity_label\":\"'+tipo+'\", \"impossible\":'+impossible+', \"qas_id\": \"'+ str(idQa)+'.1\",'\n",
    "        texto = texto+'\"query\":\"'+tipo+'\", \"span_position\":'+str(spans_pos)+', \"start_position\":'+str(starts)+\"},\" \n",
    "\n",
    "        texto = texto+'{\"context\":\"'+contexto+'\",' \n",
    "        \n",
    "        tipo='Teste'\n",
    "        if '\"' in entidade[0]:\n",
    "            print('tem aspas em ', tipo)\n",
    "        idQa=idQa+1\n",
    "\n",
    "        ends=[]\n",
    "        starts=[]\n",
    "\n",
    "        for entidade in listaEntidadesTeste:\n",
    "            indices = entidade[1]\n",
    "            start =indices[0]\n",
    "            fim = indices[-1]\n",
    "            starts.append(start)\n",
    "            ends.append(fim)\n",
    "\n",
    "        spans_pos=''\n",
    "        impossible=\"true\"\n",
    "        spans_pos = '[]'\n",
    "        if len(starts)>0:\n",
    "            impossible=\"false\"\n",
    "            spans_pos = '['\n",
    "            for a, b in zip(starts, ends):\n",
    "                spans_pos = spans_pos +'\"'+ str(a)+';'+str(b)+'\",'\n",
    "            spans_pos =spans_pos[0:-1]+ ']'\n",
    "\n",
    "\n",
    "        texto = texto+'\"end_position\": '+str(ends)+', \"entity_label\":\"'+tipo+'\", \"impossible\":'+impossible+', \"qas_id\": \"'+ str(idQa)+'.1\",'\n",
    "        texto = texto+'\"query\":\"'+tipo+'\", \"span_position\":'+str(spans_pos)+', \"start_position\":'+str(starts)+\"},\" \n",
    "\n",
    "        texto = texto+'{\"context\":\"'+contexto+'\",' \n",
    "        tipo='Tratamento'\n",
    "        if '\"' in entidade[0]:\n",
    "            print('tem aspas em ', tipo)\n",
    "        idQa=idQa+1\n",
    "\n",
    "        ends=[]\n",
    "        starts=[]\n",
    "\n",
    "        for entidade in listaEntidadesTratamento:\n",
    "            indices = entidade[1]\n",
    "            start =indices[0]\n",
    "            fim = indices[-1]\n",
    "            starts.append(start)\n",
    "            ends.append(fim)\n",
    "\n",
    "        spans_pos=''\n",
    "        impossible=\"true\"\n",
    "        spans_pos = '[]'\n",
    "        if len(starts)>0:\n",
    "            impossible=\"false\"\n",
    "            spans_pos = '['\n",
    "            for a, b in zip(starts, ends):\n",
    "                spans_pos = spans_pos +'\"'+ str(a)+';'+str(b)+'\",'\n",
    "            spans_pos =spans_pos[0:-1]+ ']'\n",
    "\n",
    "\n",
    "        texto = texto+'\"end_position\": '+str(ends)+', \"entity_label\":\"'+tipo+'\", \"impossible\":'+impossible+', \"qas_id\": \"'+ str(idQa)+'.1\",'\n",
    "        texto = texto+'\"query\":\"'+tipo+'\", \"span_position\":'+str(spans_pos)+', \"start_position\":'+str(starts)+\"},\" \n",
    "\n",
    "        texto = texto+'{\"context\":\"'+contexto+'\",' \n",
    "        tipo='Anatomia'\n",
    "        if '\"' in entidade[0]:\n",
    "            print('tem aspas em ', tipo)\n",
    "        idQa=idQa+1\n",
    "\n",
    "        ends=[]\n",
    "        starts=[]\n",
    "\n",
    "        for entidade in listaEntidadesAnatomia:\n",
    "            indices = entidade[1]\n",
    "            start =indices[0]\n",
    "            fim = indices[-1]\n",
    "            starts.append(start)\n",
    "            ends.append(fim)\n",
    "\n",
    "        spans_pos=''\n",
    "        impossible=\"true\"\n",
    "        spans_pos = '[]'\n",
    "        if len(starts)>0:\n",
    "            impossible=\"false\"\n",
    "            spans_pos = '['\n",
    "            for a, b in zip(starts, ends):\n",
    "                spans_pos = spans_pos +'\"'+ str(a)+';'+str(b)+'\",'\n",
    "            spans_pos =spans_pos[0:-1]+ ']'\n",
    "\n",
    "\n",
    "        texto = texto+'\"end_position\": '+str(ends)+', \"entity_label\":\"'+tipo+'\", \"impossible\":'+impossible+', \"qas_id\": \"'+ str(idQa)+'.1\",'\n",
    "        texto = texto+'\"query\":\"'+tipo+'\", \"span_position\":'+str(spans_pos)+', \"start_position\":'+str(starts)+\"},\" \n",
    "\n",
    "        \n",
    "    #texto = texto+']'\n",
    "    texto =texto[0:-1]+ ']'\n",
    "\n",
    "    arq = open(name, 'w', encoding='utf8')\n",
    "    arq.write(texto)\n",
    "    arq.close()\n",
    "    #print(texto)\n",
    "  \n",
    "gravaArquivoQA_mrc(dic_sentencesTest2, 'data_qa_mrc/nested/mrc-ner.test')\n",
    "gravaArquivoQA_mrc(dic_sentencesTrain2, 'data_qa_mrc/nested/mrc-ner.train')\n",
    "gravaArquivoQA_mrc(dic_sentencesDev2, 'data_qa_mrc/nested/mrc-ner.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0, 0],\n",
       "  [',', 1, 4],\n",
       "  ['ICC', 2, 6],\n",
       "  [',', 3, 10],\n",
       "  ['nega', 4, 12],\n",
       "  ['DM', 5, 17],\n",
       "  ['.', 6, 20]],\n",
       " [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "']'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#starts = [2,3,4]\n",
    "#ends=[13,14,15]\n",
    "starts = []\n",
    "ends=[]\n",
    "#spans_pos = [str(a)+\";\"+str(b) for a, b in zip(starts, ends)]\n",
    "spans_pos = '['\n",
    "for a, b in zip(starts, ends):\n",
    "    spans_pos = spans_pos +'\"'+ str(a)+';'+str(b)+'\",'\n",
    "spans_pos =spans_pos[0:-1]+ ']'\n",
    "spans_pos\n",
    "#print(str(starts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
