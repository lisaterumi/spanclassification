{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn_crfsuite as crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "NUM_JANELA=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dor'],\n",
       " ['no'],\n",
       " ['braço'],\n",
       " ['esquerdo'],\n",
       " ['e'],\n",
       " ['direito'],\n",
       " ['dor', 'no'],\n",
       " ['no', 'braço'],\n",
       " ['braço', 'esquerdo'],\n",
       " ['esquerdo', 'e'],\n",
       " ['e', 'direito'],\n",
       " ['dor', 'no', 'braço'],\n",
       " ['no', 'braço', 'esquerdo'],\n",
       " ['braço', 'esquerdo', 'e'],\n",
       " ['esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo'],\n",
       " ['no', 'braço', 'esquerdo', 'e'],\n",
       " ['braço', 'esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo', 'e'],\n",
       " ['no', 'braço', 'esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo', 'e', 'direito']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entidade = ['dor', 'no', 'braço', 'esquerdo','e','direito']\n",
    "\n",
    "lista=list()\n",
    "for i in range(1, len(entidade)+1):\n",
    "    #lista.append(entidade[i-1])\n",
    "    for j in range(len(entidade) - i + 1):\n",
    "        lista.append(entidade[j:j + i])\n",
    "        \n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)\n",
    "\n",
    "def save_obj(name, obj):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    print('Load obj em: ', 'obj/' + name + '.pkl')\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesTrain.pkl\n",
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesDev.pkl\n",
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesTest.pkl\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTrain = load_obj(r'../spanclassification/obj/dic_sentencesTrain')\n",
    "dic_sentencesDev = load_obj(r'../spanclassification/obj/dic_sentencesDev')\n",
    "dic_sentencesTest = load_obj(r'../spanclassification/obj/dic_sentencesTest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0, 59],\n",
       "  ['acompanhamento', 1, 62],\n",
       "  ['no', 2, 77],\n",
       "  ['ambualtorio', 3, 80],\n",
       "  ['há', 4, 92],\n",
       "  ['5', 5, 95],\n",
       "  ['anos', 6, 97],\n",
       "  ['por', 7, 102],\n",
       "  ['FA', 8, 106],\n",
       "  [',', 9, 108],\n",
       "  ['uso', 10, 110],\n",
       "  ['de', 11, 114],\n",
       "  ['marevan', 12, 117],\n",
       "  ['5mg', 13, 125],\n",
       "  ['1', 14, 129],\n",
       "  ['x', 15, 131],\n",
       "  ['ao', 16, 133],\n",
       "  ['dia', 17, 136],\n",
       "  ['.', 18, 139]],\n",
       " [['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListaPositiva(dic_sentences):\n",
    "    # só positivas:\n",
    "    lista=list()\n",
    "    numJanela=NUM_JANELA\n",
    "    for key, value in dic_sentences.items():\n",
    "        #print('value:', value)\n",
    "        entidades = value[1]\n",
    "        if len(entidades)==0:\n",
    "            continue\n",
    "        for entidade in entidades:\n",
    "            #print('entidade:', entidade)\n",
    "            label = entidade[2]\n",
    "            indiceEntidade1=entidade[1][0]\n",
    "            indiceEntidade2=entidade[1][-1]\n",
    "            vizinhosAntes = list()\n",
    "            vizinhosDepois = list()\n",
    "            #print('indiceEntidade:', indiceEntidade)\n",
    "            for tokens in value[0]:\n",
    "                indice=tokens[1]\n",
    "                #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "                if indice+4>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                    vizinhosAntes.append(tokens[0])\n",
    "                if indice-4<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                    vizinhosDepois.append(tokens[0])\n",
    "            lista.append([entidade[0], vizinhosAntes, vizinhosDepois, label])\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FA', ['há', '5', 'anos', 'por'], [',', 'uso', 'de', 'marevan'], 'Problema'],\n",
       " ['marevan 5mg',\n",
       "  ['FA', ',', 'uso', 'de'],\n",
       "  ['1', 'x', 'ao', 'dia'],\n",
       "  'Tratamento'],\n",
       " ['Comorbidades', [], [':', 'DM', 'há', '10'], 'Problema'],\n",
       " ['DM', ['Comorbidades', ':'], ['há', '10', 'anos', 'em'], 'Problema'],\n",
       " ['metformina 850mg',\n",
       "  ['anos', 'em', 'uso', 'de'],\n",
       "  ['3', 'cp', '/', 'dia'],\n",
       "  'Tratamento']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = getListaPositiva(dic_sentencesTest)\n",
    "lista[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPostagger['cardiaco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj em:  obj/../spanclassification/obj/dic_postagger.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPostagger = load_obj('../spanclassification/obj/dic_postagger')\n",
    "def tipoPostaggerTokens(token, dicPostagger):\n",
    "    postagger = 'N' # na duvida é N\n",
    "    if token.lower() in dicPostagger.keys():\n",
    "        postagger = dicPostagger.get(token.lower())\n",
    "    return postagger\n",
    "tipoPostaggerTokens('coração', dicPostagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    try:\n",
    "        with open(cluster_file, encoding='utf-8') as i:\n",
    "            for num, line in enumerate(i):\n",
    "                if line:\n",
    "                    word, cluster = line.strip().split('\\t')\n",
    "                    word2cluster[word] = cluster\n",
    "    except:\n",
    "        print(line)\n",
    "        print(num)\n",
    "        raise\n",
    "    return word2cluster\n",
    "\n",
    "\n",
    "def word2features(lista, word2cluster, dicPostagger):\n",
    "    features = list()\n",
    "    for sent in lista:\n",
    "        entidades = sent[0]\n",
    "        for entidade in entidades.split():\n",
    "            postag = tipoPostaggerTokens(entidade, dicPostagger)\n",
    "            features.append([\n",
    "            'bias',\n",
    "            'word.lower=' + entidade.lower(),\n",
    "            'word[-3:]=' + entidade[-3:],\n",
    "            'word[-2:]=' + entidade[-2:],\n",
    "            'word.isupper=%s' % entidade.isupper(),\n",
    "            'word.istitle=%s' % entidade.istitle(),\n",
    "            'word.isdigit=%s' % entidade.isdigit(),\n",
    "            'word.cluster=%s' % word2cluster[entidade.lower()] if entidade.lower() in word2cluster else \"0\",\n",
    "            'postag=' + postag\n",
    "            ])\n",
    "        # palavras anteriores\n",
    "        vizinhosAntes = sent[1] \n",
    "        if len(vizinhosAntes)>0:\n",
    "            for num, vizinhoAntes in enumerate(vizinhosAntes):\n",
    "                word1 = vizinhoAntes\n",
    "                postag1 =  tipoPostaggerTokens(vizinhoAntes, dicPostagger)\n",
    "                features.extend([\n",
    "                    '-'+str(num+1)+':word.lower=' + word1.lower(),\n",
    "                    '-'+str(num+1)+':word.istitle=%s' % word1.istitle(),\n",
    "                    '-'+str(num+1)+':word.isupper=%s' % word1.isupper(),\n",
    "                    '-'+str(num+1)+':postag=' + postag1\n",
    "                ])\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        # próximas palavras\n",
    "        vizinhosDepois = sent[2]\n",
    "        if len(vizinhosDepois)>0:\n",
    "            for num, vizinhoDepois in enumerate(vizinhosDepois):\n",
    "                word1 = vizinhoDepois\n",
    "                postag1 =  tipoPostaggerTokens(vizinhoDepois, dicPostagger)\n",
    "                features.extend([\n",
    "                    '+'+str(num+1)+':word.lower=' + word1.lower(),\n",
    "                    '+'+str(num+1)+':word.istitle=%s' % word1.istitle(),\n",
    "                    '+'+str(num+1)+':word.isupper=%s' % word1.isupper(),\n",
    "                    '+'+str(num+1)+':postag=' + postag1\n",
    "                ])\n",
    "        else:\n",
    "            features.append('EOS')\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent, word2cluster):\n",
    "    return [word2features(sent, i, word2cluster) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "word2cluster = read_clusters(r\"clusters/cluster-50.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dor: 25\n",
      "esquerdo: 11\n",
      "direito: 11\n",
      "peito: 31\n",
      "no: 24\n",
      "na: 24\n",
      "coração: 11\n",
      "válvula: 29\n",
      "cardíaca: 9\n",
      "miocárdica: 46\n",
      "mitral: 41\n",
      "mmii: 7\n",
      "coronária: 5\n"
     ]
    }
   ],
   "source": [
    "print('dor:',word2cluster['dor'])\n",
    "print('esquerdo:',word2cluster['esquerdo'])\n",
    "print('direito:',word2cluster['direito'])\n",
    "print('peito:',word2cluster['peito'])\n",
    "print('no:',word2cluster['no'])\n",
    "print('na:',word2cluster['na'])\n",
    "print('coração:',word2cluster['coração'])\n",
    "print('válvula:',word2cluster['válvula'])\n",
    "print('cardíaca:',word2cluster['cardíaca'])\n",
    "print('miocárdica:',word2cluster['miocárdica'])\n",
    "print('mitral:',word2cluster['mitral'])\n",
    "print('mmii:',word2cluster['mmii'])\n",
    "print('coronária:',word2cluster['coronária'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2f = word2features(lista, word2cluster, dicPostagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bias', 'word.lower=fa', 'word[-3:]=FA', 'word[-2:]=FA', 'word.isupper=True', 'word.istitle=False', 'word.isdigit=False', 'word.cluster=8', 'postag=N'], '-1:word.lower=há', '-1:word.istitle=False', '-1:word.isupper=False', '-1:postag=V', '-2:word.lower=5', '-2:word.istitle=False', '-2:word.isupper=False', '-2:postag=NUM', '-3:word.lower=anos', '-3:word.istitle=False', '-3:word.isupper=False', '-3:postag=N', '-4:word.lower=por', '-4:word.istitle=False', '-4:word.isupper=False', '-4:postag=PREP', '+1:word.lower=,', '+1:word.istitle=False', '+1:word.isupper=False']\n"
     ]
    }
   ],
   "source": [
    "print(word2f[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj em:  obj/../spanclassification/obj/dic_predictions_results_ner_800.pkl\n"
     ]
    }
   ],
   "source": [
    "dic_predictions_results_ner = load_obj(r'../spanclassification/obj/dic_predictions_results_ner_800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['em', 1],\n",
       "  ['tratamento', 2],\n",
       "  ['para', 3],\n",
       "  ['ICC', 4],\n",
       "  ['diastólica', 5],\n",
       "  ['há', 6],\n",
       "  ['cerca', 7],\n",
       "  ['de', 8],\n",
       "  ['5', 9],\n",
       "  ['a', 10],\n",
       "  ['com', 11],\n",
       "  ['melhora', 12],\n",
       "  ['importante', 13],\n",
       "  ['dos', 14],\n",
       "  ['sintomas', 15],\n",
       "  ['após', 16],\n",
       "  ['início', 17],\n",
       "  ['do', 18],\n",
       "  ['tratamento', 19],\n",
       "  ['.', 20]],\n",
       " [['ICC diastólica', [4, 5], 'Problema'], ['tratamento', [19], 'Tratamento']]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_results_ner[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dor'], ['no'], ['peito'], ['dor', 'no'], ['no', 'peito'], ['dor', 'no', 'peito']]\n"
     ]
    }
   ],
   "source": [
    "# para corpus GENIA, ideal pq pega as descontinuas\n",
    "# no nestedclin, as descontinuas\n",
    "def powersetGenia(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def powerset(entidade):    \n",
    "    lista=list()\n",
    "    for i in range(1, len(entidade)+1):\n",
    "        #lista.append(entidade[i-1])\n",
    "        for j in range(len(entidade) - i + 1):\n",
    "            lista.append(entidade[j:j + i])\n",
    "    return lista\n",
    "        \n",
    "#print(list(powerset([4, 5, 6])))\n",
    "print(powerset(['dor', 'no', 'peito']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samplecorpusformat = [[['POI', 'B-Abbreviation'],  ['DE', 'O'],  ['LAVAGEM', 'O'],  ['+', 'O'],  ['CURETA', 'O'],  ['DE', 'O'],  ['TECIDO', 'O'],  ['NECROTICO', 'O'],  ['.', 'O']], [['18', 'O'],  [':', 'O'],  ['00', 'O'],  [':', 'O'],  ['PACIENTE', 'O'],  ['RETORNOU', 'O'],  ['DO', 'O'],  ['CC', 'B-Abbreviation'],  ['LUCIDO', 'O'], [',', 'O'],  ['ORIENTADO', 'O'],  [',', 'O'],  ['COMUNICATIVO', 'O'],  [';', 'O'], ['MANTEM', 'O'],  ['AVP', 'B-Abbreviation'],  ['COM', 'O'],  ['STP', 'B-Abbreviation'], ['.', 'O']]]\n",
    "\n",
    "# corpus:\n",
    "# gravar os indices dos tokens\n",
    "# para cada entidade, gerar a combinação possivel de tokens e pegar as features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
