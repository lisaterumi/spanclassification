{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn_crfsuite as crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dor'],\n",
       " ['no'],\n",
       " ['braço'],\n",
       " ['esquerdo'],\n",
       " ['e'],\n",
       " ['direito'],\n",
       " ['dor', 'no'],\n",
       " ['no', 'braço'],\n",
       " ['braço', 'esquerdo'],\n",
       " ['esquerdo', 'e'],\n",
       " ['e', 'direito'],\n",
       " ['dor', 'no', 'braço'],\n",
       " ['no', 'braço', 'esquerdo'],\n",
       " ['braço', 'esquerdo', 'e'],\n",
       " ['esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo'],\n",
       " ['no', 'braço', 'esquerdo', 'e'],\n",
       " ['braço', 'esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo', 'e'],\n",
       " ['no', 'braço', 'esquerdo', 'e', 'direito'],\n",
       " ['dor', 'no', 'braço', 'esquerdo', 'e', 'direito']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entidade = ['dor', 'no', 'braço', 'esquerdo','e','direito']\n",
    "\n",
    "lista=list()\n",
    "for i in range(1, len(entidade)+1):\n",
    "    #lista.append(entidade[i-1])\n",
    "    for j in range(len(entidade) - i + 1):\n",
    "        lista.append(entidade[j:j + i])\n",
    "        \n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)\n",
    "\n",
    "def save_obj(name, obj):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    print('Load obj em: ', 'obj/' + name + '.pkl')\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesTrain.pkl\n",
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesDev.pkl\n",
      "Load obj em:  obj/../spanclassification/obj/dic_sentencesTest.pkl\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTrain = load_obj(r'../spanclassification/obj/dic_sentencesTrain')\n",
    "dic_sentencesDev = load_obj(r'../spanclassification/obj/dic_sentencesDev')\n",
    "dic_sentencesTest = load_obj(r'../spanclassification/obj/dic_sentencesTest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0, 59],\n",
       "  ['acompanhamento', 1, 62],\n",
       "  ['no', 2, 77],\n",
       "  ['ambualtorio', 3, 80],\n",
       "  ['há', 4, 92],\n",
       "  ['5', 5, 95],\n",
       "  ['anos', 6, 97],\n",
       "  ['por', 7, 102],\n",
       "  ['FA', 8, 106],\n",
       "  [',', 9, 108],\n",
       "  ['uso', 10, 110],\n",
       "  ['de', 11, 114],\n",
       "  ['marevan', 12, 117],\n",
       "  ['5mg', 13, 125],\n",
       "  ['1', 14, 129],\n",
       "  ['x', 15, 131],\n",
       "  ['ao', 16, 133],\n",
       "  ['dia', 17, 136],\n",
       "  ['.', 18, 139]],\n",
       " [['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FA', ['há', '5', 'anos', 'por'], [',', 'uso', 'de', 'marevan']],\n",
       " ['marevan 5mg', ['FA', ',', 'uso', 'de'], ['1', 'x', 'ao', 'dia']]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formato:\n",
    "lista=list()\n",
    "numJanela=4\n",
    "for key, value in dic_sentencesTest.items():\n",
    "    #print('value:', value)\n",
    "    entidades = value[1]\n",
    "    if len(entidades)==0:\n",
    "        continue\n",
    "    for entidade in entidades:\n",
    "        #print('entidade:', entidade)\n",
    "        indiceEntidade1=entidade[1][0]\n",
    "        indiceEntidade2=entidade[1][-1]\n",
    "        vizinhosAntes = list()\n",
    "        vizinhosDepois = list()\n",
    "        #print('indiceEntidade:', indiceEntidade)\n",
    "        for tokens in value[0]:\n",
    "            indice=tokens[1]\n",
    "            #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "            if indice+4>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                vizinhosAntes.append(tokens[0])\n",
    "            if indice-4<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                vizinhosDepois.append(tokens[0])\n",
    "        lista.append([entidade[0], vizinhosAntes, vizinhosDepois])\n",
    "    break\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom itertools import chain, combinations\\n\\ndef powerset(iterable):\\n    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\\n    s = list(iterable)\\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\\n\\npowerset([1,2,3])\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "powerset([1,2,3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load obj em:  obj/../spanclassification/obj/dic_predictions_results_ner_800.pkl\n"
     ]
    }
   ],
   "source": [
    "dic_predictions_results_ner = load_obj(r'../spanclassification/obj/dic_predictions_results_ner_800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['em', 1],\n",
       "  ['tratamento', 2],\n",
       "  ['para', 3],\n",
       "  ['ICC', 4],\n",
       "  ['diastólica', 5],\n",
       "  ['há', 6],\n",
       "  ['cerca', 7],\n",
       "  ['de', 8],\n",
       "  ['5', 9],\n",
       "  ['a', 10],\n",
       "  ['com', 11],\n",
       "  ['melhora', 12],\n",
       "  ['importante', 13],\n",
       "  ['dos', 14],\n",
       "  ['sintomas', 15],\n",
       "  ['após', 16],\n",
       "  ['início', 17],\n",
       "  ['do', 18],\n",
       "  ['tratamento', 19],\n",
       "  ['.', 20]],\n",
       " [['ICC diastólica', [4, 5], 'Problema'], ['tratamento', [19], 'Tratamento']]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_results_ner[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dor'], ['no'], ['peito'], ['dor', 'no'], ['no', 'peito'], ['dor', 'no', 'peito']]\n"
     ]
    }
   ],
   "source": [
    "# para corpus GENIA, ideal pq pega as descontinuas\n",
    "# no nestedclin, as descontinuas\n",
    "def powersetGenia(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "        \n",
    "def powerset(entidade):    \n",
    "    lista=list()\n",
    "    for i in range(1, len(entidade)+1):\n",
    "        #lista.append(entidade[i-1])\n",
    "        for j in range(len(entidade) - i + 1):\n",
    "            lista.append(entidade[j:j + i])\n",
    "    return lista\n",
    "        \n",
    "#print(list(powerset([4, 5, 6])))\n",
    "print(powerset(['dor', 'no', 'peito']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samplecorpusformat = [[['POI', 'B-Abbreviation'],  ['DE', 'O'],  ['LAVAGEM', 'O'],  ['+', 'O'],  ['CURETA', 'O'],  ['DE', 'O'],  ['TECIDO', 'O'],  ['NECROTICO', 'O'],  ['.', 'O']], [['18', 'O'],  [':', 'O'],  ['00', 'O'],  [':', 'O'],  ['PACIENTE', 'O'],  ['RETORNOU', 'O'],  ['DO', 'O'],  ['CC', 'B-Abbreviation'],  ['LUCIDO', 'O'], [',', 'O'],  ['ORIENTADO', 'O'],  [',', 'O'],  ['COMUNICATIVO', 'O'],  [';', 'O'], ['MANTEM', 'O'],  ['AVP', 'B-Abbreviation'],  ['COM', 'O'],  ['STP', 'B-Abbreviation'], ['.', 'O']]]\n",
    "\n",
    "# corpus:\n",
    "# gravar os indices dos tokens\n",
    "# para cada entidade, gerar a combinação possivel de tokens e pegar as features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/embeddings/clusters_nl.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-0c908b9e840e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mword2cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/embeddings/clusters_nl.tsv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-0c908b9e840e>\u001b[0m in \u001b[0;36mread_clusters\u001b[1;34m(cluster_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mword2cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/embeddings/clusters_nl.tsv'"
     ]
    }
   ],
   "source": [
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    with open(cluster_file) as i:\n",
    "        for line in i:\n",
    "            word, cluster = line.strip().split('\\t')\n",
    "            word2cluster[word] = cluster\n",
    "    return word2cluster\n",
    "\n",
    "\n",
    "def word2features(sent, i, word2cluster):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'word.cluster=%s' % word2cluster[word.lower()] if word.lower() in word2cluster else \"0\",\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "\n",
    "    if i > 1: \n",
    "        word2 = sent[i-2][0]\n",
    "        postag2 = sent[i-2][1]\n",
    "        features.extend([\n",
    "            '-2:word.lower=' + word2.lower(),\n",
    "            '-2:word.istitle=%s' % word2.istitle(),\n",
    "            '-2:word.isupper=%s' % word2.isupper(),\n",
    "            '-2:postag=' + postag2\n",
    "        ])        \n",
    "\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word2 = sent[i+2][0]\n",
    "        postag2 = sent[i+2][1]\n",
    "        features.extend([\n",
    "            '+2:word.lower=' + word2.lower(),\n",
    "            '+2:word.istitle=%s' % word2.istitle(),\n",
    "            '+2:word.isupper=%s' % word2.isupper(),\n",
    "            '+2:postag=' + postag2\n",
    "        ])\n",
    "\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent, word2cluster):\n",
    "    return [word2features(sent, i, word2cluster) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "word2cluster = read_clusters(\"data/embeddings/clusters_nl.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
