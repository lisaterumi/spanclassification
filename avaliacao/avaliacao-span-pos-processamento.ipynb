{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload \n",
    "#from eval import predict\n",
    "import eval\n",
    "#import importlib\n",
    "#importlib.reload(module)\n",
    "from dataset import InputFeatures, load_and_cache_examples\n",
    "import dataset\n",
    "import functionsAval as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'eval' from 'C:\\\\Users\\\\lisat\\\\OneDrive\\\\jupyter notebook\\\\spanclassification\\\\avaliacao\\\\eval.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 100\n",
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Nega', 0, 422], ['sincope', 1, 427], ['.', 2, 434]], [['sincope', [1], 'Problema']]]\n",
      "numero de sentencas no total: 100\n"
     ]
    }
   ],
   "source": [
    "# em numero de frases\n",
    "#BATCH=30\n",
    "BATCH=100 \n",
    "#BATCH=800\n",
    "#BATCH=8000 \n",
    "print('BATCH:', BATCH)\n",
    "\n",
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "#print(dicSentences_new_test[0])\n",
    "print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "[[['Lucas', 0], [',', 1], ['74', 2], ['anos', 3], ['.', 4]], []]\n",
      "len(dic_predictions): 100\n"
     ]
    }
   ],
   "source": [
    "tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Abd', 0],\n",
       "  ['globoso', 1],\n",
       "  [',', 2],\n",
       "  ['flacido', 3],\n",
       "  [',', 4],\n",
       "  ['indolor', 5],\n",
       "  ['a', 6],\n",
       "  ['palpacao', 7],\n",
       "  [',', 8],\n",
       "  ['sem', 9],\n",
       "  ['VCM', 10],\n",
       "  ['.', 11]],\n",
       " [['Abd', [0], 'Anatomia'], ['VCM', [10], 'Problema']]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.save_obj('dic_predictions_results_ner_'+str(BATCH), dic_predictions)\n",
    "#all_predictions = f.load_obj('all_predictions_results_binarios_batch_'+str(BATCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 0\n",
      "[[['Lucas', 0], [',', 1], ['74', 2], ['anos', 3], ['.', 4]], []]\n",
      "key: 1\n",
      "[[['Em', 0], ['acompanhamento', 1], ['no', 2], ['ambualtorio', 3], ['há', 4], ['5', 5], ['anos', 6], ['por', 7], ['FA', 8], [',', 9], ['uso', 10], ['de', 11], ['marevan', 12], ['5mg', 13], ['1', 14], ['x', 15], ['ao', 16], ['dia', 17], ['.', 18]], [['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]]\n",
      "key: 2\n",
      "[[['Comorbidades', 0], [':', 1], ['DM', 2], ['há', 3], ['10', 4], ['anos', 5], ['em', 6], ['uso', 7], ['de', 8], ['metformina', 9], ['850mg', 10], ['3', 11], ['cp', 12], ['/', 13], ['dia', 14], [',', 15], ['acarbose', 16], ['1', 17], ['cp', 18], ['/', 19], ['dia', 20], ['e', 21], ['glicazida', 22], ['60mg', 23], ['2', 24], ['cp', 25], ['/', 26], ['dia', 27], ['e', 28], ['insulina', 29], ['(', 30], ['24', 31], ['-', 32], ['0', 33], ['-', 34], ['24', 35], [')', 36], ['.', 37]], [['Comorbidades', [0], 'Problema'], ['DM', [2], 'Problema'], ['metformina 850mg', [9, 10], 'Tratamento'], ['acarbose', [16], 'Tratamento'], ['glicazida 60mg', [22, 23], 'Tratamento'], ['insulina', [29], 'Tratamento']]]\n",
      "key: 3\n",
      "[[['HAS', 0], ['há', 1], ['15', 2], ['anos', 3], ['em', 4], ['uso', 5], ['de', 6], ['losartana', 7], ['50mg', 8], ['/', 9], ['dia', 10], ['e', 11], ['digoxina', 12], ['1', 13], ['/', 14], ['2', 15], ['cp', 16], ['/', 17], ['dia', 18], [',', 19], ['carvedilol', 20], ['25', 21], ['12', 22], ['/', 23], ['12', 24], [',', 25], ['HCTZ', 26], ['.', 27]], [['HAS', [0], 'Problema'], ['losartana 50mg', [7, 8], 'Tratamento'], ['digoxina', [12], 'Tratamento'], ['carvedilol 25', [20, 21], 'Tratamento'], ['HCTZ', [26], 'Tratamento']]]\n",
      "key: 4\n",
      "[[['DSLP', 0], ['em', 1], ['uso', 2], ['de', 3], ['sinvastatina', 4], [',', 5], ['marevan', 6], ['1', 7], ['cp', 8], ['/', 9], ['dia', 10], ['seg', 11], ['-', 12], ['sab', 13], ['para', 14], ['no', 15], ['alvo', 16], ['sic', 17], ['.', 18]], [['DSLP', [0], 'Problema'], ['sinvastatina', [4], 'Tratamento'], ['marevan', [6], 'Tratamento']]]\n"
     ]
    }
   ],
   "source": [
    "for key, value in dic_predictions.items():\n",
    "    print('key:',key)\n",
    "    print(dic_predictions[key])\n",
    "    if key>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FA', [8], 'Problema'], ['marevan 5mg', [12, 13], 'Tratamento']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dicSentences_new_test))\n",
    "print(len(dic_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema', 'Tratamento', 'Problema', 'Problema']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_pred_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema', 'Tratamento', 'Problema', 'Problema']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_true_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 13, 13, 14, 15, 15, 15]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_erros[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(lista_erros))\n",
    "print(len(set(lista_erros)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['BC', 0], ['arritmicas', 1], [',', 2], ['NF', 3], ['SS', 4], ['2T', 5], ['.', 6]], [['SS', [4], 'Problema']]]\n",
      "[['BC arritmicas', [0, 1], 'Problema'], ['SS', [4], 'Problema']]\n"
     ]
    }
   ],
   "source": [
    "print(dic_predictions[8])\n",
    "print(dicSentences_new_test[8][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Exames', 0], ['-', 1], ['Holter', 2], [':', 3], ['FC', 4], ['controlada', 5], [',', 6], ['media', 7], ['92', 8], ['.', 9]], [['Exames', [0], 'Teste'], ['Holter', [2], 'Teste']]]\n",
      "[['Exames', [0], 'Teste'], ['Holter', [2], 'Teste']]\n"
     ]
    }
   ],
   "source": [
    "print(dic_predictions[11])\n",
    "print(dicSentences_new_test[11][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Ecocardiograma', 0], ['-', 1], ['ventrículo', 2], ['esquerdo', 3], ['com', 4], ['hipertrofia', 5], ['concentrica', 6], ['de', 7], ['grau', 8], ['discreto', 9], ['e', 10], ['função', 11], ['sistólica', 12], ['preservada', 13], ['.', 14]], [['Ecocardiograma', [0], 'Teste'], ['ventrículo esquerdo', [2, 3], 'Anatomia'], ['hipertrofia concentrica de grau discreto', [5, 6, 7, 8, 9], 'Problema']]]\n",
      "[['Ecocardiograma', [0], 'Teste'], ['ventrículo esquerdo com hipertrofia concentrica de grau discreto', [2, 3, 4, 5, 6, 7, 8, 9], 'Problema'], ['ventrículo esquerdo', [2, 3], 'Anatomia']]\n"
     ]
    }
   ],
   "source": [
    "print(dic_predictions[13])\n",
    "print(dicSentences_new_test[13][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(region_true_list): 352\n",
      "len(region_pred_list): 352\n",
      "-----Avaliando só modelo de NER:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.789474  0.277778  0.410959        54\n",
      "           O   0.000000  0.000000  0.000000        38\n",
      "    Problema   0.853211  0.823009  0.837838       113\n",
      "       Teste   0.880952  0.891566  0.886228        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.667614       352\n",
      "   macro avg   0.670352  0.564096  0.592630       352\n",
      "weighted avg   0.753305  0.667614  0.691546       352\n",
      "\n",
      "[[15 36  1  1  1]\n",
      " [ 4  0 15  9 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "print('len(region_true_list):', len(region_true_list))\n",
    "print('len(region_pred_list):', len(region_pred_list))\n",
    "#print('pred:',region_pred_list[:15])\n",
    "#print('true:',region_true_list[:15])\n",
    "\n",
    "print('-----Avaliando só modelo de NER:-----')\n",
    "\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, modelo de SpanClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando só com modelo de Spam:-----\n",
      "loading model from C:\\Users\\lisat\\OneDrive\\jupyter notebook\\span-model\\model-exp3\n",
      "num_labels: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\lisat\\OneDrive\\jupyter notebook\\span-model\\model-exp3 were not used when initializing BertForChunkClassification: ['classifier.0.weight', 'classifier.0.bias']\n",
      "- This IS expected if you are initializing BertForChunkClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForChunkClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForChunkClassification were not initialized from the model checkpoint at C:\\Users\\lisat\\OneDrive\\jupyter notebook\\span-model\\model-exp3 and are newly initialized: ['classifier.1.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print('-----Avaliando só com modelo de Spam:-----')\n",
    "model = f.getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTrainDev = f.load_obj('dic_sentencesTrainDev')\n",
    "dicPosTagger, _ = f.getDicPosTagger(dic_sentencesTrainDev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'eval' from 'C:\\\\Users\\\\lisat\\\\OneDrive\\\\jupyter notebook\\\\spanclassification\\\\avaliacao\\\\eval.py'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_postaggers_entidades = f.getListaPostaggerEntidades(dic_predictions, dicPosTagger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FA', [8], 'Problema']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para gerar arquivo de predicoes (com as tags <e1>)\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesAllPosProc(dic_predictions, dicPosTagger, lista_postaggers_entidades, True)\n",
    "print(combinacaoEntidadesAll[3])\n",
    "f.gravaArquivoPredict(combinacaoEntidadesAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesAllPosProc(dic_predictions, dicPosTagger, lista_postaggers_entidades, filtroPostagger=True):\n",
    "    #  filtro (pos-processamento)\n",
    "    palavrasDescontinua_underline=['AND_NOT', 'AND/OR', 'AS_WELL_AS', 'AND', 'OR', 'BUT_NOT', 'NEITHER_NOR', 'THAN']\n",
    "    num=0\n",
    "    numDescontinuas=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    numEntidadesRetiradas=0\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        #print('key:', key)\n",
    "        #print(value)\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        #print(so_tokens)\n",
    "        #print(entidades)\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            #print(entidade[1])\n",
    "            texto_entidade=entidade[0]\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            if areConsecutive(indices): # ver se não é descontinua\n",
    "                #print(entidade[1])\n",
    "                #print(frase)\n",
    "                frase = so_tokens.copy()\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                frase.insert(inicio, '<e1>')\n",
    "                frase.insert(fim+2, '</e1>')\n",
    "                if ' '.join(texto_entidade).strip()=='-' or ' '.join(texto_entidade).strip()=='=' or ' '.join(texto_entidade).strip()=='+' or ' '.join(texto_entidade).strip()==':' or ' '.join(texto_entidade).strip()==',' or ' '.join(texto_entidade).strip()==\"'\" or ' '.join(texto_entidade).strip()=='\"' or ' '.join(texto_entidade).strip()=='.' or ' '.join(texto_entidade).strip()==';' or ' '.join(texto_entidade).strip()=='/' or ' '.join(texto_entidade).strip()=='(' or ' '.join(texto_entidade).strip()==')' or ' '.join(texto_entidade).strip()=='[' or ' '.join(texto_entidade).strip()==']'or EntidadeUmaLetra(texto_entidade):\n",
    "                    numEntidadesRetiradas=numEntidadesRetiradas+1\n",
    "                    pass\n",
    "                #print('--texto_entidade--:', texto_entidade)\n",
    "                #print('frase:', frase)\n",
    "                #print('frase[inicio+1:fim+2]:', frase[inicio+1:fim+2])\n",
    "                #print(tokens[indice])\n",
    "                # o corpus tem alguns problemas, ex tem entidade descontinua sem o CC\n",
    "                # entao aqui, se não bater, não adicionar no arquivo de treinamento\n",
    "                # ex frase \"The inhibition of c - fos and c - jun expression by IL - 4 in LPS - treated cells was shown to be due to a lower transcription rate of the c - fos and c - jun genes .\"\n",
    "                #print(\"texto_entidade:\", texto_entidade)\n",
    "                #print(\"frase[inicio:fim]:\", frase[inicio+1:fim+2])\n",
    "                texto_entidade_comparar=' '.join(texto_entidade).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "                texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "                texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "                texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "                #tem = [palavra.lower() in texto_entidade_comparar for palavra in palavrasDescontinua]\n",
    "                #tem = [' '+palavra.lower()+' ' in ' '+texto_entidade_comparar+' ' for palavra in palavrasDescontinua]\n",
    "                #texto_frase_comparar2 = texto_frase_comparar.replace('and not', 'and_not').replace('as well as', 'as_well_as').replace('neither nor', 'neither_nor').replace('but not', 'but_not')\n",
    "                #print(frase)\n",
    "                #print('texto_frase_comparar2.split():', texto_frase_comparar2.split())\n",
    "                #print('texto_frase_comparar2.split():', texto_frase_comparar2.split()[0])\n",
    "                #tem = [texto_frase_comparar2.split()[-1]==palavra.lower() for palavra in palavrasDescontinua_underline]\n",
    "                #tem = [' '+palavra.lower()+' ' in ' '+texto_frase_comparar2+' ' for palavra in palavrasDescontinua_underline]\n",
    "                #tem_boolean = True in tem\n",
    "                #print([' '.join(frase).strip(), tipo_entidade])\n",
    "                #if not tem_boolean:\n",
    "                #if 1==2:\n",
    "                #    combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "                texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "                texto_frase_comparar = texto_frase_comparar.lower()\n",
    "                if ((texto_entidade_comparar == texto_frase_comparar) or (texto_entidade_comparar+'s' == texto_frase_comparar) or (texto_entidade_comparar+'es' == texto_frase_comparar) or (texto_entidade_comparar+'ies' == texto_frase_comparar) or (texto_entidade_comparar[:-1]+'ies' == texto_frase_comparar) or (texto_entidade_comparar[:-1] == texto_frase_comparar.replace(' /','/'))):\n",
    "                    combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "                else:\n",
    "                    #print('tem:', tem)\n",
    "                    #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "                    #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "                    #print('texto_frase_comparar2.split()[-1]:', texto_frase_comparar2.split()[-1])\n",
    "                    #print(\"' '.join(texto_entidade).strip()):\", ' '.join(texto_entidade).strip())\n",
    "                    #print(\"' '.join(frase[inicio:fim].strip():\", ' '.join(frase[inicio+1:fim+2]).strip())\n",
    "                    erro_corpus=erro_corpus+1\n",
    "                    erros_entidade.append(indices)\n",
    "                    #print('-----indices erro:----', indices)\n",
    "                    lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "            else: # agora, qdo são descontinuas\n",
    "                numDescontinuas=numDescontinuas+1\n",
    "                #print(entidade[1])\n",
    "                #frase = so_tokens.copy()\n",
    "                #inicio=indices[0]\n",
    "                #fim=indices[-1]\n",
    "                #frase.insert(inicio, '<e1>')\n",
    "                #frase.insert(fim+2, '</e1>')\n",
    "\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+2]\n",
    "                        frase.insert(indice, '<e1>')\n",
    "                        #print(i)\n",
    "\n",
    "                        frase.insert(i+2, '</e1>')\n",
    "                        frase_string=' '.join(frase).strip()\n",
    "                        #print(frase_string)\n",
    "                        #if frase_string not in combinacaoEntidades:\n",
    "                        # ver se frase não termina com pontuacao ('.',',',';','-',')','(',']','[','/','\"',)\n",
    "                        devePular = 0\n",
    "                        if '. </e1>' in frase_string or ', </e1>' in frase_string  or '; </e1>' in frase_string or '- </e1>' in frase_string  or ': </e1>' in frase_string  or '= </e1>' in frase_string  or '/ </e1>' in frase_string  or '( </e1>' in frase_string  or ') </e1>' in frase_string  or '[ </e1>' in frase_string  or '] </e1>' in frase_string  or ': </e1>' in frase_string or 'and </e1>' in frase_string or 'or </e1>' in frase_string:\n",
    "                            devePular=1\n",
    "                        if '<e1> .' in frase_string or '<e1> ,' in frase_string  or '<e1> ;' in frase_string or '<e1> -' in frase_string  or '<e1> :' in frase_string  or '<e1> =' in frase_string  or '<e1> /' in frase_string  or '<e1> (' in frase_string  or '<e1> )' in frase_string  or '<e1> [' in frase_string  or '<e1> ]' in frase_string  or '<e1> :' in frase_string  or '<e1> and' in frase_string  or '<e1> or' in frase_string:\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> [0-9]* </e1>\", frase_string):\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> . </e1>\", frase_string):# só uma letra ou numero\n",
    "                            devePular=1\n",
    "                        if '</e1> factor' in frase_string or '</e1> receptor' in frase_string or '</e1> site' in frase_string or '</e1> cell line' in frase_string or '</e1> region' in frase_string or '</e1> cell' in frase_string or '</e1> enhancer' in frase_string or '</e1> element' in frase_string or '</e1> protein' in frase_string or '</e1> gene' in frase_string:\n",
    "                            devePular=1\n",
    "                            #print('aaaaaaaaa:', frase_string)\n",
    "                        if (filtroPostagger):\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "\n",
    "                        tem_frase = 0\n",
    "                        for frase in combinacaoEntidades:\n",
    "                            if frase[0] == frase_string:\n",
    "                                tem_frase=''\n",
    "                                break\n",
    "                        if devePular==1:\n",
    "                            numEntidadesRetiradas=numEntidadesRetiradas+1\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                            #print('inserindo:', frase_string)\n",
    "                            #print('indice:', indice)\n",
    "                            combinacaoEntidades.append([frase_string, 'O'])\n",
    "        # shuffle no combinacaoEntidades\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "            #print(\"key sem entidade\", key)\n",
    "            #print('len(combinacaoEntidades):', len(combinacaoEntidades))\n",
    "            #print(so_tokens)\n",
    "        combinacaoEntidades = list()\n",
    "        if (num % 1000) ==0:\n",
    "        #if (num %3) == 0:\n",
    "            print('key:', key)\n",
    "            #break\n",
    "    print('numDescontinuas:', numDescontinuas)\n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('numEntidadesRetiradas:', numEntidadesRetiradas)\n",
    "    return combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidades(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sem taxa de Downsampling')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+2]\n",
    "                        frase.insert(indice, '<e1>')\n",
    "                        frase.insert(i+2, '</e1>')\n",
    "                        frase_string=' '.join(frase).strip()\n",
    "                        devePular = 0\n",
    "                        if '. </e1>' in frase_string or ', </e1>' in frase_string  or '; </e1>' in frase_string or '- </e1>' in frase_string  or ': </e1>' in frase_string  or '= </e1>' in frase_string  or '/ </e1>' in frase_string  or '( </e1>' in frase_string  or ') </e1>' in frase_string  or '[ </e1>' in frase_string  or '] </e1>' in frase_string  or ': </e1>' in frase_string or 'and </e1>' in frase_string or 'or </e1>' in frase_string:\n",
    "                            devePular=1\n",
    "                        if '<e1> .' in frase_string or '<e1> ,' in frase_string  or '<e1> ;' in frase_string or '<e1> -' in frase_string  or '<e1> :' in frase_string  or '<e1> =' in frase_string  or '<e1> /' in frase_string  or '<e1> (' in frase_string  or '<e1> )' in frase_string  or '<e1> [' in frase_string  or '<e1> ]' in frase_string  or '<e1> :' in frase_string  or '<e1> and' in frase_string  or '<e1> or' in frase_string:\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> [0-9]* </e1>\", frase_string):\n",
    "                            devePular=1\n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase in combinacaoEntidades:\n",
    "                            if frase[0] == frase_string:\n",
    "                                tem_frase=''\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                            combinacaoEntidades.append([frase_string, 'O'])\n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidades = combinacaoEntidades[:(num_positivas*taxaDownsampling)+num_positivas]\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    print('len(pulando_termos_postagger):', len(pulando_termos_postagger))\n",
    "    \n",
    "    return combinacaoEntidadesAll, pulando_termos_postagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntar all_predictions\n",
    "dic_predictions = {}\n",
    "for num, prediction in enumerate(all_predictions):\n",
    "    if num==0:\n",
    "        dic_predictions = prediction.copy()\n",
    "        continue\n",
    "    for key, value in prediction.items():\n",
    "        entidadesJaEstavam=dic_predictions[key][1]\n",
    "        tokens=dic_predictions[key][0]\n",
    "        lista_entidade = [e for e in entidadesJaEstavam]\n",
    "        #print('lista_entidade:', lista_entidade)\n",
    "        entidades = value[1].copy()\n",
    "        if len(entidades)>0:\n",
    "            #print(entidades)\n",
    "            for entidade in entidades:\n",
    "                lista_entidade.append(entidade)\n",
    "            dic_predictions[key]=[tokens,lista_entidade]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chamar predict                \n",
    "pred_region_labels = f.predictSpan(model)\n",
    "print('len(pred_region_labels):', len(pred_region_labels))\n",
    "\n",
    "f.save_obj('combinacaoEntidadesAll_io_bkp', combinacaoEntidadesAll)\n",
    "#combinacaoEntidadesAll = load_obj('combinacaoEntidadesAll_io_bkp')\n",
    "\n",
    "# combinacaoEntidadesAll_pred = predicoes do SpanClassification no formato para comparar com gabarito\n",
    "combinacaoEntidadesAll_pred = f.getCombinacaoEntidadesAll_pred(combinacaoEntidadesAll, pred_region_labels)\n",
    "\n",
    "dic_predictions_span = f.getDicPredictionsSpan(combinacaoEntidadesAll_pred, dic_predictions)\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_span, BATCH)\n",
    "\n",
    "\n",
    "# tbm sem o postagger pra ver\n",
    "\n",
    "print('-----Avaliando os dois modelos juntos:-----')\n",
    "\n",
    "# dic_predictions_all = mesclando o resultado dos dois modelos (dic_predictions + combinacaoEntidadesAll_pred)\n",
    "dic_predictions_all = f.getDicPredictionsAll(combinacaoEntidadesAll_pred, dic_predictions)\n",
    "f.save_obj('dic_predictions_all_io_bkp', dic_predictions_all)\n",
    "#dic_predictions_all = load_obj('dic_predictions_all_io_bkp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
