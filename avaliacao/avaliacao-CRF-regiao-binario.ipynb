{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "import model as mod\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload \n",
    "#from eval import predict\n",
    "import eval\n",
    "#import importlib\n",
    "#importlib.reload(module)\n",
    "import dataset\n",
    "from dataset import InputFeatures, load_and_cache_examples\n",
    "import functionsAval as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 506\n"
     ]
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n",
    "reload(mod)\n",
    "\n",
    "# em numero de frases\n",
    "BATCH=506\n",
    "#BATCH=5\n",
    "#BATCH=800\n",
    "#BATCH=8000 \n",
    "print('BATCH:', BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 506\n",
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "[[['Lucas', 0], [',', 1], ['74', 2], ['anos', 3], ['.', 4]], []]\n",
      "len(dic_predictions): 506\n",
      "verificando dados:\n",
      "len(dicSentences_new_test): 506\n",
      "len(dic_predictions): 506\n",
      "region_pred_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "region_true_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "lista_erros[:8]: [7, 8, 13, 13, 14, 15, 15, 15]\n",
      "len(lista_erros): 383\n",
      "len(set(lista_erros)): 169\n",
      "len(region_true_list): 1140\n",
      "len(region_pred_list): 1140\n",
      "-----Avaliando só modelo de NER:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.829268  0.346939  0.489209       196\n",
      "           O   0.000000  0.000000  0.000000       149\n",
      "    Problema   0.798319  0.843195  0.820144       338\n",
      "       Teste   0.878049  0.888889  0.883436       243\n",
      "  Tratamento   0.800905  0.827103  0.813793       214\n",
      "\n",
      "    accuracy                       0.654386      1140\n",
      "   macro avg   0.661308  0.581225  0.601316      1140\n",
      "weighted avg   0.716779  0.654386  0.668351      1140\n",
      "\n",
      "[[ 68 122   4   1   1]\n",
      " [ 14   0  64  29  42]\n",
      " [  0  53 285   0   0]\n",
      " [  0  26   0 216   1]\n",
      " [  0  33   4   0 177]]\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n",
    "tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "#print(dic_predictions[9])\n",
    "f.save_obj('dic_predictions_results_ner_'+str(BATCH), dic_predictions)\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "print('verificando dados:')\n",
    "#for key, value in dic_predictions.items():\n",
    "#    print('key:',key)\n",
    "#    print(dic_predictions[key])\n",
    "#    if key>2:\n",
    "#        break\n",
    "        \n",
    "print('len(dicSentences_new_test):', len(dicSentences_new_test))\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "f.save_obj('region_true_list'+str(BATCH), region_true_list)\n",
    "print('region_pred_list[:4]:', region_pred_list[:4])\n",
    "print('region_true_list[:4]:', region_true_list[:4])\n",
    "print('lista_erros[:8]:', lista_erros[:8])\n",
    "print('len(lista_erros):', len(lista_erros))\n",
    "print('len(set(lista_erros)):', len(set(lista_erros)))\n",
    "#print(dic_predictions[8])\n",
    "#print(dicSentences_new_test[8][1])\n",
    "print('len(region_true_list):', len(region_true_list))\n",
    "print('len(region_pred_list):', len(region_pred_list))\n",
    "#print('pred:',region_pred_list[:15])\n",
    "#print('true:',region_true_list[:15])\n",
    "\n",
    "print('-----Avaliando só modelo de NER:-----')\n",
    "\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lucas', ',', '74', 'anos', '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF primeiro nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPostagger = f.load_obj('dic_postagger')\n",
    "def tipoPostaggerTokens(token, dicPostagger):\n",
    "    postagger = 'N' # na duvida é N\n",
    "    if token.lower() in dicPostagger.keys():\n",
    "        postagger = dicPostagger.get(token.lower())\n",
    "    return postagger\n",
    "tipoPostaggerTokens('coração', dicPostagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lisat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "def has_numbers(inputString):\n",
    "    return str(bool(re.search(r'\\d', inputString)))\n",
    "\n",
    "print(has_numbers('metionina 5mg'))\n",
    "\n",
    "def isstopword(p):\n",
    "  return str(p in stopwords.words('portuguese'))\n",
    "\n",
    "print(isstopword('pelo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binario\n",
    "\n",
    "CLUSTER=300\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['O','ENT']\n",
    "\n",
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    try:\n",
    "        with open(cluster_file, encoding='utf-8') as i:\n",
    "            for num, line in enumerate(i):\n",
    "                if line:\n",
    "                    word, cluster = line.strip().split('\\t')\n",
    "                    word2cluster[word] = cluster\n",
    "    except:\n",
    "        raise\n",
    "    return word2cluster\n",
    "\n",
    "def word2features(sent, i):\n",
    "  #if str(sent[i][0])!='nan':#esto es extra\n",
    "    word = sent[i][0]\n",
    "    postag = tipoPostaggerTokens(word, dicPostagger)\n",
    "    cluster = word2cluster[word.lower()] if word.lower() in word2cluster else \"0\"\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'word.cluster': cluster\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i > 1 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-2][0]\n",
    "        #postag1 = sent[i-2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:word.istitle()': word1.istitle(),\n",
    "            '-2:word.isupper()': word1.isupper(),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_word'] = True\n",
    "\n",
    "    if i > 2 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-1][0]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-3:word.lower()': word1.lower(),\n",
    "            '-3:word.istitle()': word1.istitle(),\n",
    "            '-3:word.isupper()': word1.isupper(),\n",
    "            '-3:postag': postag1,\n",
    "            '-3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_word'] = True\n",
    "    \n",
    "    if i > 3 and JANELA==4:\n",
    "        word1 = sent[i-2][0]\n",
    "        #postag1 = sent[i-2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-4:word.lower()': word1.lower(),\n",
    "            '-4:word.istitle()': word1.istitle(),\n",
    "            '-4:word.isupper()': word1.isupper(),\n",
    "            '-4:postag': postag1,\n",
    "            '-4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_word'] = True\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        #postag1 = sent[i+1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    if i < len(sent)-2 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+2][0]\n",
    "        #postag1 = sent[i+2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:word.istitle()': word1.istitle(),\n",
    "            '+2:word.isupper()': word1.isupper(),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_to_last'] = True\n",
    "    if i < len(sent)-3 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+3][0]\n",
    "        #postag1 = sent[i+3][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+3:word.lower()': word1.lower(),\n",
    "            '+3:word.istitle()': word1.istitle(),\n",
    "            '+3:word.isupper()': word1.isupper(),\n",
    "            '+3:postag': postag1,\n",
    "            '+3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_to_last'] = True\n",
    "    \n",
    "    if i < len(sent)-4 and JANELA==4:\n",
    "        word1 = sent[i+4][0]\n",
    "        #postag1 = sent[i+4][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+4:word.lower()': word1.lower(),\n",
    "            '+4:word.istitle()': word1.istitle(),\n",
    "            '+4:word.isupper()': word1.isupper(),\n",
    "            '+4:postag': postag1,\n",
    "            '+4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_to_last'] = True\n",
    "    \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "# gen = (x for x in xyz if x not in a)\n",
    "def sent2labels(sent):\n",
    "    retorno = list()\n",
    "    for token, label in sent:\n",
    "      if label =='O':\n",
    "        retorno.append(label)\n",
    "      else:\n",
    "        retorno.append('ENT')\n",
    "    return retorno\n",
    "    #return ['ENT' if label!='O' elif 'O' for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "word2cluster = read_clusters(r\"cluster/cluster-\"+str(CLUSTER)+\".tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTest=r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\data-ner/nested_test.conll'\n",
    "\n",
    "with open(pathTest, encoding='utf-8') as fi:\n",
    "  testdata = [[tuple(w.split(' ')) for w in snt.split('\\n')] for snt in fi.read().split('\\n\\n')]\n",
    "\n",
    "X_test = [sent2features(s) for s in testdata]\n",
    "y_test = [sent2labels(s) for s in testdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'lucas',\n",
       "  'word[-3:]': 'cas',\n",
       "  'word[:3]': 'Luc',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '0',\n",
       "  'BOS': True,\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '22',\n",
       "  '+2:word.lower()': '74',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '299',\n",
       "  '+3:word.lower()': 'anos',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '134',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[:3]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '22',\n",
       "  '-1:word.lower()': 'lucas',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '0',\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '74',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '299',\n",
       "  '+2:word.lower()': 'anos',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '134',\n",
       "  '+3:word.lower()': '.',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '153',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '74',\n",
       "  'word[-3:]': '74',\n",
       "  'word[:3]': '74',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '299',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PU',\n",
       "  '-1:word.cluster': '22',\n",
       "  '-2:word.lower()': 'lucas',\n",
       "  '-2:word.istitle()': True,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '0',\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'anos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '134',\n",
       "  '+2:word.lower()': '.',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '153',\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'anos',\n",
       "  'word[-3:]': 'nos',\n",
       "  'word[:3]': 'ano',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '134',\n",
       "  '-1:word.lower()': '74',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NUM',\n",
       "  '-1:word.cluster': '299',\n",
       "  '-2:word.lower()': ',',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PU',\n",
       "  '-2:word.cluster': '22',\n",
       "  '-3:word.lower()': '74',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '299',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '153',\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[:3]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '153',\n",
       "  '-1:word.lower()': 'anos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '134',\n",
       "  '-2:word.lower()': '74',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '299',\n",
       "  '-3:word.lower()': 'anos',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '134',\n",
       "  'Fourth_word': True,\n",
       "  'EOS': True,\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True}]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binario flat, clustr 300, janela 3\n",
      "F1 weighted: 0.9404560516998399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.959     0.956     0.957      3809\n",
      "         ENT      0.898     0.905     0.901      1644\n",
      "\n",
      "    accuracy                          0.940      5453\n",
      "   macro avg      0.929     0.930     0.929      5453\n",
      "weighted avg      0.941     0.940     0.940      5453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#AVAL no conjunto de teste do ner (flat)\n",
    "\n",
    "#def getTiposEntidade():\n",
    "    #return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "print('Binario flat, clustr 300, janela 3')\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel_binario\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "\n",
    "finalScore = flat_f1_score(y_test, y_pred, average='weighted', labels=getTiposEntidade())\n",
    "print(\"F1 weighted:\",finalScore)\n",
    "\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    try:\n",
    "        with open(cluster_file, encoding='utf-8') as i:\n",
    "            for num, line in enumerate(i):\n",
    "                if line:\n",
    "                    word, cluster = line.strip().split('\\t')\n",
    "                    word2cluster[word] = cluster\n",
    "    except:\n",
    "        raise\n",
    "    return word2cluster\n",
    "\n",
    "def word2features(sent, i):\n",
    "  #if str(sent[i][0])!='nan':#esto es extra\n",
    "    word = sent[i][0]\n",
    "    postag = tipoPostaggerTokens(word, dicPostagger)\n",
    "    cluster = word2cluster[word.lower()] if word.lower() in word2cluster else \"0\"\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'word.cluster': cluster\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i > 1 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-2][0]\n",
    "        postag1 = sent[i-2][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:word.istitle()': word1.istitle(),\n",
    "            '-2:word.isupper()': word1.isupper(),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_word'] = True\n",
    "\n",
    "    if i > 2 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-3:word.lower()': word1.lower(),\n",
    "            '-3:word.istitle()': word1.istitle(),\n",
    "            '-3:word.isupper()': word1.isupper(),\n",
    "            '-3:postag': postag1,\n",
    "            '-3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_word'] = True\n",
    "    \n",
    "    if i > 3 and JANELA==4:\n",
    "        word1 = sent[i-2][0]\n",
    "        postag1 = sent[i-2][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-4:word.lower()': word1.lower(),\n",
    "            '-4:word.istitle()': word1.istitle(),\n",
    "            '-4:word.isupper()': word1.isupper(),\n",
    "            '-4:postag': postag1,\n",
    "            '-4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_word'] = True\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    if i < len(sent)-2 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+2][0]\n",
    "        postag1 = sent[i+2][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:word.istitle()': word1.istitle(),\n",
    "            '+2:word.isupper()': word1.isupper(),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_to_last'] = True\n",
    "    if i < len(sent)-3 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+3][0]\n",
    "        postag1 = sent[i+3][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+3:word.lower()': word1.lower(),\n",
    "            '+3:word.istitle()': word1.istitle(),\n",
    "            '+3:word.isupper()': word1.isupper(),\n",
    "            '+3:postag': postag1,\n",
    "            '+3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_to_last'] = True\n",
    "    \n",
    "    if i < len(sent)-4 and JANELA==4:\n",
    "        word1 = sent[i+4][0]\n",
    "        postag1 = sent[i+4][1]\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+4:word.lower()': word1.lower(),\n",
    "            '+4:word.istitle()': word1.istitle(),\n",
    "            '+4:word.isupper()': word1.isupper(),\n",
    "            '+4:postag': postag1,\n",
    "            '+4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_to_last'] = True\n",
    "    \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "#word2cluster = read_clusters(r\"cluster/cluster-\"+str(CLUSTER)+\".tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abdomen', 'Anatomia'),\n",
       " ('flácido', 'O'),\n",
       " (',', 'O'),\n",
       " ('indolor', 'O'),\n",
       " (',', 'O'),\n",
       " ('sem', 'O'),\n",
       " ('visceromegalias', 'Problema'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'ENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'ENT',\n",
       " 'ENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'em',\n",
       "  'word[-3:]': 'Em',\n",
       "  'word[:3]': 'Em',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '164',\n",
       "  'BOS': True,\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'acompanhamento',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '34',\n",
       "  '+2:word.lower()': 'no',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'ART',\n",
       "  '+2:word.cluster': '164',\n",
       "  '+3:word.lower()': 'ambualtorio',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'acompanhamento',\n",
       "  'word[-3:]': 'nto',\n",
       "  'word[:3]': 'aco',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '34',\n",
       "  '-1:word.lower()': 'em',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PREP',\n",
       "  '-1:word.cluster': '164',\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'no',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'ART',\n",
       "  '+1:word.cluster': '164',\n",
       "  '+2:word.lower()': 'ambualtorio',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'há',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'V',\n",
       "  '+3:word.cluster': '164',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-3:]': 'no',\n",
       "  'word[:3]': 'no',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'ART',\n",
       "  'word.cluster': '164',\n",
       "  '-1:word.lower()': 'acompanhamento',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '34',\n",
       "  '-2:word.lower()': 'em',\n",
       "  '-2:word.istitle()': True,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '164',\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ambualtorio',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'há',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'V',\n",
       "  '+2:word.cluster': '164',\n",
       "  '+3:word.lower()': '5',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '16',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ambualtorio',\n",
       "  'word[-3:]': 'rio',\n",
       "  'word[:3]': 'amb',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'ART',\n",
       "  '-1:word.cluster': '164',\n",
       "  '-2:word.lower()': 'acompanhamento',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '34',\n",
       "  '-3:word.lower()': 'no',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'ART',\n",
       "  '-3:word.cluster': '164',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'há',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'V',\n",
       "  '+1:word.cluster': '164',\n",
       "  '+2:word.lower()': '5',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '16',\n",
       "  '+3:word.lower()': 'anos',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '134',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'há',\n",
       "  'word[-3:]': 'há',\n",
       "  'word[:3]': 'há',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'V',\n",
       "  'word.cluster': '164',\n",
       "  '-1:word.lower()': 'ambualtorio',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': 'no',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'ART',\n",
       "  '-2:word.cluster': '164',\n",
       "  '-3:word.lower()': 'ambualtorio',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '16',\n",
       "  '+2:word.lower()': 'anos',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '134',\n",
       "  '+3:word.lower()': 'por',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '164',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5',\n",
       "  'word[-3:]': '5',\n",
       "  'word[:3]': '5',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '16',\n",
       "  '-1:word.lower()': 'há',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'V',\n",
       "  '-1:word.cluster': '164',\n",
       "  '-2:word.lower()': 'ambualtorio',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'há',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'V',\n",
       "  '-3:word.cluster': '164',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'anos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '134',\n",
       "  '+2:word.lower()': 'por',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '164',\n",
       "  '+3:word.lower()': 'fa',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': True,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '38',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'anos',\n",
       "  'word[-3:]': 'nos',\n",
       "  'word[:3]': 'ano',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '134',\n",
       "  '-1:word.lower()': '5',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NUM',\n",
       "  '-1:word.cluster': '16',\n",
       "  '-2:word.lower()': 'há',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'V',\n",
       "  '-2:word.cluster': '164',\n",
       "  '-3:word.lower()': '5',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '16',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'por',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '164',\n",
       "  '+2:word.lower()': 'fa',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': True,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '38',\n",
       "  '+3:word.lower()': ',',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '22',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'por',\n",
       "  'word[-3:]': 'por',\n",
       "  'word[:3]': 'por',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '164',\n",
       "  '-1:word.lower()': 'anos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '134',\n",
       "  '-2:word.lower()': '5',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '16',\n",
       "  '-3:word.lower()': 'anos',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '134',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'fa',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '38',\n",
       "  '+2:word.lower()': ',',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '22',\n",
       "  '+3:word.lower()': 'uso',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '63',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'fa',\n",
       "  'word[-3:]': 'FA',\n",
       "  'word[:3]': 'FA',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '38',\n",
       "  '-1:word.lower()': 'por',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PREP',\n",
       "  '-1:word.cluster': '164',\n",
       "  '-2:word.lower()': 'anos',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '134',\n",
       "  '-3:word.lower()': 'por',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '164',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '22',\n",
       "  '+2:word.lower()': 'uso',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '63',\n",
       "  '+3:word.lower()': 'de',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '91',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[:3]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '22',\n",
       "  '-1:word.lower()': 'fa',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '38',\n",
       "  '-2:word.lower()': 'por',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '164',\n",
       "  '-3:word.lower()': 'fa',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': True,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '38',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'uso',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '63',\n",
       "  '+2:word.lower()': 'de',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '91',\n",
       "  '+3:word.lower()': 'marevan',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '5',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'uso',\n",
       "  'word[-3:]': 'uso',\n",
       "  'word[:3]': 'uso',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '63',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PU',\n",
       "  '-1:word.cluster': '22',\n",
       "  '-2:word.lower()': 'fa',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': True,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '38',\n",
       "  '-3:word.lower()': ',',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PU',\n",
       "  '-3:word.cluster': '22',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '91',\n",
       "  '+2:word.lower()': 'marevan',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '5',\n",
       "  '+3:word.lower()': '5mg',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '273',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word[:3]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '91',\n",
       "  '-1:word.lower()': 'uso',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '63',\n",
       "  '-2:word.lower()': ',',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PU',\n",
       "  '-2:word.cluster': '22',\n",
       "  '-3:word.lower()': 'uso',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '63',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'marevan',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '5',\n",
       "  '+2:word.lower()': '5mg',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '273',\n",
       "  '+3:word.lower()': '1',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '267',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'marevan',\n",
       "  'word[-3:]': 'van',\n",
       "  'word[:3]': 'mar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '5',\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PREP',\n",
       "  '-1:word.cluster': '91',\n",
       "  '-2:word.lower()': 'uso',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '63',\n",
       "  '-3:word.lower()': 'de',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '91',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5mg',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '273',\n",
       "  '+2:word.lower()': '1',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '267',\n",
       "  '+3:word.lower()': 'x',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '23',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5mg',\n",
       "  'word[-3:]': '5mg',\n",
       "  'word[:3]': '5mg',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '273',\n",
       "  '-1:word.lower()': 'marevan',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '5',\n",
       "  '-2:word.lower()': 'de',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '91',\n",
       "  '-3:word.lower()': 'marevan',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '5',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '1',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '267',\n",
       "  '+2:word.lower()': 'x',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '23',\n",
       "  '+3:word.lower()': 'ao',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '164',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '1',\n",
       "  'word[-3:]': '1',\n",
       "  'word[:3]': '1',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '267',\n",
       "  '-1:word.lower()': '5mg',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '273',\n",
       "  '-2:word.lower()': 'marevan',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '5',\n",
       "  '-3:word.lower()': '5mg',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '273',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'x',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '23',\n",
       "  '+2:word.lower()': 'ao',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '164',\n",
       "  '+3:word.lower()': 'dia',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '31',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'x',\n",
       "  'word[-3:]': 'x',\n",
       "  'word[:3]': 'x',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '23',\n",
       "  '-1:word.lower()': '1',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'NUM',\n",
       "  '-1:word.cluster': '267',\n",
       "  '-2:word.lower()': '5mg',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '273',\n",
       "  '-3:word.lower()': '1',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '267',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ao',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '164',\n",
       "  '+2:word.lower()': 'dia',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '31',\n",
       "  '+3:word.lower()': '.',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '153',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ao',\n",
       "  'word[-3:]': 'ao',\n",
       "  'word[:3]': 'ao',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '164',\n",
       "  '-1:word.lower()': 'x',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '23',\n",
       "  '-2:word.lower()': '1',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '267',\n",
       "  '-3:word.lower()': 'x',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '23',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'dia',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '31',\n",
       "  '+2:word.lower()': '.',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '153',\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'dia',\n",
       "  'word[-3:]': 'dia',\n",
       "  'word[:3]': 'dia',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '31',\n",
       "  '-1:word.lower()': 'ao',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'PREP',\n",
       "  '-1:word.cluster': '164',\n",
       "  '-2:word.lower()': 'x',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '23',\n",
       "  '-3:word.lower()': 'ao',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '164',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '153',\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[:3]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '153',\n",
       "  '-1:word.lower()': 'dia',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'N',\n",
       "  '-1:word.cluster': '31',\n",
       "  '-2:word.lower()': 'ao',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '164',\n",
       "  '-3:word.lower()': 'dia',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '31',\n",
       "  'Fourth_word': True,\n",
       "  'EOS': True,\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER flat, clustr 50, janela 3\n",
      "F1 weighted: 0.9378300780433376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Problema      0.948     0.961     0.954       795\n",
      "       Teste      0.935     0.893     0.914       308\n",
      "  Tratamento      0.968     0.939     0.953       446\n",
      "    Anatomia      0.919     0.716     0.805        95\n",
      "\n",
      "   micro avg      0.950     0.928     0.939      1644\n",
      "   macro avg      0.942     0.877     0.907      1644\n",
      "weighted avg      0.949     0.928     0.938      1644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#AVAL no conjunto de teste do ner (flat)\n",
    "print('NER flat, clustr 50, janela 3')\n",
    "\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "JANELA=3\n",
    "CLUSTER=50\n",
    "\n",
    "word2cluster = read_clusters(r\"cluster/cluster-\"+str(CLUSTER)+\".tsv\")\n",
    "X_test = [sent2features(s) for s in testdata]\n",
    "y_test = [sent2labels(s) for s in testdata]\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "\n",
    "finalScore = flat_f1_score(y_test, y_pred, average='weighted', labels=getTiposEntidade())\n",
    "print(\"F1 weighted:\",finalScore)\n",
    "\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "sentences[0]: [('Lucas', 0, 43), (',', 1, 48), ('74', 2, 50), ('anos', 3, 53), ('.', 4, 57)]\n",
      "['Lucas', ',', '74', 'anos', '.']\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "\n",
    "BATCH=800\n",
    "sentences=list()\n",
    "frases=list()\n",
    "allTokens=list()\n",
    "listatokens=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        for token in tokens:\n",
    "            listatokens.append(token[0])\n",
    "            if len(token)>0:\n",
    "                frases.append(tuple(token))\n",
    "        sentences.append(frases)\n",
    "        allTokens.append(listatokens)\n",
    "        listatokens=list()\n",
    "        frases=list()\n",
    "print('sentences[0]:', sentences[0])\n",
    "print(allTokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 506\n",
      "sentences[0]: [('Lucas', 0, 43), (',', 1, 48), ('74', 2, 50), ('anos', 3, 53), ('.', 4, 57)]\n",
      "['Lucas', ',', '74', 'anos', '.']\n",
      "len(sentences): 506\n",
      "len(allTokens): 506\n",
      "len(y_pred): 506\n",
      "y_pred[1]: ['O', 'O', 'O', 'Problema', 'O', 'O', 'O', 'O', 'Problema', 'O', 'Tratamento', 'O', 'Tratamento', 'Tratamento', 'O', 'O', 'O', 'O', 'O']\n",
      "allTokens[1]: ['Em', 'acompanhamento', 'no', 'ambualtorio', 'há', '5', 'anos', 'por', 'FA', ',', 'uso', 'de', 'marevan', '5mg', '1', 'x', 'ao', 'dia', '.']\n",
      "-----Avaliando só modelo de NER com CRF, nested:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.669725  0.372449  0.478689       196\n",
      "           O   0.000000  0.000000  0.000000       605\n",
      "    Problema   0.362044  0.733728  0.484848       338\n",
      "       Teste   0.701068  0.810700  0.751908       243\n",
      "  Tratamento   0.586873  0.710280  0.642706       214\n",
      "\n",
      "    accuracy                       0.419799      1596\n",
      "   macro avg   0.463942  0.525431  0.471630      1596\n",
      "weighted avg   0.344353  0.419799  0.362127      1596\n",
      "\n",
      "[[ 73  98  18   6   1]\n",
      " [ 34   0 393  75 103]\n",
      " [  1  85 248   3   1]\n",
      " [  1  32  11 197   2]\n",
      " [  0  47  15   0 152]]\n"
     ]
    }
   ],
   "source": [
    "# BATCH 800\n",
    "\n",
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "'''\n",
    "example_sent = testdata[18]\n",
    "print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "print(' '.join(sent2tokens(example_sent)))\n",
    "print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n",
    "'''\n",
    "\n",
    "sentences=list()\n",
    "frases=list()\n",
    "allTokens=list()\n",
    "tokens=list()\n",
    "sentences=list()\n",
    "frases=list()\n",
    "allTokens=list()\n",
    "#listatokens=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        for token in tokens:\n",
    "            listatokens.append(token[0])\n",
    "            if len(token)>0:\n",
    "                frases.append(tuple(token))\n",
    "        sentences.append(frases)\n",
    "        allTokens.append(listatokens)\n",
    "        listatokens=list()\n",
    "        frases=list()\n",
    "print('sentences[0]:', sentences[0])\n",
    "print(allTokens[0])\n",
    "\n",
    "#testdata = [[tuple(w.split(' ')) for w in snt.split('\\n')] for snt in f.read().split('\\n\\n')]\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "#X_test = [sent2tokensPredict(s) for s in sentences]\n",
    "X_test = [sent2features(s) for s in sentences]\n",
    "#print('X_test[0]:', X_test[0])\n",
    "print('len(sentences):', len(sentences))\n",
    "print('len(allTokens):', len(allTokens))\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "print('len(y_pred):', len(y_pred))\n",
    "print('y_pred[1]:', y_pred[1])\n",
    "print('allTokens[1]:', allTokens[1])\n",
    "\n",
    "#for tags, tokens in zip(y_pred, allTokens):\n",
    "dic_predictions = f.getDicPredictions(y_pred, allTokens)\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "print('-----Avaliando só modelo de NER com CRF, nested:-----')\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "#tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "#dic_predictions = f.getDicPredictions(tags, tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVAL spans / regioes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0],\n",
       "  ['acompanhamento', 1],\n",
       "  ['no', 2],\n",
       "  ['ambualtorio', 3],\n",
       "  ['há', 4],\n",
       "  ['5', 5],\n",
       "  ['anos', 6],\n",
       "  ['por', 7],\n",
       "  ['FA', 8],\n",
       "  [',', 9],\n",
       "  ['uso', 10],\n",
       "  ['de', 11],\n",
       "  ['marevan', 12],\n",
       "  ['5mg', 13],\n",
       "  ['1', 14],\n",
       "  ['x', 15],\n",
       "  ['ao', 16],\n",
       "  ['dia', 17],\n",
       "  ['.', 18]],\n",
       " [['ambualtorio', [3], 'Problema'],\n",
       "  ['FA', [8], 'Problema'],\n",
       "  ['uso', [10], 'Tratamento'],\n",
       "  ['marevan 5mg', [12, 13], 'Tratamento']]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['HAS', [], ['há', '15', 'anos', 'em'], 'Problema'], ['losartana 50mg', ['anos', 'em', 'uso', 'de'], ['/', 'dia', 'e', 'digoxina'], 'Tratamento'], ['losartana', ['anos', 'em', 'uso', 'de'], ['50mg', '/', 'dia', 'e'], 'O'], ['50mg', ['em', 'uso', 'de', 'losartana'], ['/', 'dia', 'e', 'digoxina'], 'O'], ['digoxina', ['50mg', '/', 'dia', 'e'], ['1', '/', '2', 'cp'], 'Tratamento'], ['carvedilol 25', ['cp', '/', 'dia', ','], ['12', '/', '12', ','], 'Tratamento'], ['carvedilol', ['cp', '/', 'dia', ','], ['25', '12', '/', '12'], 'O'], ['25', ['/', 'dia', ',', 'carvedilol'], ['12', '/', '12', ','], 'O'], ['HCTZ', ['12', '/', '12', ','], ['.'], 'Tratamento']]\n"
     ]
    }
   ],
   "source": [
    "NUM_JANELA=4\n",
    "#def getListaPositiva(dic_sentences):\n",
    "#dic_sentences = dic_sentencesTest\n",
    "def getListaCombinacoes(dic_sentences):\n",
    "    listaSentencas=list()\n",
    "    lista=list()\n",
    "    numJanela=NUM_JANELA\n",
    "    for key, value in dic_sentences.items():\n",
    "        #print('value:', value)\n",
    "        entidades = value[1]\n",
    "        if len(entidades)==0:\n",
    "            continue\n",
    "        listaIndicesEntidadesFrase=list()\n",
    "        for entidade in entidades:\n",
    "          listaIndicesEntidadesFrase.append(entidade[1])\n",
    "        dicTokens={}\n",
    "        for token in value[0]:\n",
    "          #print('token:', token)\n",
    "          dicTokens[token[1]]=token[0]\n",
    "        for entidade in entidades:\n",
    "            #print('entidade:', entidade)\n",
    "            label = entidade[2]\n",
    "            indiceEntidade1=entidade[1][0]\n",
    "            indiceEntidade2=entidade[1][-1]\n",
    "            vizinhosAntes = list()\n",
    "            vizinhosDepois = list()\n",
    "            #print('indiceEntidade:', indiceEntidade)\n",
    "            for tokens in value[0]:\n",
    "                indice=tokens[1]\n",
    "                #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "                if indice+4>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                    vizinhosAntes.append(tokens[0])\n",
    "                if indice-4<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                    vizinhosDepois.append(tokens[0])\n",
    "            lista.append([entidade[0], vizinhosAntes, vizinhosDepois, label])\n",
    "            # agora, os negativos, para entidades com mais de um token\n",
    "            if len(entidade[0].split())>1:\n",
    "              #print('entidade[0] mais de um token: {}'.format(entidade[0].split()))\n",
    "              #listaCombinacoes = powerset(entidade[0].split())\n",
    "              listaCombinacoes = powerset(entidade[1])\n",
    "              #print(listaCombinacoes)\n",
    "              # ver se alguma dessas é positiva\n",
    "              for combinacao in listaCombinacoes:\n",
    "                #print('combinacao:', combinacao)\n",
    "                if combinacao in listaIndicesEntidadesFrase:\n",
    "                  #print('teeeem:', combinacao)\n",
    "                  pass\n",
    "                else:\n",
    "                  combinacaoTokens = [dicTokens[t] for t in combinacao]\n",
    "                  vizinhosAntes = list()\n",
    "                  vizinhosDepois = list()\n",
    "                  indiceEntidade1=combinacao[0]\n",
    "                  indiceEntidade2=combinacao[-1]\n",
    "                  for tokens in value[0]:\n",
    "                    indice=tokens[1]\n",
    "                    #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "                    if indice+4>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                        vizinhosAntes.append(tokens[0])\n",
    "                    if indice-4<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                        vizinhosDepois.append(tokens[0])\n",
    "                  lista.append([' '.join(combinacaoTokens), vizinhosAntes, vizinhosDepois, 'O'])\n",
    "        listaSentencas.append(lista)\n",
    "        lista=list()\n",
    "        #break\n",
    "    return listaSentencas\n",
    "listacombinacoes = getListaCombinacoes(dic_sentencesTest)\n",
    "print(listacombinacoes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dor'], ['no'], ['peito'], ['dor', 'no'], ['no', 'peito'], ['dor', 'no', 'peito']]\n"
     ]
    }
   ],
   "source": [
    "def powerset(entidade):    \n",
    "    lista=list()\n",
    "    for i in range(1, len(entidade)+1):\n",
    "        #lista.append(entidade[i-1])\n",
    "        for j in range(len(entidade) - i + 1):\n",
    "            lista.append(entidade[j:j + i])\n",
    "    return lista\n",
    "        \n",
    "#print(list(powerset([4, 5, 6])))\n",
    "print(powerset(['dor', 'no', 'peito']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Comorbidades', 0, 142],\n",
       "  [':', 1, 154],\n",
       "  ['DM', 2, 156],\n",
       "  ['há', 3, 159],\n",
       "  ['10', 4, 162],\n",
       "  ['anos', 5, 165],\n",
       "  ['em', 6, 170],\n",
       "  ['uso', 7, 173],\n",
       "  ['de', 8, 177],\n",
       "  ['metformina', 9, 180],\n",
       "  ['850mg', 10, 191],\n",
       "  ['3', 11, 197],\n",
       "  ['cp', 12, 199],\n",
       "  ['/', 13, 201],\n",
       "  ['dia', 14, 202],\n",
       "  [',', 15, 205],\n",
       "  ['acarbose', 16, 207],\n",
       "  ['1', 17, 216],\n",
       "  ['cp', 18, 218],\n",
       "  ['/', 19, 220],\n",
       "  ['dia', 20, 221],\n",
       "  ['e', 21, 225],\n",
       "  ['glicazida', 22, 227],\n",
       "  ['60mg', 23, 237],\n",
       "  ['2', 24, 242],\n",
       "  ['cp', 25, 244],\n",
       "  ['/', 26, 246],\n",
       "  ['dia', 27, 247],\n",
       "  ['e', 28, 251],\n",
       "  ['insulina', 29, 253],\n",
       "  ['(', 30, 262],\n",
       "  ['24', 31, 263],\n",
       "  ['-', 32, 266],\n",
       "  ['0', 33, 268],\n",
       "  ['-', 34, 270],\n",
       "  ['24', 35, 272],\n",
       "  [')', 36, 274],\n",
       "  ['.', 37, 275]],\n",
       " [['Comorbidades', [0], 'Problema'],\n",
       "  ['DM', [2], 'Problema'],\n",
       "  ['metformina 850mg', [9, 10], 'Tratamento'],\n",
       "  ['acarbose', [16], 'Tratamento'],\n",
       "  ['glicazida 60mg', [22, 23], 'Tratamento'],\n",
       "  ['insulina', [29], 'Tratamento']]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest = f.load_obj(r'dic_sentencesTest')\n",
    "dic_sentencesTest[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Comorbidades', 0],\n",
       "  [':', 1],\n",
       "  ['DM', 2],\n",
       "  ['há', 3],\n",
       "  ['10', 4],\n",
       "  ['anos', 5],\n",
       "  ['em', 6],\n",
       "  ['uso', 7],\n",
       "  ['de', 8],\n",
       "  ['metformina', 9],\n",
       "  ['850mg', 10],\n",
       "  ['3', 11],\n",
       "  ['cp', 12],\n",
       "  ['/', 13],\n",
       "  ['dia', 14],\n",
       "  [',', 15],\n",
       "  ['acarbose', 16],\n",
       "  ['1', 17],\n",
       "  ['cp', 18],\n",
       "  ['/', 19],\n",
       "  ['dia', 20],\n",
       "  ['e', 21],\n",
       "  ['glicazida', 22],\n",
       "  ['60mg', 23],\n",
       "  ['2', 24],\n",
       "  ['cp', 25],\n",
       "  ['/', 26],\n",
       "  ['dia', 27],\n",
       "  ['e', 28],\n",
       "  ['insulina', 29],\n",
       "  ['(', 30],\n",
       "  ['24', 31],\n",
       "  ['-', 32],\n",
       "  ['0', 33],\n",
       "  ['-', 34],\n",
       "  ['24', 35],\n",
       "  [')', 36],\n",
       "  ['.', 37]],\n",
       " [['Comorbidades', [0], 'Problema'],\n",
       "  ['DM', [2], 'Problema'],\n",
       "  ['metformina 850mg', [9, 10], 'Tratamento'],\n",
       "  ['acarbose', [16], 'Tratamento'],\n",
       "  ['glicazida 60mg', [22, 23], 'Tratamento'],\n",
       "  ['insulina', [29], 'Tratamento']]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Lucas', [], [',', '74', 'anos', '.'], 0],\n",
       "  [',', ['Lucas'], ['74', 'anos', '.'], 0],\n",
       "  ['74', ['Lucas', ','], ['anos', '.'], 0],\n",
       "  ['anos', ['Lucas', ',', '74'], ['.'], 0],\n",
       "  ['.', [',', '74', 'anos'], [], 0],\n",
       "  ['Lucas ,', [], ['74', 'anos', '.'], 0],\n",
       "  [', 74', ['Lucas'], ['anos', '.'], 0],\n",
       "  ['74 anos', ['Lucas', ','], ['.'], 0],\n",
       "  ['anos .', ['Lucas', ',', '74'], [], 0],\n",
       "  ['Lucas , 74', [], ['anos', '.'], 0],\n",
       "  [', 74 anos', ['Lucas'], ['.'], 0],\n",
       "  ['74 anos .', ['Lucas', ','], [], 0],\n",
       "  ['Lucas , 74 anos', [], ['.'], 0],\n",
       "  [', 74 anos .', ['Lucas'], [], 0],\n",
       "  ['Lucas , 74 anos .', [], [], 0]],\n",
       " [['Em', [], ['acompanhamento', 'no', 'ambualtorio', 'há'], 0],\n",
       "  ['acompanhamento', ['Em'], ['no', 'ambualtorio', 'há', '5'], 0],\n",
       "  ['no', ['Em', 'acompanhamento'], ['ambualtorio', 'há', '5', 'anos'], 0],\n",
       "  ['ambualtorio',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['há', '5', 'anos', 'por'],\n",
       "   0],\n",
       "  ['há',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['5', 'anos', 'por', 'FA'],\n",
       "   0],\n",
       "  ['5', ['no', 'ambualtorio', 'há'], ['anos', 'por', 'FA', ','], 0],\n",
       "  ['anos', ['ambualtorio', 'há', '5'], ['por', 'FA', ',', 'uso'], 0],\n",
       "  ['por', ['há', '5', 'anos'], ['FA', ',', 'uso', 'de'], 0],\n",
       "  ['FA', ['5', 'anos', 'por'], [',', 'uso', 'de', 'marevan'], 1],\n",
       "  [',', ['anos', 'por', 'FA'], ['uso', 'de', 'marevan', '5mg'], 0],\n",
       "  ['uso', ['por', 'FA', ','], ['de', 'marevan', '5mg', '1'], 0],\n",
       "  ['de', ['FA', ',', 'uso'], ['marevan', '5mg', '1', 'x'], 0],\n",
       "  ['marevan', [',', 'uso', 'de'], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  ['5mg', ['uso', 'de', 'marevan'], ['1', 'x', 'ao', 'dia'], 0],\n",
       "  ['1', ['de', 'marevan', '5mg'], ['x', 'ao', 'dia', '.'], 0],\n",
       "  ['x', ['marevan', '5mg', '1'], ['ao', 'dia', '.'], 0],\n",
       "  ['ao', ['5mg', '1', 'x'], ['dia', '.'], 0],\n",
       "  ['dia', ['1', 'x', 'ao'], ['.'], 0],\n",
       "  ['.', ['x', 'ao', 'dia'], [], 0],\n",
       "  ['Em acompanhamento', [], ['no', 'ambualtorio', 'há', '5'], 0],\n",
       "  ['acompanhamento no', ['Em'], ['ambualtorio', 'há', '5', 'anos'], 0],\n",
       "  ['no ambualtorio', ['Em', 'acompanhamento'], ['há', '5', 'anos', 'por'], 0],\n",
       "  ['ambualtorio há',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['5', 'anos', 'por', 'FA'],\n",
       "   0],\n",
       "  ['há 5',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['anos', 'por', 'FA', ','],\n",
       "   0],\n",
       "  ['5 anos', ['no', 'ambualtorio', 'há'], ['por', 'FA', ',', 'uso'], 0],\n",
       "  ['anos por', ['ambualtorio', 'há', '5'], ['FA', ',', 'uso', 'de'], 0],\n",
       "  ['por FA', ['há', '5', 'anos'], [',', 'uso', 'de', 'marevan'], 0],\n",
       "  ['FA ,', ['5', 'anos', 'por'], ['uso', 'de', 'marevan', '5mg'], 0],\n",
       "  [', uso', ['anos', 'por', 'FA'], ['de', 'marevan', '5mg', '1'], 0],\n",
       "  ['uso de', ['por', 'FA', ','], ['marevan', '5mg', '1', 'x'], 0],\n",
       "  ['de marevan', ['FA', ',', 'uso'], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  ['marevan 5mg', [',', 'uso', 'de'], ['1', 'x', 'ao', 'dia'], 1],\n",
       "  ['5mg 1', ['uso', 'de', 'marevan'], ['x', 'ao', 'dia', '.'], 0],\n",
       "  ['1 x', ['de', 'marevan', '5mg'], ['ao', 'dia', '.'], 0],\n",
       "  ['x ao', ['marevan', '5mg', '1'], ['dia', '.'], 0],\n",
       "  ['ao dia', ['5mg', '1', 'x'], ['.'], 0],\n",
       "  ['dia .', ['1', 'x', 'ao'], [], 0],\n",
       "  ['Em acompanhamento no', [], ['ambualtorio', 'há', '5', 'anos'], 0],\n",
       "  ['acompanhamento no ambualtorio', ['Em'], ['há', '5', 'anos', 'por'], 0],\n",
       "  ['no ambualtorio há',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['5', 'anos', 'por', 'FA'],\n",
       "   0],\n",
       "  ['ambualtorio há 5',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['anos', 'por', 'FA', ','],\n",
       "   0],\n",
       "  ['há 5 anos',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['por', 'FA', ',', 'uso'],\n",
       "   0],\n",
       "  ['5 anos por', ['no', 'ambualtorio', 'há'], ['FA', ',', 'uso', 'de'], 0],\n",
       "  ['anos por FA',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['por FA ,', ['há', '5', 'anos'], ['uso', 'de', 'marevan', '5mg'], 0],\n",
       "  ['FA , uso', ['5', 'anos', 'por'], ['de', 'marevan', '5mg', '1'], 0],\n",
       "  [', uso de', ['anos', 'por', 'FA'], ['marevan', '5mg', '1', 'x'], 0],\n",
       "  ['uso de marevan', ['por', 'FA', ','], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  ['de marevan 5mg', ['FA', ',', 'uso'], ['1', 'x', 'ao', 'dia'], 0],\n",
       "  ['marevan 5mg 1', [',', 'uso', 'de'], ['x', 'ao', 'dia', '.'], 0],\n",
       "  ['5mg 1 x', ['uso', 'de', 'marevan'], ['ao', 'dia', '.'], 0],\n",
       "  ['1 x ao', ['de', 'marevan', '5mg'], ['dia', '.'], 0],\n",
       "  ['x ao dia', ['marevan', '5mg', '1'], ['.'], 0],\n",
       "  ['ao dia .', ['5mg', '1', 'x'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio', [], ['há', '5', 'anos', 'por'], 0],\n",
       "  ['acompanhamento no ambualtorio há', ['Em'], ['5', 'anos', 'por', 'FA'], 0],\n",
       "  ['no ambualtorio há 5',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['anos', 'por', 'FA', ','],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['por', 'FA', ',', 'uso'],\n",
       "   0],\n",
       "  ['há 5 anos por',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['FA', ',', 'uso', 'de'],\n",
       "   0],\n",
       "  ['5 anos por FA',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['anos por FA ,',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['por FA , uso', ['há', '5', 'anos'], ['de', 'marevan', '5mg', '1'], 0],\n",
       "  ['FA , uso de', ['5', 'anos', 'por'], ['marevan', '5mg', '1', 'x'], 0],\n",
       "  [', uso de marevan', ['anos', 'por', 'FA'], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  ['uso de marevan 5mg', ['por', 'FA', ','], ['1', 'x', 'ao', 'dia'], 0],\n",
       "  ['de marevan 5mg 1', ['FA', ',', 'uso'], ['x', 'ao', 'dia', '.'], 0],\n",
       "  ['marevan 5mg 1 x', [',', 'uso', 'de'], ['ao', 'dia', '.'], 0],\n",
       "  ['5mg 1 x ao', ['uso', 'de', 'marevan'], ['dia', '.'], 0],\n",
       "  ['1 x ao dia', ['de', 'marevan', '5mg'], ['.'], 0],\n",
       "  ['x ao dia .', ['marevan', '5mg', '1'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há', [], ['5', 'anos', 'por', 'FA'], 0],\n",
       "  ['acompanhamento no ambualtorio há 5',\n",
       "   ['Em'],\n",
       "   ['anos', 'por', 'FA', ','],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['por', 'FA', ',', 'uso'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['FA', ',', 'uso', 'de'],\n",
       "   0],\n",
       "  ['há 5 anos por FA',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['5 anos por FA ,',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['anos por FA , uso',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['por FA , uso de', ['há', '5', 'anos'], ['marevan', '5mg', '1', 'x'], 0],\n",
       "  ['FA , uso de marevan', ['5', 'anos', 'por'], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  [', uso de marevan 5mg', ['anos', 'por', 'FA'], ['1', 'x', 'ao', 'dia'], 0],\n",
       "  ['uso de marevan 5mg 1', ['por', 'FA', ','], ['x', 'ao', 'dia', '.'], 0],\n",
       "  ['de marevan 5mg 1 x', ['FA', ',', 'uso'], ['ao', 'dia', '.'], 0],\n",
       "  ['marevan 5mg 1 x ao', [',', 'uso', 'de'], ['dia', '.'], 0],\n",
       "  ['5mg 1 x ao dia', ['uso', 'de', 'marevan'], ['.'], 0],\n",
       "  ['1 x ao dia .', ['de', 'marevan', '5mg'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5', [], ['anos', 'por', 'FA', ','], 0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos',\n",
       "   ['Em'],\n",
       "   ['por', 'FA', ',', 'uso'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['FA', ',', 'uso', 'de'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['há 5 anos por FA ,',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['anos por FA , uso de',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan', ['há', '5', 'anos'], ['5mg', '1', 'x', 'ao'], 0],\n",
       "  ['FA , uso de marevan 5mg',\n",
       "   ['5', 'anos', 'por'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  [', uso de marevan 5mg 1',\n",
       "   ['anos', 'por', 'FA'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['uso de marevan 5mg 1 x', ['por', 'FA', ','], ['ao', 'dia', '.'], 0],\n",
       "  ['de marevan 5mg 1 x ao', ['FA', ',', 'uso'], ['dia', '.'], 0],\n",
       "  ['marevan 5mg 1 x ao dia', [',', 'uso', 'de'], ['.'], 0],\n",
       "  ['5mg 1 x ao dia .', ['uso', 'de', 'marevan'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos',\n",
       "   [],\n",
       "   ['por', 'FA', ',', 'uso'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por',\n",
       "   ['Em'],\n",
       "   ['FA', ',', 'uso', 'de'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA ,',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg',\n",
       "   ['há', '5', 'anos'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['FA , uso de marevan 5mg 1',\n",
       "   ['5', 'anos', 'por'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  [', uso de marevan 5mg 1 x', ['anos', 'por', 'FA'], ['ao', 'dia', '.'], 0],\n",
       "  ['uso de marevan 5mg 1 x ao', ['por', 'FA', ','], ['dia', '.'], 0],\n",
       "  ['de marevan 5mg 1 x ao dia', ['FA', ',', 'uso'], ['.'], 0],\n",
       "  ['marevan 5mg 1 x ao dia .', [',', 'uso', 'de'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por',\n",
       "   [],\n",
       "   ['FA', ',', 'uso', 'de'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA',\n",
       "   ['Em'],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA ,',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg 1',\n",
       "   ['há', '5', 'anos'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['FA , uso de marevan 5mg 1 x', ['5', 'anos', 'por'], ['ao', 'dia', '.'], 0],\n",
       "  [', uso de marevan 5mg 1 x ao', ['anos', 'por', 'FA'], ['dia', '.'], 0],\n",
       "  ['uso de marevan 5mg 1 x ao dia', ['por', 'FA', ','], ['.'], 0],\n",
       "  ['de marevan 5mg 1 x ao dia .', ['FA', ',', 'uso'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA',\n",
       "   [],\n",
       "   [',', 'uso', 'de', 'marevan'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA ,',\n",
       "   ['Em'],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg 1',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg 1 x',\n",
       "   ['há', '5', 'anos'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['FA , uso de marevan 5mg 1 x ao', ['5', 'anos', 'por'], ['dia', '.'], 0],\n",
       "  [', uso de marevan 5mg 1 x ao dia', ['anos', 'por', 'FA'], ['.'], 0],\n",
       "  ['uso de marevan 5mg 1 x ao dia .', ['por', 'FA', ','], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA ,',\n",
       "   [],\n",
       "   ['uso', 'de', 'marevan', '5mg'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso',\n",
       "   ['Em'],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg 1',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg 1 x ao', ['há', '5', 'anos'], ['dia', '.'], 0],\n",
       "  ['FA , uso de marevan 5mg 1 x ao dia', ['5', 'anos', 'por'], ['.'], 0],\n",
       "  [', uso de marevan 5mg 1 x ao dia .', ['anos', 'por', 'FA'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso',\n",
       "   [],\n",
       "   ['de', 'marevan', '5mg', '1'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de',\n",
       "   ['Em'],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg 1',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg 1 x ao dia', ['há', '5', 'anos'], ['.'], 0],\n",
       "  ['FA , uso de marevan 5mg 1 x ao dia .', ['5', 'anos', 'por'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de',\n",
       "   [],\n",
       "   ['marevan', '5mg', '1', 'x'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan',\n",
       "   ['Em'],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg 1',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['por FA , uso de marevan 5mg 1 x ao dia .', ['há', '5', 'anos'], [], 0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan',\n",
       "   [],\n",
       "   ['5mg', '1', 'x', 'ao'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg',\n",
       "   ['Em'],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg 1',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['ambualtorio', 'há', '5'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg',\n",
       "   [],\n",
       "   ['1', 'x', 'ao', 'dia'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1',\n",
       "   ['Em'],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['no', 'ambualtorio', 'há'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1',\n",
       "   [],\n",
       "   ['x', 'ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   ['Em'],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['há 5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['acompanhamento', 'no', 'ambualtorio'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x',\n",
       "   [],\n",
       "   ['ao', 'dia', '.'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   ['Em'],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['Em', 'acompanhamento', 'no'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao',\n",
       "   [],\n",
       "   ['dia', '.'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   ['Em'],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['Em', 'acompanhamento'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia',\n",
       "   [],\n",
       "   ['.'],\n",
       "   0],\n",
       "  ['acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   ['Em'],\n",
       "   [],\n",
       "   0],\n",
       "  ['Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .',\n",
       "   [],\n",
       "   [],\n",
       "   0]]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listacombinacoesTest = getListaCombinacoes(dic_sentencesTest)\n",
    "listaTest[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getListaPositiva(dic_sentences):\n",
    "#dic_sentences = dic_sentencesTest\n",
    "def getListaCombinacoes(dic_sentences):\n",
    "    listaSentencas=list()\n",
    "    lista=list()\n",
    "    dicPred={}\n",
    "    #numJanela=NUM_JANELA\n",
    "    numJanela=4\n",
    "    num1=-1\n",
    "    num2=0\n",
    "    for key, value in dic_sentences.items():\n",
    "        num1=num1+1\n",
    "        #print('value:', value)\n",
    "        entidades = value[1]\n",
    "        if len(entidades)==0:\n",
    "            dicPred[num2]=num1\n",
    "            continue\n",
    "        dicPred[num2]=num1\n",
    "        num2=num2+1\n",
    "        listaIndicesEntidadesFrase=list()\n",
    "        for entidade in entidades:\n",
    "          listaIndicesEntidadesFrase.append(entidade[1])\n",
    "        dicTokens={}\n",
    "        for token in value[0]:\n",
    "          #print('token:', token)\n",
    "          dicTokens[token[1]]=token[0]\n",
    "        for entidade in entidades:\n",
    "            #print('entidade:', entidade)\n",
    "            #label = entidade[2]\n",
    "            label = 'ENT'\n",
    "            indiceEntidade1=entidade[1][0]\n",
    "            indiceEntidade2=entidade[1][-1]\n",
    "            vizinhosAntes = list()\n",
    "            vizinhosDepois = list()\n",
    "            #print('indiceEntidade:', indiceEntidade)\n",
    "            for tokens in value[0]:\n",
    "                indice=tokens[1]\n",
    "                #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "                if indice+numJanela>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                    vizinhosAntes.append(tokens[0])\n",
    "                if indice-numJanela<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                    vizinhosDepois.append(tokens[0])\n",
    "            lista.append([entidade[0], vizinhosAntes, vizinhosDepois, label])\n",
    "            # agora, os negativos, para entidades com mais de um token\n",
    "            if len(entidade[0].split())>1:\n",
    "              #print('entidade[0] mais de um token: {}'.format(entidade[0].split()))\n",
    "              #listaCombinacoes = powerset(entidade[0].split())\n",
    "              listaCombinacoes = powerset(entidade[1])\n",
    "              #print(listaCombinacoes)\n",
    "              # ver se alguma dessas é positiva\n",
    "              for combinacao in listaCombinacoes:\n",
    "                #print('combinacao:', combinacao)\n",
    "                if combinacao in listaIndicesEntidadesFrase:\n",
    "                  #print('teeeem:', combinacao)\n",
    "                  pass\n",
    "                else:\n",
    "                  combinacaoTokens = [dicTokens[t] for t in combinacao]\n",
    "                  vizinhosAntes = list()\n",
    "                  vizinhosDepois = list()\n",
    "                  indiceEntidade1=combinacao[0]\n",
    "                  indiceEntidade2=combinacao[-1]\n",
    "                  for tokens in value[0]:\n",
    "                    indice=tokens[1]\n",
    "                    #print('token: {}, indice: {}'.format(tokens[0], indice))\n",
    "                    if indice+numJanela>=indiceEntidade1 and indice<indiceEntidade1:\n",
    "                        vizinhosAntes.append(tokens[0])\n",
    "                    if indice-numJanela<=indiceEntidade2 and indice>indiceEntidade2:\n",
    "                        vizinhosDepois.append(tokens[0])\n",
    "                  lista.append([' '.join(combinacaoTokens), vizinhosAntes, vizinhosDepois, 'O'])\n",
    "        listaSentencas.append(lista)\n",
    "        lista=list()\n",
    "        #break\n",
    "    return listaSentencas, dicPred\n",
    "\n",
    "listacombinacoesTest = getListaCombinacoes(dic_sentencesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    #return ['Problema','Teste','Tratamento','Anatomia']\n",
    "    return ['O','ENT']\n",
    "\n",
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    try:\n",
    "        with open(cluster_file, encoding='utf-8') as i:\n",
    "            for num, line in enumerate(i):\n",
    "                if line:\n",
    "                    word, cluster = line.strip().split('\\t')\n",
    "                    word2cluster[word] = cluster\n",
    "    except:\n",
    "        print(line)\n",
    "        print(num)\n",
    "        raise\n",
    "    return word2cluster\n",
    "\n",
    "\n",
    "def word2features(sent, word2cluster, dicPostagger):\n",
    "    try:\n",
    "        features = list()\n",
    "        entidades = sent[0]\n",
    "        for entidade in entidades.split():\n",
    "            postag = tipoPostaggerTokens(entidade, dicPostagger)\n",
    "            features.extend([\n",
    "            'bias',\n",
    "            'word.lower=' + entidade.lower(),\n",
    "            'word[-3:]=' + entidade[-3:],\n",
    "            'word[:3]=' + entidade[:3],\n",
    "            'word.isupper=%s' % entidade.isupper(),\n",
    "            'word.istitle=%s' % entidade.istitle(),\n",
    "            'word.isdigit=%s' % entidade.isdigit(),\n",
    "            'word.cluster=%s' % word2cluster[entidade.lower()] if entidade.lower() in word2cluster else \"0\",\n",
    "            'postag=' + postag\n",
    "            ])\n",
    "        # palavras anteriores\n",
    "        vizinhosAntes = sent[1] \n",
    "        if len(vizinhosAntes)>0:\n",
    "            for num, vizinhoAntes in enumerate(vizinhosAntes):\n",
    "                word1 = vizinhoAntes\n",
    "                postag1 =  tipoPostaggerTokens(vizinhoAntes, dicPostagger)\n",
    "                features.extend([\n",
    "                    '-'+str(num+1)+':word.lower=' + word1.lower(),\n",
    "                    '-'+str(num+1)+':word.istitle=%s' % word1.istitle(),\n",
    "                    '-'+str(num+1)+':word.isupper=%s' % word1.isupper(),\n",
    "                    '-'+str(num+1)+':postag=' + postag1\n",
    "                ])\n",
    "        else:\n",
    "            features.append('BOS')\n",
    "\n",
    "        # próximas palavras\n",
    "        vizinhosDepois = sent[2]\n",
    "        if len(vizinhosDepois)>0:\n",
    "            for num, vizinhoDepois in enumerate(vizinhosDepois):\n",
    "                word1 = vizinhoDepois\n",
    "                postag1 =  tipoPostaggerTokens(vizinhoDepois, dicPostagger)\n",
    "                features.extend([\n",
    "                    '+'+str(num+1)+':word.lower=' + word1.lower(),\n",
    "                    '+'+str(num+1)+':word.istitle=%s' % word1.istitle(),\n",
    "                    '+'+str(num+1)+':word.isupper=%s' % word1.isupper(),\n",
    "                    '+'+str(num+1)+':postag=' + postag1\n",
    "                ])\n",
    "        else:\n",
    "            features.append('EOS')\n",
    "    except:\n",
    "        print('sent:', sent)\n",
    "        raise\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(lista, word2cluster, dicPostagger):\n",
    "    #return word2features(lista, word2cluster, dicPostagger)\n",
    "    return [word2features(lista[i], word2cluster, dicPostagger) for i in range(len(lista))]\n",
    "\n",
    "def sent2labels(lista):\n",
    "    #return [label for _, _, _, label in lista]\n",
    "    return [label for _, _, _, label in lista]\n",
    "\n",
    "#def sent2tokens(sent):\n",
    "#    return [token for token, postag, label in sent]\n",
    "\n",
    "#word2cluster = read_clusters(r\"cluster/cluster-50.tsv\")\n",
    "#word2cluster = read_clusters(r\"clusters/cluster-5.tsv\")\n",
    "#word2cluster = read_clusters(r\"clusters/cluster-10.tsv\")\n",
    "#word2cluster = read_clusters(r\"clusters/cluster-100.tsv\")\n",
    "#word2cluster = read_clusters(r\"clusters/cluster-300.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [sent2features(s, word2cluster, dicPostagger) for s in listacombinacoesTest]\n",
    "y_test = [sent2labels(s) for s in listacombinacoesTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regiao CRF binario, cluster 300, janela 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.948     0.944     0.946      2730\n",
      "         ENT      0.848     0.859     0.853       991\n",
      "\n",
      "    accuracy                          0.921      3721\n",
      "   macro avg      0.898     0.901     0.900      3721\n",
      "weighted avg      0.922     0.921     0.921      3721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLUSTER=300\n",
    "word2cluster = read_clusters(r\"cluster/cluster-300.tsv\")\n",
    "\n",
    "print('regiao CRF binario, cluster 300, janela 4')\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_binario_regiao\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agora, com pipeline... regiao CRF binario, cluster 300, janela 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.809     0.907     0.855      1307\n",
      "         ENT      0.896     0.790     0.840      1334\n",
      "\n",
      "    accuracy                          0.848      2641\n",
      "   macro avg      0.853     0.848     0.847      2641\n",
      "weighted avg      0.853     0.848     0.847      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CLUSTER=300\n",
    "word2cluster = read_clusters(r\"cluster/cluster-300.tsv\")\n",
    "\n",
    "print('agora, com pipeline... regiao CRF binario, cluster 300, janela 4')\n",
    "\n",
    "listacombinacoesTestPred = getListaCombinacoes(dic_predictions)\n",
    "\n",
    "X_test = [sent2features(s, word2cluster, dicPostagger) for s in listacombinacoesTestPred]\n",
    "y_test = [sent2labels(s) for s in listacombinacoesTest]\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_binario_regiao\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agora, com pipeline... regiao CRF binario com NER BERT, cluster 300, janela 4\n"
     ]
    }
   ],
   "source": [
    "# com CRF ner - BERT\n",
    "\n",
    "CLUSTER=300\n",
    "word2cluster = read_clusters(r\"cluster/cluster-300.tsv\")\n",
    "\n",
    "print('agora, com pipeline... regiao CRF binario com NER BERT, cluster 300, janela 4')\n",
    "\n",
    "listacombinacoesTestPred, dicPred = getListaCombinacoes(dic_predictions)\n",
    "\n",
    "\n",
    "X_test = [sent2features(s, word2cluster, dicPostagger) for s in listacombinacoesTestPred]\n",
    "#y_test = [sent2labels(s) for s in listacombinacoesTest]\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_binario_regiao\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1,\n",
       " 1: 2,\n",
       " 2: 3,\n",
       " 3: 4,\n",
       " 4: 6,\n",
       " 5: 7,\n",
       " 6: 8,\n",
       " 7: 9,\n",
       " 8: 10,\n",
       " 9: 11,\n",
       " 10: 12,\n",
       " 11: 13,\n",
       " 12: 14,\n",
       " 13: 15,\n",
       " 14: 16,\n",
       " 15: 17,\n",
       " 16: 18,\n",
       " 17: 19,\n",
       " 18: 20,\n",
       " 19: 23,\n",
       " 20: 24,\n",
       " 21: 25,\n",
       " 22: 26,\n",
       " 23: 27,\n",
       " 24: 28,\n",
       " 25: 30,\n",
       " 26: 31,\n",
       " 27: 32,\n",
       " 28: 33,\n",
       " 29: 34,\n",
       " 30: 35,\n",
       " 31: 36,\n",
       " 32: 37,\n",
       " 33: 38,\n",
       " 34: 41,\n",
       " 35: 42,\n",
       " 36: 43,\n",
       " 37: 44,\n",
       " 38: 45,\n",
       " 39: 46,\n",
       " 40: 47,\n",
       " 41: 48,\n",
       " 42: 49,\n",
       " 43: 50,\n",
       " 44: 51,\n",
       " 45: 52,\n",
       " 46: 53,\n",
       " 47: 54,\n",
       " 48: 56,\n",
       " 49: 58,\n",
       " 50: 59,\n",
       " 51: 60,\n",
       " 52: 61,\n",
       " 53: 62,\n",
       " 54: 63,\n",
       " 55: 64,\n",
       " 56: 65,\n",
       " 57: 66,\n",
       " 58: 67,\n",
       " 59: 68,\n",
       " 60: 69,\n",
       " 61: 70,\n",
       " 62: 71,\n",
       " 63: 72,\n",
       " 64: 74,\n",
       " 65: 75,\n",
       " 66: 76,\n",
       " 67: 77,\n",
       " 68: 78,\n",
       " 69: 79,\n",
       " 70: 80,\n",
       " 71: 82,\n",
       " 72: 83,\n",
       " 73: 84,\n",
       " 74: 85,\n",
       " 75: 86,\n",
       " 76: 87,\n",
       " 77: 88,\n",
       " 78: 89,\n",
       " 79: 90,\n",
       " 80: 93,\n",
       " 81: 94,\n",
       " 82: 95,\n",
       " 83: 97,\n",
       " 84: 98,\n",
       " 85: 99,\n",
       " 86: 101,\n",
       " 87: 103,\n",
       " 88: 104,\n",
       " 89: 105,\n",
       " 90: 106,\n",
       " 91: 107,\n",
       " 92: 109,\n",
       " 93: 110,\n",
       " 94: 111,\n",
       " 95: 112,\n",
       " 96: 113,\n",
       " 97: 114,\n",
       " 98: 115,\n",
       " 99: 116,\n",
       " 100: 117,\n",
       " 101: 118,\n",
       " 102: 119,\n",
       " 103: 121,\n",
       " 104: 123,\n",
       " 105: 124,\n",
       " 106: 130,\n",
       " 107: 131,\n",
       " 108: 132,\n",
       " 109: 133,\n",
       " 110: 134,\n",
       " 111: 135,\n",
       " 112: 136,\n",
       " 113: 137,\n",
       " 114: 138,\n",
       " 115: 139,\n",
       " 116: 140,\n",
       " 117: 141,\n",
       " 118: 142,\n",
       " 119: 143,\n",
       " 120: 144,\n",
       " 121: 145,\n",
       " 122: 146,\n",
       " 123: 148,\n",
       " 124: 149,\n",
       " 125: 150,\n",
       " 126: 151,\n",
       " 127: 152,\n",
       " 128: 153,\n",
       " 129: 154,\n",
       " 130: 155,\n",
       " 131: 157,\n",
       " 132: 158,\n",
       " 133: 159,\n",
       " 134: 161,\n",
       " 135: 162,\n",
       " 136: 163,\n",
       " 137: 164,\n",
       " 138: 165,\n",
       " 139: 166,\n",
       " 140: 167,\n",
       " 141: 168,\n",
       " 142: 170,\n",
       " 143: 171,\n",
       " 144: 172,\n",
       " 145: 173,\n",
       " 146: 174,\n",
       " 147: 175,\n",
       " 148: 177,\n",
       " 149: 178,\n",
       " 150: 180,\n",
       " 151: 181,\n",
       " 152: 182,\n",
       " 153: 183,\n",
       " 154: 184,\n",
       " 155: 185,\n",
       " 156: 188,\n",
       " 157: 189,\n",
       " 158: 190,\n",
       " 159: 192,\n",
       " 160: 194,\n",
       " 161: 195,\n",
       " 162: 198,\n",
       " 163: 199,\n",
       " 164: 200,\n",
       " 165: 201,\n",
       " 166: 202,\n",
       " 167: 203,\n",
       " 168: 204,\n",
       " 169: 205,\n",
       " 170: 206,\n",
       " 171: 207,\n",
       " 172: 208,\n",
       " 173: 209,\n",
       " 174: 211,\n",
       " 175: 212,\n",
       " 176: 213,\n",
       " 177: 215,\n",
       " 178: 216,\n",
       " 179: 217,\n",
       " 180: 218,\n",
       " 181: 220,\n",
       " 182: 221,\n",
       " 183: 222,\n",
       " 184: 223,\n",
       " 185: 224,\n",
       " 186: 225,\n",
       " 187: 227,\n",
       " 188: 228,\n",
       " 189: 229,\n",
       " 190: 230,\n",
       " 191: 231,\n",
       " 192: 233,\n",
       " 193: 234,\n",
       " 194: 235,\n",
       " 195: 236,\n",
       " 196: 237,\n",
       " 197: 238,\n",
       " 198: 239,\n",
       " 199: 240,\n",
       " 200: 242,\n",
       " 201: 243,\n",
       " 202: 244,\n",
       " 203: 245,\n",
       " 204: 246,\n",
       " 205: 247,\n",
       " 206: 248,\n",
       " 207: 249,\n",
       " 208: 250,\n",
       " 209: 251,\n",
       " 210: 252,\n",
       " 211: 253,\n",
       " 212: 254,\n",
       " 213: 255,\n",
       " 214: 256,\n",
       " 215: 257,\n",
       " 216: 258,\n",
       " 217: 259,\n",
       " 218: 260,\n",
       " 219: 261,\n",
       " 220: 263,\n",
       " 221: 264,\n",
       " 222: 265,\n",
       " 223: 266,\n",
       " 224: 267,\n",
       " 225: 269,\n",
       " 226: 270,\n",
       " 227: 272,\n",
       " 228: 273,\n",
       " 229: 274,\n",
       " 230: 276,\n",
       " 231: 277,\n",
       " 232: 278,\n",
       " 233: 279,\n",
       " 234: 280,\n",
       " 235: 281,\n",
       " 236: 282,\n",
       " 237: 283,\n",
       " 238: 284,\n",
       " 239: 286,\n",
       " 240: 287,\n",
       " 241: 288,\n",
       " 242: 289,\n",
       " 243: 290,\n",
       " 244: 293,\n",
       " 245: 294,\n",
       " 246: 295,\n",
       " 247: 296,\n",
       " 248: 298,\n",
       " 249: 299,\n",
       " 250: 300,\n",
       " 251: 301,\n",
       " 252: 302,\n",
       " 253: 303,\n",
       " 254: 304,\n",
       " 255: 305,\n",
       " 256: 308,\n",
       " 257: 309,\n",
       " 258: 310,\n",
       " 259: 311,\n",
       " 260: 312,\n",
       " 261: 313,\n",
       " 262: 314,\n",
       " 263: 315,\n",
       " 264: 316,\n",
       " 265: 317,\n",
       " 266: 318,\n",
       " 267: 319,\n",
       " 268: 320,\n",
       " 269: 321,\n",
       " 270: 322,\n",
       " 271: 323,\n",
       " 272: 324,\n",
       " 273: 325,\n",
       " 274: 327,\n",
       " 275: 328,\n",
       " 276: 331,\n",
       " 277: 332,\n",
       " 278: 333,\n",
       " 279: 334,\n",
       " 280: 335,\n",
       " 281: 336,\n",
       " 282: 337,\n",
       " 283: 338,\n",
       " 284: 339,\n",
       " 285: 340,\n",
       " 286: 341,\n",
       " 287: 342,\n",
       " 288: 344,\n",
       " 289: 345,\n",
       " 290: 346,\n",
       " 291: 349,\n",
       " 292: 350,\n",
       " 293: 351,\n",
       " 294: 352,\n",
       " 295: 353,\n",
       " 296: 354,\n",
       " 297: 355,\n",
       " 298: 356,\n",
       " 299: 357,\n",
       " 300: 358,\n",
       " 301: 359,\n",
       " 302: 360,\n",
       " 303: 361,\n",
       " 304: 362,\n",
       " 305: 363,\n",
       " 306: 364,\n",
       " 307: 365,\n",
       " 308: 366,\n",
       " 309: 368,\n",
       " 310: 369,\n",
       " 311: 370,\n",
       " 312: 371,\n",
       " 313: 372,\n",
       " 314: 373,\n",
       " 315: 374,\n",
       " 316: 375,\n",
       " 317: 376,\n",
       " 318: 377,\n",
       " 319: 378,\n",
       " 320: 379,\n",
       " 321: 380,\n",
       " 322: 381,\n",
       " 323: 384,\n",
       " 324: 385,\n",
       " 325: 386,\n",
       " 326: 387,\n",
       " 327: 388,\n",
       " 328: 390,\n",
       " 329: 392,\n",
       " 330: 393,\n",
       " 331: 394,\n",
       " 332: 395,\n",
       " 333: 399,\n",
       " 334: 401,\n",
       " 335: 402,\n",
       " 336: 404,\n",
       " 337: 407,\n",
       " 338: 408,\n",
       " 339: 409,\n",
       " 340: 410,\n",
       " 341: 412,\n",
       " 342: 413,\n",
       " 343: 414,\n",
       " 344: 415,\n",
       " 345: 416,\n",
       " 346: 418,\n",
       " 347: 419,\n",
       " 348: 420,\n",
       " 349: 421,\n",
       " 350: 422,\n",
       " 351: 423,\n",
       " 352: 424,\n",
       " 353: 425,\n",
       " 354: 426,\n",
       " 355: 430,\n",
       " 356: 431,\n",
       " 357: 432,\n",
       " 358: 438,\n",
       " 359: 439,\n",
       " 360: 440,\n",
       " 361: 442,\n",
       " 362: 443,\n",
       " 363: 444,\n",
       " 364: 445,\n",
       " 365: 446,\n",
       " 366: 447,\n",
       " 367: 448,\n",
       " 368: 449,\n",
       " 369: 450,\n",
       " 370: 451,\n",
       " 371: 452,\n",
       " 372: 453,\n",
       " 373: 454,\n",
       " 374: 455,\n",
       " 375: 456,\n",
       " 376: 457,\n",
       " 377: 458,\n",
       " 378: 459,\n",
       " 379: 461,\n",
       " 380: 462,\n",
       " 381: 463,\n",
       " 382: 464,\n",
       " 383: 465,\n",
       " 384: 469,\n",
       " 385: 470,\n",
       " 386: 471,\n",
       " 387: 472,\n",
       " 388: 473,\n",
       " 389: 474,\n",
       " 390: 476,\n",
       " 391: 477,\n",
       " 392: 478,\n",
       " 393: 479,\n",
       " 394: 480,\n",
       " 395: 481,\n",
       " 396: 482,\n",
       " 397: 483,\n",
       " 398: 484,\n",
       " 399: 486,\n",
       " 400: 488,\n",
       " 401: 489,\n",
       " 402: 490,\n",
       " 403: 491,\n",
       " 404: 492,\n",
       " 405: 496,\n",
       " 406: 498,\n",
       " 407: 499,\n",
       " 408: 500,\n",
       " 409: 501,\n",
       " 410: 503,\n",
       " 411: 505}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "[[['EX', 0, 1151], ['DE', 1, 1154], ['CONTROLE', 2, 1157], ['.', 3, 1165]], [['EX', [0], 'Teste']]]\n"
     ]
    }
   ],
   "source": [
    "print(len(dic_sentencesTest))\n",
    "print(dic_sentencesTest[503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_predictions2={}\n",
    "num=0\n",
    "for key, value in dic_predictions.items():\n",
    "    if value[1]:\n",
    "        dic_predictions2[num] = dic_predictions[key]\n",
    "        num=num+1\n",
    "        \n",
    "        #dic_sentencesTest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTest2={}\n",
    "num=0\n",
    "for key, value in dic_sentencesTest.items():\n",
    "    if value[1]:\n",
    "        dic_sentencesTest2[num] = dic_sentencesTest[key]\n",
    "        num=num+1\n",
    "        \n",
    "        #dic_sentencesTest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ARea', 0], ['de', 1], ['1', 2], [',', 3], ['12', 4], ['.', 5]],\n",
       " [['ARea', [0], 'Teste']]]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions2[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['S', 0, 448], [':', 1, 450], ['Assintomática', 2, 452], ['.', 3, 465]], []]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(regiao_pred): 3355\n",
      "len(regiao_true): 3355\n",
      "regiao_pred[:4]: ['1', '1', '0', '0']\n",
      "regiao_true[:4]: ['1', '1', '0', '0']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.841     0.849     0.845       906\n",
      "           0      0.944     0.941     0.942      2449\n",
      "\n",
      "    accuracy                          0.916      3355\n",
      "   macro avg      0.893     0.895     0.894      3355\n",
      "weighted avg      0.916     0.916     0.916      3355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regiao_true=list()\n",
    "regiao_pred=list()\n",
    "\n",
    "dicLabels={'O':'0', 'ENT':'1', 'Problema':'1', 'Tratamento':'1', 'Teste':'1', 'Anatomia':'1'}\n",
    "\n",
    "num=0\n",
    "for ent, label in zip(listacombinacoesTestPred, y_pred):\n",
    "    #print(num)\n",
    "    #print(ent)\n",
    "    numKey=dicPred[num]\n",
    "    listaTrue=dic_sentencesTest[numKey][1]\n",
    "    num=num+1\n",
    "    #print(listaTrue)\n",
    "    for entidade in ent:\n",
    "        textoEntidade=entidade[0]\n",
    "        labelEntidade=dicLabels[entidade[3]]\n",
    "        labelEntidadeTrue='0'\n",
    "        #print(textoEntidade)\n",
    "        tem=0\n",
    "        for entTrue in listaTrue:\n",
    "            textoEntidadeTrue=entTrue[0]\n",
    "            if textoEntidade == textoEntidadeTrue:\n",
    "                #print('temmm:', textoEntidade)\n",
    "                labelEntidadeTrue = dicLabels[entTrue[2]]\n",
    "                tem=1\n",
    "                break\n",
    "        if tem==1:\n",
    "            if labelEntidade == labelEntidadeTrue:\n",
    "                regiao_true.append(labelEntidadeTrue)\n",
    "                regiao_pred.append(labelEntidade)\n",
    "            else:\n",
    "                #print('aaaaaaaaaaaaa',textoEntidade)\n",
    "                #print(num)\n",
    "                regiao_true.append(labelEntidadeTrue)\n",
    "                regiao_pred.append(labelEntidade)\n",
    "        else:\n",
    "            #if labelEntidade!='0':\n",
    "                #print('nao temmm:{}, {}'.format(textoEntidade,labelEntidade))\n",
    "                #print(num)\n",
    "            regiao_true.append(labelEntidadeTrue)\n",
    "            regiao_pred.append(labelEntidade)\n",
    "                \n",
    "    #if num>40:\n",
    "    #    break\n",
    "print('len(regiao_pred):', len(regiao_pred))\n",
    "print('len(regiao_true):', len(regiao_true))\n",
    "\n",
    "print('regiao_pred[:4]:', regiao_pred[:4])\n",
    "print('regiao_true[:4]:', regiao_true[:4])\n",
    "\n",
    "print(flat_classification_report(\n",
    "    regiao_pred, regiao_true, labels=['1','0'], digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'ENT']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTiposEntidade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora, ner CRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agora, com pipeline... regiao CRF binario com NER BERT, cluster 300, janela 4\n",
      "len(regiao_pred): 3456\n",
      "len(regiao_true): 3456\n",
      "regiao_pred[:4]: ['1', '1', '0', '0']\n",
      "regiao_true[:4]: ['1', '1', '0', '0']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.796     0.821     0.808       871\n",
      "           0      0.939     0.929     0.934      2585\n",
      "\n",
      "    accuracy                          0.902      3456\n",
      "   macro avg      0.868     0.875     0.871      3456\n",
      "weighted avg      0.903     0.902     0.902      3456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# com CRF ner - BERT\n",
    "\n",
    "#transformar y_pred em dic_predictions\n",
    "dic_predictions = f.getDicPredictions(y_pred, allTokens)\n",
    "\n",
    "\n",
    "CLUSTER=300\n",
    "word2cluster = read_clusters(r\"cluster/cluster-300.tsv\")\n",
    "\n",
    "print('agora, com pipeline... regiao CRF binario com NER BERT, cluster 300, janela 4')\n",
    "\n",
    "listacombinacoesTestPred, dicPred = getListaCombinacoes(dic_predictions)\n",
    "\n",
    "\n",
    "X_test = [sent2features(s, word2cluster, dicPostagger) for s in listacombinacoesTestPred]\n",
    "#y_test = [sent2labels(s) for s in listacombinacoesTest]\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_binario_regiao\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "regiao_true=list()\n",
    "regiao_pred=list()\n",
    "\n",
    "dicLabels={'O':'0', 'ENT':'1', 'Problema':'1', 'Tratamento':'1', 'Teste':'1', 'Anatomia':'1'}\n",
    "\n",
    "num=0\n",
    "for ent, label in zip(listacombinacoesTestPred, y_pred):\n",
    "    #print(num)\n",
    "    #print(ent)\n",
    "    numKey=dicPred[num]\n",
    "    listaTrue=dic_sentencesTest[numKey][1]\n",
    "    num=num+1\n",
    "    #print(listaTrue)\n",
    "    for entidade in ent:\n",
    "        textoEntidade=entidade[0]\n",
    "        labelEntidade=dicLabels[entidade[3]]\n",
    "        labelEntidadeTrue='0'\n",
    "        #print(textoEntidade)\n",
    "        tem=0\n",
    "        for entTrue in listaTrue:\n",
    "            textoEntidadeTrue=entTrue[0]\n",
    "            if textoEntidade == textoEntidadeTrue:\n",
    "                #print('temmm:', textoEntidade)\n",
    "                labelEntidadeTrue = dicLabels[entTrue[2]]\n",
    "                tem=1\n",
    "                break\n",
    "        if tem==1:\n",
    "            if labelEntidade == labelEntidadeTrue:\n",
    "                regiao_true.append(labelEntidadeTrue)\n",
    "                regiao_pred.append(labelEntidade)\n",
    "            else:\n",
    "                #print('aaaaaaaaaaaaa',textoEntidade)\n",
    "                #print(num)\n",
    "                regiao_true.append(labelEntidadeTrue)\n",
    "                regiao_pred.append(labelEntidade)\n",
    "        else:\n",
    "            #if labelEntidade!='0':\n",
    "                #print('nao temmm:{}, {}'.format(textoEntidade,labelEntidade))\n",
    "                #print(num)\n",
    "            regiao_true.append(labelEntidadeTrue)\n",
    "            regiao_pred.append(labelEntidade)\n",
    "                \n",
    "    #if num>40:\n",
    "    #    break\n",
    "print('len(regiao_pred):', len(regiao_pred))\n",
    "print('len(regiao_true):', len(regiao_true))\n",
    "\n",
    "print('regiao_pred[:4]:', regiao_pred[:4])\n",
    "print('regiao_true[:4]:', regiao_true[:4])\n",
    "\n",
    "print(flat_classification_report(\n",
    "    regiao_pred, regiao_true, labels=['1','0'], digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora,  pipeline completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Abdomen flácido , indolor , sem visceromegalias .\n",
      "Abdomen flácido , indolor , sem visceromegalias .\n",
      "Predicted: Teste O O O O O Problema O\n",
      "Correct:   Anatomia O O O O O Problema O\n"
     ]
    }
   ],
   "source": [
    "example_sent = testdata[18]\n",
    "print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "print(' '.join(sent2tokens(example_sent)))\n",
    "print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicts_crf_1nivel.csv','w') as arq:\n",
    "    for i in range(0,len(testdata)):\n",
    "        example_sent = testdata[i]\n",
    "        #arq.write(\"\\n\\nSentence:\"+ ' '.join(sent2tokens(example_sent)))\n",
    "        #arq.write(\"\\nPredicted:\"+ ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "        #arq.write(\"\\nCorrect:  \"+ ' '.join(sent2labels(example_sent)))\n",
    "        arq.write('\"')\n",
    "        arq.write('\",\"'.join(sent2tokens(example_sent)))\n",
    "        arq.write('\"\\n\"')\n",
    "        arq.write('\",\"'.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "        arq.write('\"\\n\"')\n",
    "        arq.write('\",\"'.join(sent2labels(example_sent)))\n",
    "        arq.write('\"\\n\\n')\n",
    "        #print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "        #print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "        #print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Lucas , 74 anos .\n",
      "Predicted: O O O O O\n",
      "Correct:   O O O O O\n",
      "\n",
      "Sentence: Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .\n",
      "Predicted: O O O O O O O O Problema O O O Tratamento Tratamento O O O O O\n",
      "Correct:   O O O O O O O O Problema O O O Tratamento Tratamento O O O O O\n",
      "\n",
      "Sentence: Comorbidades : DM há 10 anos em uso de metformina 850mg 3 cp / dia , acarbose 1 cp / dia e glicazida 60mg 2 cp / dia e insulina ( 24 - 0 - 24 ) .\n",
      "Predicted: Problema O Problema O O O O O O Tratamento Tratamento O O O O O O O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O O O O\n",
      "Correct:   Problema O Problema O O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O O O O\n",
      "\n",
      "Sentence: HAS há 15 anos em uso de losartana 50mg / dia e digoxina 1 / 2 cp / dia , carvedilol 25 12 / 12 , HCTZ .\n",
      "Predicted: Problema O O O O O O Tratamento Tratamento O O O Teste O O O O O O O Tratamento Tratamento O O O O O O\n",
      "Correct:   Problema O O O O O O Tratamento Tratamento O O O Tratamento O O O O O O O Tratamento Tratamento O O O O Tratamento O\n",
      "\n",
      "Sentence: DSLP em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .\n",
      "Predicted: Problema O O O Tratamento Tratamento Tratamento O O O O O O O O O O O O\n",
      "Correct:   Problema O O O Tratamento O Tratamento O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    example_sent = testdata[i]\n",
    "    print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "    print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "    print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
