{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "import model as mod\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload \n",
    "#from eval import predict\n",
    "import eval\n",
    "#import importlib\n",
    "#importlib.reload(module)\n",
    "import dataset\n",
    "from dataset import InputFeatures, load_and_cache_examples\n",
    "import functionsAval as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 100\n"
     ]
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n",
    "reload(mod)\n",
    "\n",
    "# em numero de frases\n",
    "BATCH=100\n",
    "#BATCH=5\n",
    "#BATCH=800\n",
    "#BATCH=8000 \n",
    "print('BATCH:', BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 100\n",
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "[[['Lucas', 0], [',', 1], ['74', 2], ['anos', 3], ['.', 4]], []]\n",
      "len(dic_predictions): 100\n",
      "verificando dados:\n",
      "len(dicSentences_new_test): 100\n",
      "len(dic_predictions): 100\n",
      "region_pred_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "region_true_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "lista_erros[:8]: [7, 8, 13, 13, 14, 15, 15, 15]\n",
      "len(lista_erros): 114\n",
      "len(set(lista_erros)): 45\n",
      "len(region_true_list): 352\n",
      "len(region_pred_list): 352\n",
      "-----Avaliando só modelo de NER:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.789474  0.277778  0.410959        54\n",
      "           O   0.000000  0.000000  0.000000        38\n",
      "    Problema   0.853211  0.823009  0.837838       113\n",
      "       Teste   0.880952  0.891566  0.886228        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.667614       352\n",
      "   macro avg   0.670352  0.564096  0.592630       352\n",
      "weighted avg   0.753305  0.667614  0.691546       352\n",
      "\n",
      "[[15 36  1  1  1]\n",
      " [ 4  0 15  9 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n",
    "tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "#print(dic_predictions[9])\n",
    "f.save_obj('dic_predictions_results_ner_'+str(BATCH), dic_predictions)\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "print('verificando dados:')\n",
    "#for key, value in dic_predictions.items():\n",
    "#    print('key:',key)\n",
    "#    print(dic_predictions[key])\n",
    "#    if key>2:\n",
    "#        break\n",
    "        \n",
    "print('len(dicSentences_new_test):', len(dicSentences_new_test))\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "f.save_obj('region_true_list'+str(BATCH), region_true_list)\n",
    "print('region_pred_list[:4]:', region_pred_list[:4])\n",
    "print('region_true_list[:4]:', region_true_list[:4])\n",
    "print('lista_erros[:8]:', lista_erros[:8])\n",
    "print('len(lista_erros):', len(lista_erros))\n",
    "print('len(set(lista_erros)):', len(set(lista_erros)))\n",
    "#print(dic_predictions[8])\n",
    "#print(dicSentences_new_test[8][1])\n",
    "print('len(region_true_list):', len(region_true_list))\n",
    "print('len(region_pred_list):', len(region_pred_list))\n",
    "#print('pred:',region_pred_list[:15])\n",
    "#print('true:',region_true_list[:15])\n",
    "\n",
    "print('-----Avaliando só modelo de NER:-----')\n",
    "\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF primeiro nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicPostagger = f.load_obj('dic_postagger')\n",
    "def tipoPostaggerTokens(token, dicPostagger):\n",
    "    postagger = 'N' # na duvida é N\n",
    "    if token.lower() in dicPostagger.keys():\n",
    "        postagger = dicPostagger.get(token.lower())\n",
    "    return postagger\n",
    "tipoPostaggerTokens('coração', dicPostagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lisat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "def has_numbers(inputString):\n",
    "    return str(bool(re.search(r'\\d', inputString)))\n",
    "\n",
    "print(has_numbers('metionina 5mg'))\n",
    "\n",
    "def isstopword(p):\n",
    "  return str(p in stopwords.words('portuguese'))\n",
    "\n",
    "print(isstopword('pelo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER = 50\n",
    "JANELA=3\n",
    "\n",
    "def read_clusters(cluster_file):\n",
    "    word2cluster = {}\n",
    "    try:\n",
    "        with open(cluster_file, encoding='utf-8') as i:\n",
    "            for num, line in enumerate(i):\n",
    "                if line:\n",
    "                    word, cluster = line.strip().split('\\t')\n",
    "                    word2cluster[word] = cluster\n",
    "    except:\n",
    "        raise\n",
    "    return word2cluster\n",
    "\n",
    "def word2features(sent, i):\n",
    "  #if str(sent[i][0])!='nan':#esto es extra\n",
    "    word = sent[i][0]\n",
    "    postag = tipoPostaggerTokens(word, dicPostagger)\n",
    "    cluster = word2cluster[word.lower()] if word.lower() in word2cluster else \"0\"\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'word.cluster': cluster\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i > 1 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-2][0]\n",
    "        #postag1 = sent[i-2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:word.istitle()': word1.istitle(),\n",
    "            '-2:word.isupper()': word1.isupper(),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_word'] = True\n",
    "\n",
    "    if i > 2 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i-1][0]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-3:word.lower()': word1.lower(),\n",
    "            '-3:word.istitle()': word1.istitle(),\n",
    "            '-3:word.isupper()': word1.isupper(),\n",
    "            '-3:postag': postag1,\n",
    "            '-3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_word'] = True\n",
    "    \n",
    "    if i > 3 and JANELA==4:\n",
    "        word1 = sent[i-2][0]\n",
    "        #postag1 = sent[i-2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '-4:word.lower()': word1.lower(),\n",
    "            '-4:word.istitle()': word1.istitle(),\n",
    "            '-4:word.isupper()': word1.isupper(),\n",
    "            '-4:postag': postag1,\n",
    "            '-4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_word'] = True\n",
    "    \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        #postag1 = sent[i+1][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    if i < len(sent)-2 and (JANELA==2 or JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+2][0]\n",
    "        #postag1 = sent[i+2][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:word.istitle()': word1.istitle(),\n",
    "            '+2:word.isupper()': word1.isupper(),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Second_to_last'] = True\n",
    "    if i < len(sent)-3 and (JANELA==3 or JANELA==4):\n",
    "        word1 = sent[i+3][0]\n",
    "        #postag1 = sent[i+3][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+3:word.lower()': word1.lower(),\n",
    "            '+3:word.istitle()': word1.istitle(),\n",
    "            '+3:word.isupper()': word1.isupper(),\n",
    "            '+3:postag': postag1,\n",
    "            '+3:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Third_to_last'] = True\n",
    "    \n",
    "    if i < len(sent)-4 and JANELA==4:\n",
    "        word1 = sent[i+4][0]\n",
    "        #postag1 = sent[i+4][1]\n",
    "        postag1 = tipoPostaggerTokens(word1, dicPostagger)\n",
    "        cluster = word2cluster[word1.lower()] if word1.lower() in word2cluster else \"0\"\n",
    "        features.update({\n",
    "            '+4:word.lower()': word1.lower(),\n",
    "            '+4:word.istitle()': word1.istitle(),\n",
    "            '+4:word.isupper()': word1.isupper(),\n",
    "            '+4:postag': postag1,\n",
    "            '+4:word.cluster': cluster\n",
    "        })\n",
    "    else:\n",
    "        features['Fourth_to_last'] = True\n",
    "    \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "def sent2tokensPredict(sent):\n",
    "    return [token for token in sent]\n",
    "\n",
    "\n",
    "word2cluster = read_clusters(r\"cluster/cluster-\"+str(CLUSTER)+\".tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTest=r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\data-ner/nested_test.conll'\n",
    "\n",
    "with open(pathTest, encoding='utf-8') as fi:\n",
    "  testdata = [[tuple(w.split(' ')) for w in snt.split('\\n')] for snt in fi.read().split('\\n\\n')]\n",
    "\n",
    "X_test = [sent2features(s) for s in testdata]\n",
    "y_test = [sent2labels(s) for s in testdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abdomen', 'Anatomia'),\n",
       " ('flácido', 'O'),\n",
       " (',', 'O'),\n",
       " ('indolor', 'O'),\n",
       " (',', 'O'),\n",
       " ('sem', 'O'),\n",
       " ('visceromegalias', 'Problema'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'em',\n",
       "  'word[-3:]': 'Em',\n",
       "  'word[:3]': 'Em',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  'BOS': True,\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'acompanhamento',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '29',\n",
       "  '+2:word.lower()': 'no',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'ART',\n",
       "  '+2:word.cluster': '24',\n",
       "  '+3:word.lower()': 'ambualtorio',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'acompanhamento',\n",
       "  'word[-3:]': 'nto',\n",
       "  'word[:3]': 'aco',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '29',\n",
       "  '-1:word.lower()': 'em',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'no',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'ART',\n",
       "  '+1:word.cluster': '24',\n",
       "  '+2:word.lower()': 'ambualtorio',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'há',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'V',\n",
       "  '+3:word.cluster': '23',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-3:]': 'no',\n",
       "  'word[:3]': 'no',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'ART',\n",
       "  'word.cluster': '24',\n",
       "  '-1:word.lower()': 'acompanhamento',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '29',\n",
       "  '-2:word.lower()': 'em',\n",
       "  '-2:word.istitle()': True,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ambualtorio',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'há',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'V',\n",
       "  '+2:word.cluster': '23',\n",
       "  '+3:word.lower()': '5',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ambualtorio',\n",
       "  'word[-3:]': 'rio',\n",
       "  'word[:3]': 'amb',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '24',\n",
       "  '-2:word.lower()': 'acompanhamento',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '29',\n",
       "  '-3:word.lower()': 'no',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'ART',\n",
       "  '-3:word.cluster': '24',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'há',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'V',\n",
       "  '+1:word.cluster': '23',\n",
       "  '+2:word.lower()': '5',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'anos',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'há',\n",
       "  'word[-3:]': 'há',\n",
       "  'word[:3]': 'há',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'V',\n",
       "  'word.cluster': '23',\n",
       "  '-1:word.lower()': 'ambualtorio',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': 'no',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'ART',\n",
       "  '-2:word.cluster': '24',\n",
       "  '-3:word.lower()': 'ambualtorio',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'anos',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': 'por',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '2',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5',\n",
       "  'word[-3:]': '5',\n",
       "  'word[:3]': '5',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': 'há',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '23',\n",
       "  '-2:word.lower()': 'ambualtorio',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'há',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'V',\n",
       "  '-3:word.cluster': '23',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'anos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': 'por',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '2',\n",
       "  '+3:word.lower()': 'fa',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': True,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'anos',\n",
       "  'word[-3:]': 'nos',\n",
       "  'word[:3]': 'ano',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': '5',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': 'há',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'V',\n",
       "  '-2:word.cluster': '23',\n",
       "  '-3:word.lower()': '5',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'por',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '2',\n",
       "  '+2:word.lower()': 'fa',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': True,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': ',',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'por',\n",
       "  'word[-3:]': 'por',\n",
       "  'word[:3]': 'por',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '2',\n",
       "  '-1:word.lower()': 'anos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': '5',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'anos',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'fa',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': ',',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'uso',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'fa',\n",
       "  'word[-3:]': 'FA',\n",
       "  'word[:3]': 'FA',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': 'por',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '2',\n",
       "  '-2:word.lower()': 'anos',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': 'por',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '2',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'uso',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': 'de',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[:3]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'fa',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:postag': 'Problema',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': 'por',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '2',\n",
       "  '-3:word.lower()': 'fa',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': True,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'uso',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': 'de',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'marevan',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'uso',\n",
       "  'word[-3:]': 'uso',\n",
       "  'word[:3]': 'uso',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'fa',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': True,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': ',',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PU',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'marevan',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '5mg',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word[:3]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'uso',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': ',',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PU',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'uso',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'marevan',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '5mg',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '1',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'marevan',\n",
       "  'word[-3:]': 'van',\n",
       "  'word[:3]': 'mar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'uso',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': 'de',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5mg',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '1',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'x',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5mg',\n",
       "  'word[-3:]': '5mg',\n",
       "  'word[:3]': '5mg',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'marevan',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'Tratamento',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'de',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'marevan',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '1',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'x',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': 'ao',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '1',\n",
       "  'word[-3:]': '1',\n",
       "  'word[:3]': '1',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': '5mg',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'Tratamento',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'marevan',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': '5mg',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'x',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': 'ao',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'dia',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'x',\n",
       "  'word[-3:]': 'x',\n",
       "  'word[:3]': 'x',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': '1',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': '5mg',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': '1',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ao',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'dia',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '.',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ao',\n",
       "  'word[-3:]': 'ao',\n",
       "  'word[:3]': 'ao',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'x',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': '1',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'x',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'dia',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '.',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '26',\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'dia',\n",
       "  'word[-3:]': 'dia',\n",
       "  'word[:3]': 'dia',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'ao',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'x',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': 'ao',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '26',\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[:3]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'dia',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'ao',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'dia',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  'EOS': True,\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 weighted: 0.8367784189386672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Problema      0.761     0.884     0.818       795\n",
      "       Teste      0.859     0.828     0.843       308\n",
      "  Tratamento      0.847     0.922     0.883       446\n",
      "    Anatomia      0.758     0.758     0.758        95\n",
      "\n",
      "   micro avg      0.800     0.877     0.837      1644\n",
      "   macro avg      0.806     0.848     0.825      1644\n",
      "weighted avg      0.802     0.877     0.837      1644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#AVAL no conjunto de teste do ner (flat)\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "\n",
    "finalScore = flat_f1_score(y_test, y_pred, average='weighted', labels=getTiposEntidade())\n",
    "print(\"F1 weighted:\",finalScore)\n",
    "\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'em',\n",
       "  'word[-3:]': 'Em',\n",
       "  'word[:3]': 'Em',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  'BOS': True,\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'acompanhamento',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '29',\n",
       "  '+2:word.lower()': 'no',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'ART',\n",
       "  '+2:word.cluster': '24',\n",
       "  '+3:word.lower()': 'ambualtorio',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'acompanhamento',\n",
       "  'word[-3:]': 'nto',\n",
       "  'word[:3]': 'aco',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '29',\n",
       "  '-1:word.lower()': 'em',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  'Second_word': True,\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'no',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'ART',\n",
       "  '+1:word.cluster': '24',\n",
       "  '+2:word.lower()': 'ambualtorio',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'há',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'V',\n",
       "  '+3:word.cluster': '23',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-3:]': 'no',\n",
       "  'word[:3]': 'no',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'ART',\n",
       "  'word.cluster': '24',\n",
       "  '-1:word.lower()': 'acompanhamento',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '29',\n",
       "  '-2:word.lower()': 'em',\n",
       "  '-2:word.istitle()': True,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  'Third_word': True,\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ambualtorio',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'há',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'V',\n",
       "  '+2:word.cluster': '23',\n",
       "  '+3:word.lower()': '5',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ambualtorio',\n",
       "  'word[-3:]': 'rio',\n",
       "  'word[:3]': 'amb',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '24',\n",
       "  '-2:word.lower()': 'acompanhamento',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '29',\n",
       "  '-3:word.lower()': 'no',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'ART',\n",
       "  '-3:word.cluster': '24',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'há',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'V',\n",
       "  '+1:word.cluster': '23',\n",
       "  '+2:word.lower()': '5',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'anos',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'há',\n",
       "  'word[-3:]': 'há',\n",
       "  'word[:3]': 'há',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'V',\n",
       "  'word.cluster': '23',\n",
       "  '-1:word.lower()': 'ambualtorio',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': 'no',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'ART',\n",
       "  '-2:word.cluster': '24',\n",
       "  '-3:word.lower()': 'ambualtorio',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'anos',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': 'por',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '2',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5',\n",
       "  'word[-3:]': '5',\n",
       "  'word[:3]': '5',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': 'há',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '23',\n",
       "  '-2:word.lower()': 'ambualtorio',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'há',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'V',\n",
       "  '-3:word.cluster': '23',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'anos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': 'por',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '2',\n",
       "  '+3:word.lower()': 'fa',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': True,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'anos',\n",
       "  'word[-3:]': 'nos',\n",
       "  'word[:3]': 'ano',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': '5',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': 'há',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'V',\n",
       "  '-2:word.cluster': '23',\n",
       "  '-3:word.lower()': '5',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'por',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '2',\n",
       "  '+2:word.lower()': 'fa',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': True,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': ',',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'por',\n",
       "  'word[-3:]': 'por',\n",
       "  'word[:3]': 'por',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '2',\n",
       "  '-1:word.lower()': 'anos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': '5',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'anos',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'fa',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': ',',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'uso',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'fa',\n",
       "  'word[-3:]': 'FA',\n",
       "  'word[:3]': 'FA',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': 'por',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '2',\n",
       "  '-2:word.lower()': 'anos',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': 'por',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '2',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'uso',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': 'de',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[:3]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'fa',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:postag': 'Problema',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': 'por',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '2',\n",
       "  '-3:word.lower()': 'fa',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': True,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'uso',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': 'de',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'marevan',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'uso',\n",
       "  'word[-3:]': 'uso',\n",
       "  'word[:3]': 'uso',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'fa',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': True,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': ',',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PU',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'marevan',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '5mg',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word[:3]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'uso',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': ',',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PU',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'uso',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'marevan',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '5mg',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '1',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'NUM',\n",
       "  '+3:word.cluster': '0',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'marevan',\n",
       "  'word[-3:]': 'van',\n",
       "  'word[:3]': 'mar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'uso',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': 'de',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '5mg',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '1',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'NUM',\n",
       "  '+2:word.cluster': '0',\n",
       "  '+3:word.lower()': 'x',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '8',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '5mg',\n",
       "  'word[-3:]': '5mg',\n",
       "  'word[:3]': '5mg',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'marevan',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'Tratamento',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'de',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'marevan',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '1',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'NUM',\n",
       "  '+1:word.cluster': '0',\n",
       "  '+2:word.lower()': 'x',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '8',\n",
       "  '+3:word.lower()': 'ao',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PREP',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '1',\n",
       "  'word[-3:]': '1',\n",
       "  'word[:3]': '1',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'NUM',\n",
       "  'word.cluster': '0',\n",
       "  '-1:word.lower()': '5mg',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'Tratamento',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'marevan',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': '5mg',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'x',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '8',\n",
       "  '+2:word.lower()': 'ao',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PREP',\n",
       "  '+2:word.cluster': '26',\n",
       "  '+3:word.lower()': 'dia',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'N',\n",
       "  '+3:word.cluster': '17',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'x',\n",
       "  'word[-3:]': 'x',\n",
       "  'word[:3]': 'x',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '8',\n",
       "  '-1:word.lower()': '1',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '0',\n",
       "  '-2:word.lower()': '5mg',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '17',\n",
       "  '-3:word.lower()': '1',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'NUM',\n",
       "  '-3:word.cluster': '0',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'ao',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PREP',\n",
       "  '+1:word.cluster': '26',\n",
       "  '+2:word.lower()': 'dia',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'N',\n",
       "  '+2:word.cluster': '17',\n",
       "  '+3:word.lower()': '.',\n",
       "  '+3:word.istitle()': False,\n",
       "  '+3:word.isupper()': False,\n",
       "  '+3:postag': 'PU',\n",
       "  '+3:word.cluster': '26',\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ao',\n",
       "  'word[-3:]': 'ao',\n",
       "  'word[:3]': 'ao',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PREP',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'x',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '8',\n",
       "  '-2:word.lower()': '1',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'NUM',\n",
       "  '-2:word.cluster': '0',\n",
       "  '-3:word.lower()': 'x',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '8',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': 'dia',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'N',\n",
       "  '+1:word.cluster': '17',\n",
       "  '+2:word.lower()': '.',\n",
       "  '+2:word.istitle()': False,\n",
       "  '+2:word.isupper()': False,\n",
       "  '+2:postag': 'PU',\n",
       "  '+2:word.cluster': '26',\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'dia',\n",
       "  'word[-3:]': 'dia',\n",
       "  'word[:3]': 'dia',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'N',\n",
       "  'word.cluster': '17',\n",
       "  '-1:word.lower()': 'ao',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '26',\n",
       "  '-2:word.lower()': 'x',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'N',\n",
       "  '-2:word.cluster': '8',\n",
       "  '-3:word.lower()': 'ao',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'PREP',\n",
       "  '-3:word.cluster': '26',\n",
       "  'Fourth_word': True,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'PU',\n",
       "  '+1:word.cluster': '26',\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[:3]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'PU',\n",
       "  'word.cluster': '26',\n",
       "  '-1:word.lower()': 'dia',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'O',\n",
       "  '-1:word.cluster': '17',\n",
       "  '-2:word.lower()': 'ao',\n",
       "  '-2:word.istitle()': False,\n",
       "  '-2:word.isupper()': False,\n",
       "  '-2:postag': 'PREP',\n",
       "  '-2:word.cluster': '26',\n",
       "  '-3:word.lower()': 'dia',\n",
       "  '-3:word.istitle()': False,\n",
       "  '-3:word.isupper()': False,\n",
       "  '-3:postag': 'N',\n",
       "  '-3:word.cluster': '17',\n",
       "  'Fourth_word': True,\n",
       "  'EOS': True,\n",
       "  'Second_to_last': True,\n",
       "  'Third_to_last': True,\n",
       "  'Fourth_to_last': True}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list()\n",
    "frases=list()\n",
    "allTokens=list()\n",
    "listatokens=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        #print(tokens)\n",
    "        #tokens = [tok[0] for tok in tokens]\n",
    "        #sentences.append(' '.join(tokens).strip())\n",
    "        for token in tokens:\n",
    "            listatokens.append(token[0])\n",
    "            #sentences.append(tuple([token[0],'O']))\n",
    "            if len(token)>0:\n",
    "                #t = token[0]\n",
    "                #print('t:', t)\n",
    "                #print('tuple(t):', tuple(t))\n",
    "                #sentences.append(tuple(t))\n",
    "                frases.append(tuple(token))\n",
    "        sentences.append(frases)\n",
    "        allTokens.append(listatokens)\n",
    "        listatokens=list()\n",
    "        frases=list()\n",
    "    break\n",
    "print('sentences[0]:', sentences[0])\n",
    "print(allTokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-f35f362a98b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;31m#print('tuple(t):', tuple(t))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;31m#sentences.append(tuple(t))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mfrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mallTokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "'''\n",
    "example_sent = testdata[18]\n",
    "print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "print(' '.join(sent2tokens(example_sent)))\n",
    "print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n",
    "'''\n",
    "\n",
    "sentences=list()\n",
    "frases=list()\n",
    "allTokens=list()\n",
    "tokens=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        #print(tokens)\n",
    "        #tokens = [tok[0] for tok in tokens]\n",
    "        #sentences.append(' '.join(tokens).strip())\n",
    "        for token in tokens:\n",
    "            tokens.append(token[0])\n",
    "            #sentences.append(tuple([token[0],'O']))\n",
    "            if len(token)>0:\n",
    "                #t = token[0]\n",
    "                #print('t:', t)\n",
    "                #print('tuple(t):', tuple(t))\n",
    "                #sentences.append(tuple(t))\n",
    "                frases.append(tuple(token))\n",
    "        sentences.append(frases)\n",
    "        allTokens.append(tokens)\n",
    "        tokens=list()\n",
    "        frases=list()\n",
    "print('sentences[0]:', sentences[0])\n",
    "print(allTokens[0])\n",
    "\n",
    "#testdata = [[tuple(w.split(' ')) for w in snt.split('\\n')] for snt in f.read().split('\\n\\n')]\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "#X_test = [sent2tokensPredict(s) for s in sentences]\n",
    "X_test = [sent2features(s) for s in sentences]\n",
    "#print('X_test[0]:', X_test[0])\n",
    "print('len(sentences):', len(sentences))\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "print('len(y_pred):', len(y_pred))\n",
    "\n",
    "for tags, tokens in zip(y_pred, allTokens):\n",
    "    print(tokens)\n",
    "    dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "print('-----Avaliando só modelo de NER com CRF:-----')\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "#tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "#dic_predictions = f.getDicPredictions(tags, tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'O', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['Problema', 'Problema', 'O', 'O', 'Problema', 'O', 'O'],\n",
       " ['Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['Anatomia', 'O', 'Problema', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['Teste', 'O', 'Teste', 'O', 'Problema', 'Problema', 'O', 'Teste', 'O', 'O'],\n",
       " ['Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['Teste',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['Problema', 'Problema', 'O', 'Problema', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'Problema', 'O'],\n",
       " ['Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['Anatomia', 'O', 'O', 'Problema', 'O', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'Tratamento', 'O', 'Tratamento', 'O', 'Tratamento', 'O', 'O', 'O'],\n",
       " ['Problema', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'Problema', 'Problema', 'O', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'Problema', 'Teste', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'O', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'Tratamento', 'O'],\n",
       " ['O', 'Tratamento', 'Tratamento', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Teste',\n",
       "  'O'],\n",
       " ['O', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'Tratamento', 'O', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'Problema', 'Problema', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'Tratamento', 'Anatomia', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'O', 'Tratamento', 'O', 'Tratamento', 'O', 'Tratamento', 'O'],\n",
       " ['O', 'O', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'O'],\n",
       " ['O', 'O', 'Teste', 'O', 'O', 'O', 'O', 'Teste', 'O', 'O'],\n",
       " ['Anatomia', 'O', 'O'],\n",
       " ['Anatomia', 'O', 'O'],\n",
       " ['Anatomia', 'O', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'Problema', 'O', 'Teste', 'O', 'Teste', 'O'],\n",
       " ['Problema', 'Problema', 'Problema', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'Problema', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'Problema', 'O', 'Tratamento', 'O'],\n",
       " ['Tratamento', 'Tratamento', 'Problema', 'O', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Anatomia',\n",
       "  'Anatomia',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['Problema',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'Teste', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['Anatomia', 'O', 'Problema', 'Problema', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['Tratamento', 'Problema', 'Problema', 'O'],\n",
       " ['Problema', 'Problema', 'O'],\n",
       " ['O', 'Anatomia', 'O', 'O'],\n",
       " ['Anatomia', 'O', 'O'],\n",
       " ['Teste', 'O', 'Problema', 'O', 'O', 'O', 'O', 'Teste', 'O', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'O', 'Problema', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Teste',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'Problema',\n",
       "  'O'],\n",
       " ['O', 'O', 'Tratamento', 'Tratamento', 'Tratamento', 'O', 'O'],\n",
       " ['Problema', 'Problema', 'O'],\n",
       " ['O', 'O', 'O', 'Tratamento', 'O'],\n",
       " ['O', 'Tratamento', 'O', 'Tratamento', 'O', 'O', 'O'],\n",
       " ['O', 'Tratamento', 'Tratamento', 'O', 'Tratamento', 'Tratamento', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'Teste', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'Problema', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'Tratamento',\n",
       "  'Tratamento',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'Problema', 'O'],\n",
       " ['O', 'O', 'Problema', 'O', 'O', 'Problema', 'O', 'O', 'Problema', 'O'],\n",
       " ['O', 'Problema', 'Problema', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O',\n",
       "  'Problema',\n",
       "  'O']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "#AVAL no conjunto de teste com as entidades aninhadas\n",
    "\n",
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']\n",
    "\n",
    "OUTPUT_PATH = \"CRF\"\n",
    "OUTPUT_FILE = \"crf_model_primeiro_nivel\"\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "\n",
    "finalScore = flat_f1_score(y_test, y_pred, average='weighted', labels=getTiposEntidade())\n",
    "print(\"F1 weighted:\",finalScore)\n",
    "\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=getTiposEntidade(), digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Abdomen flácido , indolor , sem visceromegalias .\n",
      "Abdomen flácido , indolor , sem visceromegalias .\n",
      "Predicted: Teste O O O O O Problema O\n",
      "Correct:   Anatomia O O O O O Problema O\n"
     ]
    }
   ],
   "source": [
    "example_sent = testdata[18]\n",
    "print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "print(' '.join(sent2tokens(example_sent)))\n",
    "print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Lucas , 74 anos .\n",
      "Predicted: O O O O O\n",
      "Correct:   O O O O O\n",
      "\n",
      "Sentence: Em acompanhamento no ambualtorio há 5 anos por FA , uso de marevan 5mg 1 x ao dia .\n",
      "Predicted: O O O O O O O O Problema O O O Tratamento Tratamento O O O O O\n",
      "Correct:   O O O O O O O O Problema O O O Tratamento Tratamento O O O O O\n",
      "\n",
      "Sentence: Comorbidades : DM há 10 anos em uso de metformina 850mg 3 cp / dia , acarbose 1 cp / dia e glicazida 60mg 2 cp / dia e insulina ( 24 - 0 - 24 ) .\n",
      "Predicted: Problema O Problema O O O O O O Tratamento Tratamento O O O O O O O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O O O O\n",
      "Correct:   Problema O Problema O O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O Tratamento Tratamento O O O O O Tratamento O O O O O O O O\n",
      "\n",
      "Sentence: HAS há 15 anos em uso de losartana 50mg / dia e digoxina 1 / 2 cp / dia , carvedilol 25 12 / 12 , HCTZ .\n",
      "Predicted: Problema O O O O O O Tratamento Tratamento O O O Teste O O O O O O O Tratamento Tratamento O O O O O O\n",
      "Correct:   Problema O O O O O O Tratamento Tratamento O O O Tratamento O O O O O O O Tratamento Tratamento O O O O Tratamento O\n",
      "\n",
      "Sentence: DSLP em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .\n",
      "Predicted: Problema O O O Tratamento Tratamento Tratamento O O O O O O O O O O O O\n",
      "Correct:   Problema O O O Tratamento O Tratamento O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    example_sent = testdata[i]\n",
    "    print(\"\\nSentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "    print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent)])[0]))\n",
    "    print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelo de sentence-pais\n",
    "#### primeiro sozinho...\n",
    "1) Com filtro + downsampling\n",
    "\n",
    "2) Com filtro\n",
    "\n",
    "3) Só positivos (dai preciso de um CRF pro filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando só com modelo de Sentence Pairs:-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeb1fc41af44b0bf95421a0455505a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2d8c8fdc44bf2bd9533f1db8aaf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"version\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                     raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1792: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-03d79b075fa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----Avaliando só com modelo de Sentence Pairs:-----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lisaterumi/sentence_pairs_nested_filtro_downsampling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 self.model = model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    411\u001b[0m                     \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m                 \u001b[1;31m# Time to load the checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2132\u001b[1;33m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m             \u001b[1;31m# set dtype to instantiate the model under:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    475\u001b[0m                     ) from e\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             raise OSError(\n\u001b[0m\u001b[0;32m    478\u001b[0m                 \u001b[1;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "print('-----Avaliando só com modelo de Sentence Pairs:-----')\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro_downsampling', use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[0,0]\n",
    "[num for num in range(a[0], a[1]+1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = reload(f)\n",
    "#combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "#combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de sentenças de test: 1,913\n",
      "\n",
      "         text_b                                             text_a  labels\n",
      "0   marevan 5mg  Em acompanhamento no ambualtorio há 5 anos por...       2\n",
      "1       marevan  Em acompanhamento no ambualtorio há 5 anos por...       0\n",
      "2  Comorbidades  Comorbidades : DM há 10 anos em uso de metform...       1\n",
      "Primeiro, com filtro postagger:\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 14\n",
      "len(combinacaoEntidadesAll:) 100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"..\\preProcessamento\\sentence-pairs-filtro\\sentence_pairs.test\", delimiter='\\t', header=0, names=['text_b', 'text_a', 'labels'])\n",
    "df = df.dropna(axis=0, how='any')\n",
    "print('Número de sentenças de test: {:,}\\n'.format(df.shape[0]))\n",
    "#df=df[:50]\n",
    "test_df = df\n",
    "print(test_df[:3])\n",
    "lista=list()\n",
    "#for index, row in test_df.iterrows():\n",
    "#    lista.append([row['text_b'], row['text_a']])\n",
    "#predictions, raw_outputs = model.predict(\n",
    "#  lista\n",
    "#)\n",
    "dic_sentencesTrainDev = f.load_obj('dic_sentencesTrainDev')\n",
    "dicPosTagger, _ = f.getDicPosTagger(dic_sentencesTrainDev)\n",
    "lista_postaggers_entidades = f.getListaPostaggerEntidades(dic_predictions, dicPosTagger)\n",
    "# para gerar arquivo de predicoes (com as tags <e1>)\n",
    "print('Primeiro, com filtro postagger:')\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "f.save_obj('combinacaoEntidadesAllSentence_'+str(BATCH), combinacaoEntidadesAll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# para suprimir output\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "pred_region_labels2 = list()\n",
    "for a in pred_region_labels:\n",
    "    for b in a:\n",
    "        pred_region_labels2.append(labels[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Problema', 'Tratamento', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'O']\n",
      "['Problema', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'Problema', 'Tratamento']\n",
      "564\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "print(pred_region_labels2[:10])\n",
    "print(region_true_list[:10])\n",
    "print(len(pred_region_labels2))\n",
    "print(len(region_true_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0],\n",
       "  ['há', 1],\n",
       "  ['15', 2],\n",
       "  ['anos', 3],\n",
       "  ['em', 4],\n",
       "  ['uso', 5],\n",
       "  ['de', 6],\n",
       "  ['losartana', 7],\n",
       "  ['50mg', 8],\n",
       "  ['/', 9],\n",
       "  ['dia', 10],\n",
       "  ['e', 11],\n",
       "  ['digoxina', 12],\n",
       "  ['1', 13],\n",
       "  ['/', 14],\n",
       "  ['2', 15],\n",
       "  ['cp', 16],\n",
       "  ['/', 17],\n",
       "  ['dia', 18],\n",
       "  [',', 19],\n",
       "  ['carvedilol', 20],\n",
       "  ['25', 21],\n",
       "  ['12', 22],\n",
       "  ['/', 23],\n",
       "  ['12', 24],\n",
       "  [',', 25],\n",
       "  ['HCTZ', 26],\n",
       "  ['.', 27]],\n",
       " [['HAS', [0], 'Problema'],\n",
       "  ['losartana 50mg', [7, 8], 'Tratamento'],\n",
       "  ['digoxina', [12], 'Tratamento'],\n",
       "  ['carvedilol 25', [20, 21], 'Tratamento'],\n",
       "  ['HCTZ', [26], 'Tratamento']]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[4]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            if tipo_previsto!=0:\n",
    "                entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade está na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Região):-----\n",
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Região):-----\n",
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro (Região):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df462b3fed964f1395551a965d628a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c631420dc345c384995d2f4292b591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd449083b554e778e5be575c8fd43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ecebf97cad44a39e22d0bc0c352897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6f1a1bba394feb855337afd28b9a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro (Região):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 2],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence2(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[5]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade está na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "# om 30 epocas\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n",
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD\n",
    "dic_predictions_sentence = getDicPredictionsSentence2(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31  17   4   2   0]\n",
      " [  6 145  81  24  29]\n",
      " [  0  17  96   0   0]\n",
      " [  1   7   0  75   0]\n",
      " [  0   8   0   0  56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'O']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_true_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_pred_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0, 269],\n",
       "  ['refere', 1, 278],\n",
       "  ['fraqueza', 2, 285],\n",
       "  ['intermitente', 3, 294],\n",
       "  ['em', 4, 307],\n",
       "  ['MMII', 5, 310],\n",
       "  [',', 6, 314],\n",
       "  ['associada', 7, 316],\n",
       "  ['a', 8, 324],\n",
       "  ['tontura', 9, 328],\n",
       "  [',', 10, 335],\n",
       "  ['turvação', 11, 337],\n",
       "  ['visual', 12, 346],\n",
       "  ['e', 13, 353],\n",
       "  ['epigastralgia', 14, 355],\n",
       "  ['.', 15, 368]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicSentences_new_test[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['fraqueza intermitente', [2, 3], 'Problema'],\n",
       "  ['intermitente em MMII', [3, 4, 5], 'Problema']]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo sem filtro (Região):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102499d488da4d2b9b3bf9b3ba83b669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0291985584e48aa5ff3f341718027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d2bf402f9d49249b138aa89354960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b15b3ff136468c96cb99f1187c19e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8843ea20e91b4a72b245324511bd7fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681a40ef2be4d2f97dbf0f4d4919c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "# AKI rodar de novo\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, False, '', 0, '')\n",
    "print('-----Avaliando modelo sem filtro (Região):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_all', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-sem-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentence_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "f.save_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Otimizo', 0], ['dose', 1], ['da', 2], ['sinvastatina', 3], ['para', 4], ['40mg', 5], ['/', 6], ['dia', 7], ['.', 8]], [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'], ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]\n",
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "print(dic_predictions_sentence[20])\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.753695  0.536842  0.627049       285\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.686144       599\n",
      "   macro avg   0.708428  0.764704  0.722687       599\n",
      "weighted avg   0.712228  0.686144  0.683106       599\n",
      "\n",
      "[[ 38  12   4   0   0]\n",
      " [ 10 153  77  12  33]\n",
      " [  0  26  86   1   0]\n",
      " [  0   7   0  76   0]\n",
      " [  0   5   0   1  58]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O \n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, merge com dic_predictions?\n",
    "\n",
    "Acho que nao precisa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, com pos processamento\n",
    "\n",
    "Regras:\n",
    "\n",
    "- Pega os de dentro (aninhadas), apenas se o tipo for diferente do tipo da entidade de fora... se for do mesmo tipo, não pega..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isEntidadeAninhada(a, b):\n",
    "    inicio1=a[0]\n",
    "    fim1=a[-1]\n",
    "    #print(inicio1, fim1)\n",
    "    inicio2=b[0]\n",
    "    fim2=b[-1]\n",
    "    #print(inicio2, fim2)\n",
    "    if inicio1 >=inicio2 and fim1 <=fim2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "isEntidadeAninhada([2], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento\n",
      "numErro1: 65\n",
      "numErro2: 43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.822222  0.685185  0.747475        54\n",
      "           O   0.000000  0.000000  0.000000        43\n",
      "    Problema   0.838384  0.734513  0.783019       113\n",
      "       Teste   0.860465  0.891566  0.875740        83\n",
      "  Tratamento   0.838710  0.812500  0.825397        64\n",
      "\n",
      "    accuracy                       0.689076       357\n",
      "   macro avg   0.671956  0.624753  0.646326       357\n",
      "weighted avg   0.740150  0.689076  0.712483       357\n",
      "\n",
      "[[37 16  1  0  0]\n",
      " [ 8  0 15 10 10]\n",
      " [ 0 29 83  1  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  1 52]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento')\n",
    "\n",
    "def posProcessamento(dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions_sentence.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = list()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeNer in listaEntidadesSpan:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            #print('entidadeSpan:', entidadeSpan)\n",
    "            indices = entidadeSpan[1]\n",
    "            #print('incluindo:', entidadeSpan)\n",
    "            # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "            # ver pelo indice\n",
    "            isAninhada=False\n",
    "            isMesmoTipo=False\n",
    "            tipo=entidadeSpan[2]\n",
    "            for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                isAninhada=False\n",
    "                if indices!=indicesNer:\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                if isAninhada and tipoNer == tipo:\n",
    "                    isMesmoTipo=True\n",
    "                    #print('-----mesmo tipo!!')\n",
    "                    #print('entidadeSpan:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    #print('indices:', indices)\n",
    "                    #print('indicesNer:', indicesNer)\n",
    "                    #print('indices!=indicesNer:', indices!=indicesNer)\n",
    "                    break\n",
    "            # ver pelo tipo tb\n",
    "            if not (isAninhada and isMesmoTipo):\n",
    "                #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                listaEntidadesNer.append(entidadeSpan)\n",
    "            else:\n",
    "                #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento + dic_predictions (primeiro nivel -> ner)\n",
      "numErro1: 53\n",
      "numErro2: 46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.826087  0.703704  0.760000        54\n",
      "           O   0.000000  0.000000  0.000000        46\n",
      "    Problema   0.830357  0.823009  0.826667       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.716667       360\n",
      "   macro avg   0.671031  0.649281  0.659149       360\n",
      "weighted avg   0.732494  0.716667  0.723812       360\n",
      "\n",
      "[[38 13  1  1  1]\n",
      " [ 8  0 18 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento + dic_predictions (primeiro nivel -> ner)')\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_all)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juntando sentence + ner, com pos processamento\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# juntar all_predictions\n",
    "print('juntando sentence + ner, com pos processamento')\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "#dic_predictions_all = f.getDicPredictionsAll(combinacaoEntidadesAll_pred, dic_predictions)\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "#region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "#print(confusion_matrix(region_true_list, region_pred_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia']]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 66\n",
      "numErro2: 130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.766667  0.425926  0.547619        54\n",
      "           O   0.561290  0.669231  0.610526       130\n",
      "    Problema   0.845455  0.823009  0.834081       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.743243       444\n",
      "   macro avg   0.774425  0.727571  0.740261       444\n",
      "weighted avg   0.754871  0.743243  0.741689       444\n",
      "\n",
      "[[23 28  1  1  1]\n",
      " [ 7 87 16 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO, OLD com O\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbuminúria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['alterações', 26],\n",
       "  ['tróficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepicárdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['alteração', 41],\n",
       "  ['de', 42],\n",
       "  ['repolarização', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['não', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['ausência', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diastólica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medicação', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbuminúria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['alterações tróficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepicárdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['alteração de repolarização ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['RA', [87], 'Problema'],\n",
       "  ['sopros', [94], 'Problema'],\n",
       "  ['hipofonese de bulhas', [97, 98, 99], 'Problema'],\n",
       "  ['abdome', [102], 'Anatomia'],\n",
       "  ['massas', [111], 'Problema'],\n",
       "  ['visceromegalias', [113], 'Problema'],\n",
       "  ['MMII', [114], 'Anatomia'],\n",
       "  ['edema', [116], 'Problema'],\n",
       "  ['ICC diastólica classe III', [120, 121, 122, 123], 'Problema'],\n",
       "  ['HAS', [125], 'Problema'],\n",
       "  ['DMII', [127], 'Problema'],\n",
       "  ['Dislipidemia', [129], 'Problema'],\n",
       "  ['medicação', [134], 'Tratamento'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbuminúria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['alterações', 26],\n",
       "  ['tróficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepicárdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['alteração', 41],\n",
       "  ['de', 42],\n",
       "  ['repolarização', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['não', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['ausência', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diastólica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medicação', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbuminúria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['alterações tróficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepicárdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['alteração de repolarização ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['sopros', [94], 'Teste'],\n",
       "  ['medicação', [134], 'Teste'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['laboratoriais', [142], 'Teste'],\n",
       "  ['tróficas', [27], 'Problema'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aumento moderado de átrio esquerdo',\n",
       "  'aumento moderado de átrio esquerdo .',\n",
       "  [0, 1, 2, 3, 4],\n",
       "  1,\n",
       "  1],\n",
       " ['de', 'aumento moderado de átrio esquerdo .', [2], 0, 0],\n",
       " ['átrio esquerdo', 'aumento moderado de átrio esquerdo .', [3, 4], 0, 4],\n",
       " ['moderado de átrio',\n",
       "  'aumento moderado de átrio esquerdo .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0],\n",
       " ['esquerdo', 'aumento moderado de átrio esquerdo .', [4], 0, 4],\n",
       " ['moderado', 'aumento moderado de átrio esquerdo .', [1], 0, 0],\n",
       " ['átrio', 'aumento moderado de átrio esquerdo .', [3], 0, 0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
