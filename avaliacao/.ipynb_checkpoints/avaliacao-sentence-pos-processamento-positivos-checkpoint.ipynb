{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "import model as mod\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 10\n"
     ]
    }
   ],
   "source": [
    "BATCH=10\n",
    "print('BATCH:', BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)\n",
    "\n",
    "\n",
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    with open('../obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('../obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('../obj')\n",
    "    try:\n",
    "        with open('../obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        print('erro ao pegar obj')\n",
    "        return None\n",
    "    \n",
    "def loadSentencesTest():\n",
    "    # gabarito = dicSentences_new.pkl\n",
    "    #dicSentences_train = load_obj('dic_sentencesTrain.pkl')\n",
    "    #dicSentences_train[32]\n",
    "    # gabarito = dicSentences_new.pkl\n",
    "    print('Pegando sentencas de teste gabarito: dic_sentencesTest.pkl')\n",
    "    dicSentences_new_test = load_obj('dic_sentencesTest')\n",
    "    return dicSentences_new_test\n",
    "\n",
    "def predictBERTNER_IO(sentencas, tipo_entidade):\n",
    "\n",
    "    model=''\n",
    "    if tipo_entidade == 'Tratamento':\n",
    "        model = 'lisaterumi/portuguese-ner-biobertptclin-tratamento'\n",
    "    elif tipo_entidade == 'Teste':\n",
    "        model = 'lisaterumi/portuguese-ner-biobertptclin-teste'\n",
    "    elif tipo_entidade == 'Anatomia':\n",
    "        model = 'lisaterumi/portuguese-ner-biobertptclin-anatomia'\n",
    "    elif tipo_entidade == 'Problema':\n",
    "        model = 'lisaterumi/portuguese-ner-biobertptclin-problema'\n",
    "    elif tipo_entidade == 'all':\n",
    "        #model = 'lisaterumi/portuguese-ner-nestedclinbr-biobertpt-all'\n",
    "        #model = 'lisaterumi/portuguese-ner-nestedclinbr-biobertpt-clin'\n",
    "        model=r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\NER-nestedclinbr\\all'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    config = AutoConfig.from_pretrained(model)\n",
    "    idx2tag = config.id2label\n",
    "    #idx2tag = config.label2id \n",
    "    print('idx2tag:', idx2tag)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model)\n",
    "\n",
    "    predictedModel=[]\n",
    "    all_tokens=[]\n",
    "    \n",
    "    for test_sentence in sentencas:\n",
    "        #print('test_sentence:', test_sentence)\n",
    "        tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "        input_ids = torch.tensor([tokenized_sentence])#.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(label_idx)\n",
    "                new_tokens.append(token)\n",
    "            \n",
    "        FinalLabelSentence = []\n",
    "        FinalToken = []\n",
    "        #print(len(new_tokens))\n",
    "        #print(len(new_labels))\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            label = idx2tag[label]\n",
    "            #label = str(label)\n",
    "            if label == \"O\" or label == \"X\":\n",
    "                FinalLabelSentence.append(\"O\")\n",
    "                FinalToken.append(token)\n",
    "            else:\n",
    "                FinalLabelSentence.append(label)\n",
    "                FinalToken.append(token)\n",
    "                \n",
    "        predictedModel.append(FinalLabelSentence[1:-1]) # delete [SEP] and [CLS]\n",
    "        all_tokens.append(FinalToken[1:-1])\n",
    "                    \n",
    "    return predictedModel, all_tokens\n",
    "\n",
    "def getDicPredictions(tags, tokens):\n",
    "    dic_predictions = {}\n",
    "    num=-1\n",
    "    #[tokens], [indices], tag\n",
    "    for tags_sentence, tokens_sentence in zip(tags, tokens):\n",
    "        num=num+1\n",
    "        entidades = []\n",
    "        tokens_frase = []\n",
    "        entidade_anterior=0\n",
    "        #print('---inicio sentenca:--')\n",
    "        num_token=0\n",
    "        indices_entidade = list()\n",
    "        tokens_entidade = list()\n",
    "        for tag, token in zip(tags_sentence, tokens_sentence):\n",
    "            #print(token + ' ' + tag)\n",
    "            tokens_frase.append([token, num_token])\n",
    "            if tag!='O' and (entidade_anterior==tag or num==0):\n",
    "                #entidades.append([num_token, tag])\n",
    "                indices_entidade.append(num_token)\n",
    "                #print('token:', token)\n",
    "                tokens_entidade.append(token)\n",
    "                entidade_anterior=tag\n",
    "            #elif tag!='O': # nova entidade\n",
    "            else:\n",
    "                if entidade_anterior!='O' and len(tokens_entidade)>0: \n",
    "                    entidades.append([' '.join(tokens_entidade), indices_entidade, entidade_anterior])\n",
    "                indices_entidade=list()\n",
    "                tokens_entidade = list()\n",
    "                if tag!='O': \n",
    "                    indices_entidade.append(num_token)\n",
    "                    tokens_entidade.append(token)\n",
    "                entidade_anterior=tag\n",
    "            num_token=num_token+1\n",
    "        #dic_predictions[num]=entidades\n",
    "        dic_predictions[num]=[tokens_frase, entidades]\n",
    "        entidades=list()\n",
    "    return dic_predictions\n",
    "\n",
    "def getDicPosTagger(dic_sentencesTrainDev):\n",
    "    dicPostagger = load_obj('dic_postagger')\n",
    "    allFrases = load_obj('allFrases')\n",
    "    if dicPostagger==None or allFrases==None:\n",
    "        print('Nao foi possivel recuperar obj, gerando dicPosTagger novamente')\n",
    "        dicPostagger = {}\n",
    "        allFrases=[]\n",
    "        for key, value in dic_sentencesTrainDev.items():\n",
    "            tokens=value[0]\n",
    "            frase = [t[0] for t in tokens]\n",
    "            frase = ' '.join(frase)\n",
    "            allFrases.append(frase)\n",
    "            #print(frase)\n",
    "\n",
    "        #print(allFrases)\n",
    "        doc = nlp_token_class(allFrases)\n",
    "        #print(doc)\n",
    "        for frase in doc:\n",
    "            for d in frase:\n",
    "                #print(d)\n",
    "                pos = d['entity_group']\n",
    "                #print(pos)\n",
    "                token=d['word']\n",
    "                if pos=='PREP+ART':\n",
    "                    pos='ART'\n",
    "                if pos=='NPROP':\n",
    "                    pos='N'\n",
    "                if 'ADV' in pos: # ADV-KS, ADV-KS-REL\t\n",
    "                    pos='ADV'\n",
    "                if 'PRON' in pos: # PRO-KS\t, PRO-KS-REL, PROPESS, PROPSUB\n",
    "                    pos='PRON'\n",
    "                if pos=='VAUX'or pos=='PCP': #participio\n",
    "                    pos='V'\n",
    "                dicPostagger[token] = pos    \n",
    "\n",
    "        save_obj('dicPostagger', dicPostagger)\n",
    "        save_obj('allFrases', allFrases)\n",
    "    return dicPostagger, allFrases\n",
    "\n",
    "def tipoPostaggerTokens(entidade_token, dicPostagger):\n",
    "    postagger = ''\n",
    "    for p in entidade_token:\n",
    "        #print('p:', p)\n",
    "        if p.lower() in dicPostagger.keys():\n",
    "            postagger = postagger + '-' + dicPostagger.get(p.lower())\n",
    "        else:\n",
    "            #print('nao tem:', p)\n",
    "            # se nao tem, considera N\n",
    "            postagger = postagger + '-' + 'N'\n",
    "    return postagger\n",
    "\n",
    "\n",
    "def getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger):\n",
    "    lista_postaggers_entidades = []\n",
    "    for key, value in dic_sentencesTrainDev.items():\n",
    "        #print('key:', key)\n",
    "        entidades = value[1]\n",
    "        for entidade in entidades:\n",
    "            #print(entidade[0])\n",
    "            pos_tagger=tipoPostaggerTokens(entidade[0].split(), dicPosTagger)\n",
    "            if pos_tagger not in lista_postaggers_entidades:\n",
    "                lista_postaggers_entidades.append(pos_tagger)\n",
    "    return lista_postaggers_entidades\n",
    "\n",
    "def getCombinacaoEntidadesSentence(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling, lista_postaggers_entidades):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidadesNeg = list()\n",
    "    combinacaoEntidades = list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Sentence Pairs - Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Sentence Pairs - Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem taxa de Downsampling')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        lista_indices = list()\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            lista_indices = [inicio, fim]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                lista_indices_proc = [num for num in range(lista_indices[0], lista_indices[1]+1, 1)]\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), lista_indices_proc, labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "        # agora, os negativos\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+1]\n",
    "                        frase_string=' '.join(termo).strip()\n",
    "                        devePular = 0\n",
    "                        if '.' in frase_string[-1:] or ',' in frase_string[-1:]  or ';' in frase_string[-1:] or '-' in frase_string[-1:]  or ':' in frase_string[-1:]  or '=' in frase_string[-1:]  or '/' in frase_string[-1:]  or '(' in frase_string[-1:]  or ')' in frase_string[-1:]  or '[' in frase_string[-1:]  or ']' in frase_string[-1:]  or ':' in frase_string[-1:]:\n",
    "                            devePular=1\n",
    "                        if '.' in frase_string[:1] or ',' in frase_string[:1]  or ';' in frase_string[:1] or '-' in frase_string[:1]  or ':' in frase_string[:1]  or '=' in frase_string[:1] or '/' in frase_string[:1]  or '(' in frase_string[:1]  or ')' in frase_string[:1] or '[' in frase_string[:1]  or ']' in frase_string[:1]  or ':' in frase_string[:1]:\n",
    "                            devePular=1\n",
    "                        if re.search(\"^[0-9]*mg\", frase_string):\n",
    "                            devePular=1\n",
    "                            \n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase_l in combinacaoEntidadesPos:\n",
    "                            if frase_l[0] == frase_string:\n",
    "                                tem_frase='1'\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                        #print('tem_frase:', tem_frase)\n",
    "                        #if tem_frase==0:\n",
    "                            lista_indices = [indice, len(frase_string.split())-1+indice]\n",
    "                            lista_indices_proc = [num for num in range(lista_indices[0], lista_indices[1]+1, 1)]\n",
    "                            combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), lista_indices_proc, labels['O']])\n",
    "                        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidadesNeg = combinacaoEntidadesNeg[:(num_positivas*taxaDownsampling)+num_positivas]\n",
    "            random.shuffle(combinacaoEntidadesNeg)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        #combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos+combinacaoEntidadesNeg\n",
    "        combinacaoEntidades = combinacaoEntidadesPos+combinacaoEntidadesNeg\n",
    "        combinacaoEntidadesPos=list()\n",
    "        combinacaoEntidadesNeg=list()\n",
    "        combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    \n",
    "    return combinacaoEntidadesAll\n",
    "\n",
    "def isEntidadeAninhada(a, b):\n",
    "    inicio1=a[0]\n",
    "    fim1=a[-1]\n",
    "    #print(inicio1, fim1)\n",
    "    inicio2=b[0]\n",
    "    fim2=b[-1]\n",
    "    #print(inicio2, fim2)\n",
    "    if inicio1 >=inicio2 and fim1 <=fim2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def posProcessamento(dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions_sentence.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = list()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeNer in listaEntidadesSpan:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            #print('entidadeSpan:', entidadeSpan)\n",
    "            indices = entidadeSpan[1]\n",
    "            #print('incluindo:', entidadeSpan)\n",
    "            # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "            # ver pelo indice\n",
    "            isAninhada=False\n",
    "            isMesmoTipo=False\n",
    "            tipo=entidadeSpan[2]\n",
    "            for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                isAninhada=False\n",
    "                if indices!=indicesNer:\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                if isAninhada and tipoNer == tipo:\n",
    "                    isMesmoTipo=True\n",
    "                    #print('-----mesmo tipo!!')\n",
    "                    #print('entidadeSpan:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    #print('indices:', indices)\n",
    "                    #print('indicesNer:', indicesNer)\n",
    "                    #print('indices!=indicesNer:', indices!=indicesNer)\n",
    "                    break\n",
    "            # ver pelo tipo tb\n",
    "            if not (isAninhada and isMesmoTipo):\n",
    "                #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                listaEntidadesNer.append(entidadeSpan)\n",
    "            else:\n",
    "                #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "def AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH):\n",
    "\n",
    "    region_true_list, region_pred_list = list(), list() # labels\n",
    "    region_true_count, region_pred_count = 0, 0 # contagem\n",
    "    numErro1=0\n",
    "    numErro2=0\n",
    "\n",
    "    for i in range(0, BATCH, 1):\n",
    "        if i<len(dicSentences_new_test):\n",
    "            #print('\\n---Label vs predicao--------')\n",
    "            #print('frase:', dicSentences_new_test[i][0])\n",
    "            #print('i:', i)\n",
    "            #print(dicSentences_new_test[i][1])\n",
    "            #print(dic_predictions_all[i][1])\n",
    "\n",
    "\n",
    "            entidades_gabarito = dicSentences_new_test[i][1]\n",
    "            try:\n",
    "                entidades_preditas = dic_predictions_all[i][1]\n",
    "            except:\n",
    "                print('erro, caiu no except na AvalFinal:', str(i))\n",
    "            #print('---entidades_gabarito--:', entidades_gabarito)\n",
    "            #print('entidades_preditas:', entidades_preditas)\n",
    "            for entidade_gabarito in entidades_gabarito:\n",
    "                indices_gabarito = entidade_gabarito[1]\n",
    "                tag_gabarito = entidade_gabarito[2]\n",
    "                region_true_count=region_true_count+1\n",
    "                region_true_list.append(tag_gabarito)\n",
    "                # ver se previu essa entidade\n",
    "                previu=0\n",
    "                for entidade_predita in entidades_preditas:\n",
    "                    indices_predita = entidade_predita[1]\n",
    "                    tag_predita = entidade_predita[2]\n",
    "                    if indices_predita == indices_gabarito:\n",
    "                        region_pred_list.append(tag_predita)\n",
    "                        previu=1\n",
    "                        if tag_predita !='O':\n",
    "                            region_pred_count=region_pred_count+1\n",
    "                        break\n",
    "                if previu==0:\n",
    "                    numErro1=numErro1+1\n",
    "                    region_pred_list.append('O')\n",
    "\n",
    "            # agora o contrario, ver o q previu mas nao era\n",
    "\n",
    "            for entidade_predita in entidades_preditas:\n",
    "                indices_predita = entidade_predita[1]\n",
    "                tag_predita = entidade_predita[2]\n",
    "                # ver se a entidade prevista existe ou é FP\n",
    "                existe=0\n",
    "                for entidade_gabarito in entidades_gabarito:\n",
    "                    indices_gabarito = entidade_gabarito[1]\n",
    "                    if indices_predita == indices_gabarito:\n",
    "                        existe=1\n",
    "                        break\n",
    "                if existe==0:\n",
    "                    numErro2 = numErro2+1\n",
    "                    region_true_list.append('O')\n",
    "                    region_pred_list.append(tag_predita)\n",
    "\n",
    "    #print('===resultados====')\n",
    "    #print('region_true_list:', region_true_list)\n",
    "    #print('region_pred_list:', region_pred_list)\n",
    "\n",
    "    print('numErro1:', numErro1)\n",
    "    print('numErro2:', numErro2)\n",
    "    #print(classification_report(region_true_list, region_pred_list, labels=['O', 'PROTEIN', 'DNA', 'RNA', 'CELL_TYPE', 'CELL_LINE'], target_names=['O', 'PROTEIN', 'DNA', 'RNA', 'CELL_TYPE', 'CELL_LINE'], digits=6))\n",
    "    print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "\n",
    "    return region_true_list, region_pred_list\n",
    "\n",
    "def getDicPredictionsSentence(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {0:'Problema', 1:'Tratamento', 2:'Teste', 3:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[4]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            if tipo_previsto!=0:\n",
    "                entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade está na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 10\n",
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "len(dic_predictions): 10\n",
      "len(dicSentences_new_test): 10\n",
      "len(dic_predictions): 10\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n",
    "tags, tokens = predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = getDicPredictions(tags, tokens)\n",
    "#print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "#print(dic_predictions[9])\n",
    "        \n",
    "print('len(dicSentences_new_test):', len(dicSentences_new_test))\n",
    "print('len(dic_predictions):', len(dic_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelo de sentence-pais\n",
    "Só positivos com pos processamento\n",
    "1) com filtro postagger\n",
    "\n",
    "2) sem filtro postagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando só com modelo de Sentence Pairs:-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7defc5e0fab42ad9f6eaaf3af02f60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5572c43a22d4853aa2521cccb326014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c018becb45d34d7aa60c25e1148263ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ead46fc67f4de4a4d50f15428a99c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b6fea822394d2f9dea29a03140dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd601a45023343ec9efd41344ee000dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "print('-----Avaliando só com modelo de Sentence Pairs:-----')\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_positivos', use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiro, com filtro postagger:\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 2\n",
      "len(combinacaoEntidadesAll:) 10\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(r\"..\\preProcessamento\\sentence-pairs-filtro\\sentence_pairs.test\", delimiter='\\t', header=0, names=['text_b', 'text_a', 'labels'])\n",
    "#df = df.dropna(axis=0, how='any')\n",
    "#print('Número de sentenças de test: {:,}\\n'.format(df.shape[0]))\n",
    "#df=df[:50]\n",
    "#test_df = df\n",
    "#print(test_df[:3])\n",
    "#lista=list()\n",
    "#for index, row in test_df.iterrows():\n",
    "#    lista.append([row['text_b'], row['text_a']])\n",
    "#predictions, raw_outputs = model.predict(\n",
    "#  lista\n",
    "#)\n",
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')\n",
    "dicPosTagger, _ = getDicPosTagger(dic_sentencesTrainDev)\n",
    "lista_postaggers_entidades = getListaPostaggerEntidades(dic_predictions, dicPosTagger)\n",
    "# para gerar arquivo de predicoes (com as tags <e1>)\n",
    "print('Primeiro, com filtro postagger:')\n",
    "combinacaoEntidadesAll = getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# para suprimir output\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Em', 0],\n",
       "  ['acompanhamento', 1],\n",
       "  ['no', 2],\n",
       "  ['ambualtorio', 3],\n",
       "  ['há', 4],\n",
       "  ['5', 5],\n",
       "  ['anos', 6],\n",
       "  ['por', 7],\n",
       "  ['FA', 8],\n",
       "  [',', 9],\n",
       "  ['uso', 10],\n",
       "  ['de', 11],\n",
       "  ['marevan', 12],\n",
       "  ['5mg', 13],\n",
       "  ['1', 14],\n",
       "  ['x', 15],\n",
       "  ['ao', 16],\n",
       "  ['dia', 17],\n",
       "  ['.', 18]],\n",
       " [['marevan 5mg', [12, 13], 'Tratamento'], ['marevan', [12], 'Tratamento']]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro:-----\n",
      "numErro1: 16\n",
      "numErro2: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   1.000000  1.000000  1.000000         1\n",
      "           O   0.000000  0.000000  0.000000         5\n",
      "    Problema   0.000000  0.000000  0.000000        15\n",
      "       Teste   1.000000  0.666667  0.800000         3\n",
      "  Tratamento   0.687500  1.000000  0.814815        11\n",
      "\n",
      "    accuracy                       0.400000        35\n",
      "   macro avg   0.537500  0.533333  0.522963        35\n",
      "weighted avg   0.330357  0.400000  0.353228        35\n",
      "\n",
      "[[ 1  0  0  0  0]\n",
      " [ 0  0  0  0  5]\n",
      " [ 0 15  0  0  0]\n",
      " [ 0  1  0  2  0]\n",
      " [ 0  0  0  0 11]]\n",
      "Com pos processamento\n",
      "numErro1: 16\n",
      "numErro2: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   1.000000  1.000000  1.000000         1\n",
      "           O   0.000000  0.000000  0.000000         0\n",
      "    Problema   0.000000  0.000000  0.000000        15\n",
      "       Teste   1.000000  0.666667  0.800000         3\n",
      "  Tratamento   1.000000  1.000000  1.000000        11\n",
      "\n",
      "    accuracy                       0.466667        30\n",
      "   macro avg   0.600000  0.533333  0.560000        30\n",
      "weighted avg   0.500000  0.466667  0.480000        30\n",
      "\n",
      "[[ 1  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0 15  0  0  0]\n",
      " [ 0  1  0  2  0]\n",
      " [ 0  0  0  0 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisat\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lisat\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro:-----')\n",
    "\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "print('Com pos processamento')\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "print('Juntando - pos processamento + dic_predictions (primeiro nivel -> ner)')\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_all)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinacaoEntidadesAll = getCombinacaoEntidadesSentence(dic_predictions, False, '', 0, '')\n",
    "print('-----Avaliando modelo sem filtro:-----')\n",
    "print('Chamando predict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "print('com pos processamento')\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n",
    "\n",
    "\n",
    "print('Juntando - pos processamento + dic_predictions (primeiro nivel -> ner)')\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_all)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
