{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "import model as mod\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload \n",
    "#from eval import predict\n",
    "import eval\n",
    "#import importlib\n",
    "#importlib.reload(module)\n",
    "import dataset\n",
    "from dataset import InputFeatures, load_and_cache_examples\n",
    "import functionsAval as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 100\n"
     ]
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n",
    "reload(mod)\n",
    "\n",
    "# em numero de frases\n",
    "BATCH=100\n",
    "#BATCH=5\n",
    "#BATCH=800\n",
    "#BATCH=8000 \n",
    "print('BATCH:', BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 100\n",
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "len(dic_predictions): 100\n",
      "verificando dados:\n",
      "len(dicSentences_new_test): 100\n",
      "len(dic_predictions): 100\n",
      "region_pred_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "region_true_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "lista_erros[:8]: [7, 8, 13, 13, 14, 15, 15, 15]\n",
      "len(lista_erros): 114\n",
      "len(set(lista_erros)): 45\n",
      "len(region_true_list): 352\n",
      "len(region_pred_list): 352\n",
      "-----Avaliando só modelo de NER:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.789474  0.277778  0.410959        54\n",
      "           O   0.000000  0.000000  0.000000        38\n",
      "    Problema   0.853211  0.823009  0.837838       113\n",
      "       Teste   0.880952  0.891566  0.886228        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.667614       352\n",
      "   macro avg   0.670352  0.564096  0.592630       352\n",
      "weighted avg   0.753305  0.667614  0.691546       352\n",
      "\n",
      "[[15 36  1  1  1]\n",
      " [ 4  0 15  9 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n",
    "tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "#print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "#print(dic_predictions[9])\n",
    "f.save_obj('dic_predictions_results_ner_'+str(BATCH), dic_predictions)\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "print('verificando dados:')\n",
    "#for key, value in dic_predictions.items():\n",
    "#    print('key:',key)\n",
    "#    print(dic_predictions[key])\n",
    "#    if key>2:\n",
    "#        break\n",
    "        \n",
    "print('len(dicSentences_new_test):', len(dicSentences_new_test))\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "f.save_obj('region_true_list'+str(BATCH), region_true_list)\n",
    "print('region_pred_list[:4]:', region_pred_list[:4])\n",
    "print('region_true_list[:4]:', region_true_list[:4])\n",
    "print('lista_erros[:8]:', lista_erros[:8])\n",
    "print('len(lista_erros):', len(lista_erros))\n",
    "print('len(set(lista_erros)):', len(set(lista_erros)))\n",
    "#print(dic_predictions[8])\n",
    "#print(dicSentences_new_test[8][1])\n",
    "print('len(region_true_list):', len(region_true_list))\n",
    "print('len(region_pred_list):', len(region_pred_list))\n",
    "#print('pred:',region_pred_list[:15])\n",
    "#print('true:',region_true_list[:15])\n",
    "\n",
    "print('-----Avaliando só modelo de NER:-----')\n",
    "\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelo de sentence-pais\n",
    "#### primeiro sozinho...\n",
    "1) Com filtro + downsampling\n",
    "\n",
    "2) Com filtro\n",
    "\n",
    "3) Só positivos (dai preciso de um CRF pro filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando só com modelo de Sentence Pairs:-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeb1fc41af44b0bf95421a0455505a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2d8c8fdc44bf2bd9533f1db8aaf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"version\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                     raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1792: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-03d79b075fa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----Avaliando só com modelo de Sentence Pairs:-----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lisaterumi/sentence_pairs_nested_filtro_downsampling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 self.model = model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    411\u001b[0m                     \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m                 \u001b[1;31m# Time to load the checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2132\u001b[1;33m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m             \u001b[1;31m# set dtype to instantiate the model under:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    475\u001b[0m                     ) from e\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             raise OSError(\n\u001b[0m\u001b[0;32m    478\u001b[0m                 \u001b[1;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "print('-----Avaliando só com modelo de Sentence Pairs:-----')\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro_downsampling', use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[0,0]\n",
    "[num for num in range(a[0], a[1]+1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = reload(f)\n",
    "#combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "#combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de sentenças de test: 1,913\n",
      "\n",
      "         text_b                                             text_a  labels\n",
      "0   marevan 5mg  Em acompanhamento no ambualtorio há 5 anos por...       2\n",
      "1       marevan  Em acompanhamento no ambualtorio há 5 anos por...       0\n",
      "2  Comorbidades  Comorbidades : DM há 10 anos em uso de metform...       1\n",
      "Primeiro, com filtro postagger:\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 14\n",
      "len(combinacaoEntidadesAll:) 100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"..\\preProcessamento\\sentence-pairs-filtro\\sentence_pairs.test\", delimiter='\\t', header=0, names=['text_b', 'text_a', 'labels'])\n",
    "df = df.dropna(axis=0, how='any')\n",
    "print('Número de sentenças de test: {:,}\\n'.format(df.shape[0]))\n",
    "#df=df[:50]\n",
    "test_df = df\n",
    "print(test_df[:3])\n",
    "lista=list()\n",
    "#for index, row in test_df.iterrows():\n",
    "#    lista.append([row['text_b'], row['text_a']])\n",
    "#predictions, raw_outputs = model.predict(\n",
    "#  lista\n",
    "#)\n",
    "dic_sentencesTrainDev = f.load_obj('dic_sentencesTrainDev')\n",
    "dicPosTagger, _ = f.getDicPosTagger(dic_sentencesTrainDev)\n",
    "lista_postaggers_entidades = f.getListaPostaggerEntidades(dic_predictions, dicPosTagger)\n",
    "# para gerar arquivo de predicoes (com as tags <e1>)\n",
    "print('Primeiro, com filtro postagger:')\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "f.save_obj('combinacaoEntidadesAllSentence_'+str(BATCH), combinacaoEntidadesAll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# para suprimir output\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "pred_region_labels2 = list()\n",
    "for a in pred_region_labels:\n",
    "    for b in a:\n",
    "        pred_region_labels2.append(labels[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Problema', 'Tratamento', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'O']\n",
      "['Problema', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'Problema', 'Tratamento']\n",
      "564\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "print(pred_region_labels2[:10])\n",
    "print(region_true_list[:10])\n",
    "print(len(pred_region_labels2))\n",
    "print(len(region_true_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0],\n",
       "  ['há', 1],\n",
       "  ['15', 2],\n",
       "  ['anos', 3],\n",
       "  ['em', 4],\n",
       "  ['uso', 5],\n",
       "  ['de', 6],\n",
       "  ['losartana', 7],\n",
       "  ['50mg', 8],\n",
       "  ['/', 9],\n",
       "  ['dia', 10],\n",
       "  ['e', 11],\n",
       "  ['digoxina', 12],\n",
       "  ['1', 13],\n",
       "  ['/', 14],\n",
       "  ['2', 15],\n",
       "  ['cp', 16],\n",
       "  ['/', 17],\n",
       "  ['dia', 18],\n",
       "  [',', 19],\n",
       "  ['carvedilol', 20],\n",
       "  ['25', 21],\n",
       "  ['12', 22],\n",
       "  ['/', 23],\n",
       "  ['12', 24],\n",
       "  [',', 25],\n",
       "  ['HCTZ', 26],\n",
       "  ['.', 27]],\n",
       " [['HAS', [0], 'Problema'],\n",
       "  ['losartana 50mg', [7, 8], 'Tratamento'],\n",
       "  ['digoxina', [12], 'Tratamento'],\n",
       "  ['carvedilol 25', [20, 21], 'Tratamento'],\n",
       "  ['HCTZ', [26], 'Tratamento']]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[4]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            if tipo_previsto!=0:\n",
    "                entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade está na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Região):-----\n",
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Região):-----\n",
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Região):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro (Região):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df462b3fed964f1395551a965d628a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c631420dc345c384995d2f4292b591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd449083b554e778e5be575c8fd43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ecebf97cad44a39e22d0bc0c352897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6f1a1bba394feb855337afd28b9a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro (Região):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 2],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence2(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[5]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade está na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "# om 30 epocas\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n",
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD\n",
    "dic_predictions_sentence = getDicPredictionsSentence2(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31  17   4   2   0]\n",
      " [  6 145  81  24  29]\n",
      " [  0  17  96   0   0]\n",
      " [  1   7   0  75   0]\n",
      " [  0   8   0   0  56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'O']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_true_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_pred_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0, 269],\n",
       "  ['refere', 1, 278],\n",
       "  ['fraqueza', 2, 285],\n",
       "  ['intermitente', 3, 294],\n",
       "  ['em', 4, 307],\n",
       "  ['MMII', 5, 310],\n",
       "  [',', 6, 314],\n",
       "  ['associada', 7, 316],\n",
       "  ['a', 8, 324],\n",
       "  ['tontura', 9, 328],\n",
       "  [',', 10, 335],\n",
       "  ['turvação', 11, 337],\n",
       "  ['visual', 12, 346],\n",
       "  ['e', 13, 353],\n",
       "  ['epigastralgia', 14, 355],\n",
       "  ['.', 15, 368]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicSentences_new_test[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['fraqueza intermitente', [2, 3], 'Problema'],\n",
       "  ['intermitente em MMII', [3, 4, 5], 'Problema']]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo sem filtro (Região):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102499d488da4d2b9b3bf9b3ba83b669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0291985584e48aa5ff3f341718027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d2bf402f9d49249b138aa89354960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b15b3ff136468c96cb99f1187c19e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8843ea20e91b4a72b245324511bd7fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681a40ef2be4d2f97dbf0f4d4919c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "# AKI rodar de novo\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, False, '', 0, '')\n",
    "print('-----Avaliando modelo sem filtro (Região):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_all', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-sem-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentence_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "f.save_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Otimizo', 0], ['dose', 1], ['da', 2], ['sinvastatina', 3], ['para', 4], ['40mg', 5], ['/', 6], ['dia', 7], ['.', 8]], [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'], ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]\n",
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "print(dic_predictions_sentence[20])\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.753695  0.536842  0.627049       285\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.686144       599\n",
      "   macro avg   0.708428  0.764704  0.722687       599\n",
      "weighted avg   0.712228  0.686144  0.683106       599\n",
      "\n",
      "[[ 38  12   4   0   0]\n",
      " [ 10 153  77  12  33]\n",
      " [  0  26  86   1   0]\n",
      " [  0   7   0  76   0]\n",
      " [  0   5   0   1  58]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O \n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, merge com dic_predictions?\n",
    "\n",
    "Acho que nao precisa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, com pos processamento\n",
    "\n",
    "Regras:\n",
    "\n",
    "- Pega os de dentro (aninhadas), apenas se o tipo for diferente do tipo da entidade de fora... se for do mesmo tipo, não pega..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isEntidadeAninhada(a, b):\n",
    "    inicio1=a[0]\n",
    "    fim1=a[-1]\n",
    "    #print(inicio1, fim1)\n",
    "    inicio2=b[0]\n",
    "    fim2=b[-1]\n",
    "    #print(inicio2, fim2)\n",
    "    if inicio1 >=inicio2 and fim1 <=fim2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "isEntidadeAninhada([2], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento\n",
      "numErro1: 65\n",
      "numErro2: 43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.822222  0.685185  0.747475        54\n",
      "           O   0.000000  0.000000  0.000000        43\n",
      "    Problema   0.838384  0.734513  0.783019       113\n",
      "       Teste   0.860465  0.891566  0.875740        83\n",
      "  Tratamento   0.838710  0.812500  0.825397        64\n",
      "\n",
      "    accuracy                       0.689076       357\n",
      "   macro avg   0.671956  0.624753  0.646326       357\n",
      "weighted avg   0.740150  0.689076  0.712483       357\n",
      "\n",
      "[[37 16  1  0  0]\n",
      " [ 8  0 15 10 10]\n",
      " [ 0 29 83  1  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  1 52]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento')\n",
    "\n",
    "def posProcessamento(dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions_sentence.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = list()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeNer in listaEntidadesSpan:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            #print('entidadeSpan:', entidadeSpan)\n",
    "            indices = entidadeSpan[1]\n",
    "            #print('incluindo:', entidadeSpan)\n",
    "            # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "            # ver pelo indice\n",
    "            isAninhada=False\n",
    "            isMesmoTipo=False\n",
    "            tipo=entidadeSpan[2]\n",
    "            for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                isAninhada=False\n",
    "                if indices!=indicesNer:\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                if isAninhada and tipoNer == tipo:\n",
    "                    isMesmoTipo=True\n",
    "                    #print('-----mesmo tipo!!')\n",
    "                    #print('entidadeSpan:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    #print('indices:', indices)\n",
    "                    #print('indicesNer:', indicesNer)\n",
    "                    #print('indices!=indicesNer:', indices!=indicesNer)\n",
    "                    break\n",
    "            # ver pelo tipo tb\n",
    "            if not (isAninhada and isMesmoTipo):\n",
    "                #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                listaEntidadesNer.append(entidadeSpan)\n",
    "            else:\n",
    "                #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento + dic_predictions (primeiro nivel -> ner)\n",
      "numErro1: 53\n",
      "numErro2: 46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.826087  0.703704  0.760000        54\n",
      "           O   0.000000  0.000000  0.000000        46\n",
      "    Problema   0.830357  0.823009  0.826667       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.716667       360\n",
      "   macro avg   0.671031  0.649281  0.659149       360\n",
      "weighted avg   0.732494  0.716667  0.723812       360\n",
      "\n",
      "[[38 13  1  1  1]\n",
      " [ 8  0 18 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento + dic_predictions (primeiro nivel -> ner)')\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_all)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juntando sentence + ner, com pos processamento\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# juntar all_predictions\n",
    "print('juntando sentence + ner, com pos processamento')\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "#dic_predictions_all = f.getDicPredictionsAll(combinacaoEntidadesAll_pred, dic_predictions)\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # só acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # é aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('Não é aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "#region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "#print(confusion_matrix(region_true_list, region_pred_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia']]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 66\n",
      "numErro2: 130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.766667  0.425926  0.547619        54\n",
      "           O   0.561290  0.669231  0.610526       130\n",
      "    Problema   0.845455  0.823009  0.834081       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.743243       444\n",
      "   macro avg   0.774425  0.727571  0.740261       444\n",
      "weighted avg   0.754871  0.743243  0.741689       444\n",
      "\n",
      "[[23 28  1  1  1]\n",
      " [ 7 87 16 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO, OLD com O\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbuminúria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['alterações', 26],\n",
       "  ['tróficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepicárdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['alteração', 41],\n",
       "  ['de', 42],\n",
       "  ['repolarização', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['não', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['ausência', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diastólica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medicação', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbuminúria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['alterações tróficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepicárdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['alteração de repolarização ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['RA', [87], 'Problema'],\n",
       "  ['sopros', [94], 'Problema'],\n",
       "  ['hipofonese de bulhas', [97, 98, 99], 'Problema'],\n",
       "  ['abdome', [102], 'Anatomia'],\n",
       "  ['massas', [111], 'Problema'],\n",
       "  ['visceromegalias', [113], 'Problema'],\n",
       "  ['MMII', [114], 'Anatomia'],\n",
       "  ['edema', [116], 'Problema'],\n",
       "  ['ICC diastólica classe III', [120, 121, 122, 123], 'Problema'],\n",
       "  ['HAS', [125], 'Problema'],\n",
       "  ['DMII', [127], 'Problema'],\n",
       "  ['Dislipidemia', [129], 'Problema'],\n",
       "  ['medicação', [134], 'Tratamento'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbuminúria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['alterações', 26],\n",
       "  ['tróficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepicárdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['alteração', 41],\n",
       "  ['de', 42],\n",
       "  ['repolarização', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['não', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['ausência', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diastólica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medicação', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbuminúria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['alterações tróficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepicárdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['alteração de repolarização ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['sopros', [94], 'Teste'],\n",
       "  ['medicação', [134], 'Teste'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['laboratoriais', [142], 'Teste'],\n",
       "  ['tróficas', [27], 'Problema'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turvação', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turvação visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aumento moderado de átrio esquerdo',\n",
       "  'aumento moderado de átrio esquerdo .',\n",
       "  [0, 1, 2, 3, 4],\n",
       "  1,\n",
       "  1],\n",
       " ['de', 'aumento moderado de átrio esquerdo .', [2], 0, 0],\n",
       " ['átrio esquerdo', 'aumento moderado de átrio esquerdo .', [3, 4], 0, 4],\n",
       " ['moderado de átrio',\n",
       "  'aumento moderado de átrio esquerdo .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0],\n",
       " ['esquerdo', 'aumento moderado de átrio esquerdo .', [4], 0, 4],\n",
       " ['moderado', 'aumento moderado de átrio esquerdo .', [1], 0, 0],\n",
       " ['átrio', 'aumento moderado de átrio esquerdo .', [3], 0, 0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
