{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "# ver qtos o modelo apenas de ner acertaria\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize \n",
    "import torch\n",
    "from transformers import BertTokenizer,BertForTokenClassification\n",
    "import numpy as np\n",
    "import json   \n",
    "from importlib import reload  # Python 3.4+\n",
    "import random\n",
    "import model as mod\n",
    "from model import BertForChunkClassification\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload \n",
    "#from eval import predict\n",
    "import eval\n",
    "#import importlib\n",
    "#importlib.reload(module)\n",
    "import dataset\n",
    "from dataset import InputFeatures, load_and_cache_examples\n",
    "import functionsAval as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 100\n"
     ]
    }
   ],
   "source": [
    "f = reload(f)\n",
    "reload(dataset)\n",
    "reload(eval)\n",
    "reload(mod)\n",
    "\n",
    "# em numero de frases\n",
    "BATCH=100\n",
    "#BATCH=5\n",
    "#BATCH=800\n",
    "#BATCH=8000 \n",
    "print('BATCH:', BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegando sentencas de teste gabarito: dic_sentencesTest.pkl\n",
      "506\n",
      "[[['Lucas', 0, 43], [',', 1, 48], ['74', 2, 50], ['anos', 3, 53], ['.', 4, 57]], []]\n",
      "numero de sentencas no total: 100\n",
      "idx2tag: {0: 'Teste', 1: 'Anatomia', 2: 'O', 3: 'Problema', 4: 'Tratamento', 5: '<pad>'}\n",
      "len(dic_predictions): 100\n",
      "verificando dados:\n",
      "len(dicSentences_new_test): 100\n",
      "len(dic_predictions): 100\n",
      "region_pred_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "region_true_list[:4]: ['Problema', 'Tratamento', 'Problema', 'Problema']\n",
      "lista_erros[:8]: [7, 8, 13, 13, 14, 15, 15, 15]\n",
      "len(lista_erros): 114\n",
      "len(set(lista_erros)): 45\n",
      "len(region_true_list): 352\n",
      "len(region_pred_list): 352\n",
      "-----Avaliando s贸 modelo de NER:-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.789474  0.277778  0.410959        54\n",
      "           O   0.000000  0.000000  0.000000        38\n",
      "    Problema   0.853211  0.823009  0.837838       113\n",
      "       Teste   0.880952  0.891566  0.886228        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.667614       352\n",
      "   macro avg   0.670352  0.564096  0.592630       352\n",
      "weighted avg   0.753305  0.667614  0.691546       352\n",
      "\n",
      "[[15 36  1  1  1]\n",
      " [ 4  0 15  9 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "dicSentences_new_test = f.loadSentencesTest()\n",
    "print(len(dicSentences_new_test))\n",
    "dicSentences_new_test = {k: v for k, v in dicSentences_new_test.items() if k<BATCH}\n",
    "print(dicSentences_new_test[0])\n",
    "#print(dicSentences_new_test[27])\n",
    "print('numero de sentencas no total:', len(dicSentences_new_test))\n",
    "\n",
    "sentences=list()\n",
    "for key, value in dicSentences_new_test.items():\n",
    "    if key<BATCH:\n",
    "        tokens = value[0]\n",
    "        tokens = [tok[0] for tok in tokens]\n",
    "        sentences.append(' '.join(tokens).strip())\n",
    "#print(sentences[0])\n",
    "\n",
    "tags, tokens = f.predictBERTNER_IO(sentences, 'all')\n",
    "dic_predictions = f.getDicPredictions(tags, tokens)\n",
    "#print(dic_predictions[0])\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "#print(dic_predictions[9])\n",
    "f.save_obj('dic_predictions_results_ner_'+str(BATCH), dic_predictions)\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "print('verificando dados:')\n",
    "#for key, value in dic_predictions.items():\n",
    "#    print('key:',key)\n",
    "#    print(dic_predictions[key])\n",
    "#    if key>2:\n",
    "#        break\n",
    "        \n",
    "print('len(dicSentences_new_test):', len(dicSentences_new_test))\n",
    "print('len(dic_predictions):', len(dic_predictions))\n",
    "\n",
    "region_true_list, region_pred_list, lista_erros = f.getListaRegionsTruePred(BATCH, dicSentences_new_test, dic_predictions)\n",
    "f.save_obj('region_true_list'+str(BATCH), region_true_list)\n",
    "print('region_pred_list[:4]:', region_pred_list[:4])\n",
    "print('region_true_list[:4]:', region_true_list[:4])\n",
    "print('lista_erros[:8]:', lista_erros[:8])\n",
    "print('len(lista_erros):', len(lista_erros))\n",
    "print('len(set(lista_erros)):', len(set(lista_erros)))\n",
    "#print(dic_predictions[8])\n",
    "#print(dicSentences_new_test[8][1])\n",
    "print('len(region_true_list):', len(region_true_list))\n",
    "print('len(region_pred_list):', len(region_pred_list))\n",
    "#print('pred:',region_pred_list[:15])\n",
    "#print('true:',region_true_list[:15])\n",
    "\n",
    "print('-----Avaliando s贸 modelo de NER:-----')\n",
    "\n",
    "print(classification_report(region_true_list, region_pred_list, digits=6))\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelo de sentence-pais\n",
    "#### primeiro sozinho...\n",
    "1) Com filtro + downsampling\n",
    "\n",
    "2) Com filtro\n",
    "\n",
    "3) S贸 positivos (dai preciso de um CRF pro filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando s贸 com modelo de Sentence Pairs:-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeb1fc41af44b0bf95421a0455505a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2d8c8fdc44bf2bd9533f1db8aaf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"version\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                     raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1792: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-03d79b075fa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----Avaliando s贸 com modelo de Sentence Pairs:-----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lisaterumi/sentence_pairs_nested_filtro_downsampling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, tokenizer_type, tokenizer_name, num_labels, weight, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 self.model = model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    411\u001b[0m                     \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m                 \u001b[1;31m# Time to load the checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2132\u001b[1;33m                 \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m             \u001b[1;31m# set dtype to instantiate the model under:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    475\u001b[0m                     ) from e\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             raise OSError(\n\u001b[0m\u001b[0;32m    478\u001b[0m                 \u001b[1;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870' at 'C:\\Users\\lisat/.cache\\huggingface\\transformers\\c4bce3fe90b1eab988c19335c2069b6dbc9689aa1209f5d3358a1ab9b8bf138b.7c5536218ae2048f916deae92cf501ffdf0a509aaf74d638632d0ca7e5722870'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "print('-----Avaliando s贸 com modelo de Sentence Pairs:-----')\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro_downsampling', use_cuda=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[0,0]\n",
    "[num for num in range(a[0], a[1]+1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = reload(f)\n",
    "#combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "#combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N煤mero de senten莽as de test: 1,913\n",
      "\n",
      "         text_b                                             text_a  labels\n",
      "0   marevan 5mg  Em acompanhamento no ambualtorio h谩 5 anos por...       2\n",
      "1       marevan  Em acompanhamento no ambualtorio h谩 5 anos por...       0\n",
      "2  Comorbidades  Comorbidades : DM h谩 10 anos em uso de metform...       1\n",
      "Primeiro, com filtro postagger:\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 14\n",
      "len(combinacaoEntidadesAll:) 100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"..\\preProcessamento\\sentence-pairs-filtro\\sentence_pairs.test\", delimiter='\\t', header=0, names=['text_b', 'text_a', 'labels'])\n",
    "df = df.dropna(axis=0, how='any')\n",
    "print('N煤mero de senten莽as de test: {:,}\\n'.format(df.shape[0]))\n",
    "#df=df[:50]\n",
    "test_df = df\n",
    "print(test_df[:3])\n",
    "lista=list()\n",
    "#for index, row in test_df.iterrows():\n",
    "#    lista.append([row['text_b'], row['text_a']])\n",
    "#predictions, raw_outputs = model.predict(\n",
    "#  lista\n",
    "#)\n",
    "dic_sentencesTrainDev = f.load_obj('dic_sentencesTrainDev')\n",
    "dicPosTagger, _ = f.getDicPosTagger(dic_sentencesTrainDev)\n",
    "lista_postaggers_entidades = f.getListaPostaggerEntidades(dic_predictions, dicPosTagger)\n",
    "# para gerar arquivo de predicoes (com as tags <e1>)\n",
    "print('Primeiro, com filtro postagger:')\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, True, dicPosTagger, 0, lista_postaggers_entidades)\n",
    "f.save_obj('combinacaoEntidadesAllSentence_'+str(BATCH), combinacaoEntidadesAll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# para suprimir output\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel1_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "pred_region_labels2 = list()\n",
    "for a in pred_region_labels:\n",
    "    for b in a:\n",
    "        pred_region_labels2.append(labels[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Problema', 'Tratamento', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'O']\n",
      "['Problema', 'Tratamento', 'Problema', 'Problema', 'Tratamento', 'Tratamento', 'Tratamento', 'Tratamento', 'Problema', 'Tratamento']\n",
      "564\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "print(pred_region_labels2[:10])\n",
    "print(region_true_list[:10])\n",
    "print(len(pred_region_labels2))\n",
    "print(len(region_true_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0],\n",
       "  ['h谩', 1],\n",
       "  ['15', 2],\n",
       "  ['anos', 3],\n",
       "  ['em', 4],\n",
       "  ['uso', 5],\n",
       "  ['de', 6],\n",
       "  ['losartana', 7],\n",
       "  ['50mg', 8],\n",
       "  ['/', 9],\n",
       "  ['dia', 10],\n",
       "  ['e', 11],\n",
       "  ['digoxina', 12],\n",
       "  ['1', 13],\n",
       "  ['/', 14],\n",
       "  ['2', 15],\n",
       "  ['cp', 16],\n",
       "  ['/', 17],\n",
       "  ['dia', 18],\n",
       "  [',', 19],\n",
       "  ['carvedilol', 20],\n",
       "  ['25', 21],\n",
       "  ['12', 22],\n",
       "  ['/', 23],\n",
       "  ['12', 24],\n",
       "  [',', 25],\n",
       "  ['HCTZ', 26],\n",
       "  ['.', 27]],\n",
       " [['HAS', [0], 'Problema'],\n",
       "  ['losartana 50mg', [7, 8], 'Tratamento'],\n",
       "  ['digoxina', [12], 'Tratamento'],\n",
       "  ['carvedilol 25', [20, 21], 'Tratamento'],\n",
       "  ['HCTZ', [26], 'Tratamento']]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina', 'Otimizo dose da sinvastatina para 40mg / dia .', [3], 0, 0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[4]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            if tipo_previsto!=0:\n",
    "                entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade est谩 na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Regi茫o):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Regi茫o):-----\n",
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Regi茫o):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro + downsampling (Regi茫o):-----\n",
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O\n",
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro + downsampling (Regi茫o):-----')\n",
    "\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo com filtro (Regi茫o):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df462b3fed964f1395551a965d628a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c631420dc345c384995d2f4292b591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd449083b554e778e5be575c8fd43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ecebf97cad44a39e22d0bc0c352897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6f1a1bba394feb855337afd28b9a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "print('-----Avaliando modelo com filtro (Regi茫o):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_filtro', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "#f.save_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 2],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dose da sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3, 4, 5],\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " ['sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3],\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " ['dose', 'Otimizo dose da sinvastatina para 40mg / dia .', [1], 0, 0, 0],\n",
       " ['dose da sinvastatina',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  2,\n",
       "  2],\n",
       " ['da', 'Otimizo dose da sinvastatina para 40mg / dia .', [2], 0, 0, 0],\n",
       " ['para', 'Otimizo dose da sinvastatina para 40mg / dia .', [4], 0, 0, 0],\n",
       " ['sinvastatina para 40mg',\n",
       "  'Otimizo dose da sinvastatina para 40mg / dia .',\n",
       "  [3, 4, 5],\n",
       "  0,\n",
       "  2,\n",
       "  2]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDicPredictionsSentence2(combinacaoEntidadesAll_pred, dic_predictions):\n",
    "    labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    num=-1\n",
    "    dic_predictions_sentence={}\n",
    "    entidades=list()\n",
    "    numAcrescentou=0\n",
    "    for frases in combinacaoEntidadesAll_pred:\n",
    "        num=num+1\n",
    "        for valor in frases:\n",
    "            #print('num:', num)\n",
    "            #print('valor:', valor)\n",
    "            #print('valor[1]:', valor[1])\n",
    "            tokens_entidade = valor[0]\n",
    "            #frase = valor[1]\n",
    "            indices = valor[2]\n",
    "            tipo_previsto=valor[5]\n",
    "            #print('tokens_entidade:', tokens_entidade)\n",
    "            #print('frase:', frase)\n",
    "            #print('indices:', indices)\n",
    "            #print('tipo_previsto:', tipo_previsto)\n",
    "            entidades.append([tokens_entidade, indices, labels[tipo_previsto]])\n",
    "            # ver se entidade est谩 na dic_prediction\n",
    "        frase = dic_predictions[num].copy()[0]\n",
    "        dic_predictions_sentence[num] = [frase, entidades]\n",
    "        entidades = list()\n",
    "                     \n",
    "    return dic_predictions_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "# om 30 epocas\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Otimizo', 0],\n",
       "  ['dose', 1],\n",
       "  ['da', 2],\n",
       "  ['sinvastatina', 3],\n",
       "  ['para', 4],\n",
       "  ['40mg', 5],\n",
       "  ['/', 6],\n",
       "  ['dia', 7],\n",
       "  ['.', 8]],\n",
       " [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'],\n",
       "  ['dose da sinvastatina', [1, 2, 3], 'Tratamento'],\n",
       "  ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel2_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "dic_predictions_sentence[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 49\n",
      "numErro2: 140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.000000  0.000000  0.000000       140\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.568282       454\n",
      "   macro avg   0.549515  0.640449  0.578774       454\n",
      "weighted avg   0.457676  0.568282  0.497704       454\n",
      "\n",
      "[[31 17  4  2  0]\n",
      " [ 6  0 81 24 29]\n",
      " [ 0 17 96  0  0]\n",
      " [ 1  7  0 75  0]\n",
      " [ 0  8  0  0 56]]\n"
     ]
    }
   ],
   "source": [
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.815789  0.574074  0.673913        54\n",
      "           O   0.747423  0.508772  0.605428       285\n",
      "    Problema   0.530387  0.849558  0.653061       113\n",
      "       Teste   0.742574  0.903614  0.815217        83\n",
      "  Tratamento   0.658824  0.875000  0.751678        64\n",
      "\n",
      "    accuracy                       0.672788       599\n",
      "   macro avg   0.698999  0.742204  0.699859       599\n",
      "weighted avg   0.702504  0.672788  0.665283       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD\n",
    "dic_predictions_sentence = getDicPredictionsSentence2(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31  17   4   2   0]\n",
      " [  6 145  81  24  29]\n",
      " [  0  17  96   0   0]\n",
      " [  1   7   0  75   0]\n",
      " [  0   8   0   0  56]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'O']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_true_list[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Problema',\n",
       " 'Tratamento',\n",
       " 'O',\n",
       " 'Problema',\n",
       " 'Problema',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento',\n",
       " 'Tratamento']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_pred_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0, 269],\n",
       "  ['refere', 1, 278],\n",
       "  ['fraqueza', 2, 285],\n",
       "  ['intermitente', 3, 294],\n",
       "  ['em', 4, 307],\n",
       "  ['MMII', 5, 310],\n",
       "  [',', 6, 314],\n",
       "  ['associada', 7, 316],\n",
       "  ['a', 8, 324],\n",
       "  ['tontura', 9, 328],\n",
       "  [',', 10, 335],\n",
       "  ['turva莽茫o', 11, 337],\n",
       "  ['visual', 12, 346],\n",
       "  ['e', 13, 353],\n",
       "  ['epigastralgia', 14, 355],\n",
       "  ['.', 15, 368]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turva莽茫o visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicSentences_new_test[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turva莽茫o', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turva莽茫o visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turva莽茫o', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turva莽茫o visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia'],\n",
       "  ['fraqueza intermitente', [2, 3], 'Problema'],\n",
       "  ['intermitente em MMII', [3, 4, 5], 'Problema']]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Avaliando modelo sem filtro (Regi茫o):-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102499d488da4d2b9b3bf9b3ba83b669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0291985584e48aa5ff3f341718027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d2bf402f9d49249b138aa89354960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b15b3ff136468c96cb99f1187c19e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8843ea20e91b4a72b245324511bd7fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681a40ef2be4d2f97dbf0f4d4919c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamando predict\n"
     ]
    }
   ],
   "source": [
    "# avaliar por regiao.. \n",
    "# AKI rodar de novo\n",
    "combinacaoEntidadesAll = f.getCombinacaoEntidadesSentence(dic_predictions, False, '', 0, '')\n",
    "print('-----Avaliando modelo sem filtro (Regi茫o):-----')\n",
    "# para suprimir output\n",
    "model = ClassificationModel('bert', 'lisaterumi/sentence_pairs_nested_all', use_cuda=False)\n",
    "#model = ClassificationModel('bert', r'C:\\Users\\lisat\\Downloads\\sentece-sem-filtro', use_cuda=False)\n",
    "#combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAll_'+str(BATCH))\n",
    "print('Chamando predict')\n",
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentence_'+str(BATCH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pred_region_labels = list()\n",
    "for key, combinacao in enumerate(combinacaoEntidadesAll):\n",
    "    if key<BATCH:    \n",
    "        if len(combinacao)>0:\n",
    "            lista = [l[0:2] for l in combinacao]\n",
    "            predictions, _ = model.predict(lista) \n",
    "            pred_region_labels.append(predictions)\n",
    "            for comb, label in zip(combinacao, predictions):\n",
    "                comb.append(label)\n",
    "f.save_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH), combinacaoEntidadesAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Otimizo', 0], ['dose', 1], ['da', 2], ['sinvastatina', 3], ['para', 4], ['40mg', 5], ['/', 6], ['dia', 7], ['.', 8]], [['dose da sinvastatina para 40mg', [1, 2, 3, 4, 5], 'Tratamento'], ['sinvastatina para 40mg', [3, 4, 5], 'Tratamento']]]\n",
      "numErro1: 50\n",
      "numErro2: 132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.000000  0.000000  0.000000       132\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.578475       446\n",
      "   macro avg   0.557689  0.657336  0.597277       446\n",
      "weighted avg   0.474937  0.578475  0.516752       446\n",
      "\n",
      "[[38 12  4  0  0]\n",
      " [10  0 77 12 33]\n",
      " [ 0 26 86  1  0]\n",
      " [ 0  7  0 76  0]\n",
      " [ 0  5  0  1 58]]\n"
     ]
    }
   ],
   "source": [
    "combinacaoEntidadesAll = f.load_obj('combinacaoEntidadesAllSentenceComLabel3_'+str(BATCH))\n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "print(dic_predictions_sentence[20])\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 35\n",
      "numErro2: 285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.791667  0.703704  0.745098        54\n",
      "           O   0.753695  0.536842  0.627049       285\n",
      "    Problema   0.514970  0.761062  0.614286       113\n",
      "       Teste   0.844444  0.915663  0.878613        83\n",
      "  Tratamento   0.637363  0.906250  0.748387        64\n",
      "\n",
      "    accuracy                       0.686144       599\n",
      "   macro avg   0.708428  0.764704  0.722687       599\n",
      "weighted avg   0.712228  0.686144  0.683106       599\n",
      "\n",
      "[[ 38  12   4   0   0]\n",
      " [ 10 153  77  12  33]\n",
      " [  0  26  86   1   0]\n",
      " [  0   7   0  76   0]\n",
      " [  0   5   0   1  58]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO - OLD, com O \n",
    "dic_predictions_sentence = getDicPredictionsSentence(combinacaoEntidadesAll, dic_predictions)\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_sentence, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, merge com dic_predictions?\n",
    "\n",
    "Acho que nao precisa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, com pos processamento\n",
    "\n",
    "Regras:\n",
    "\n",
    "- Pega os de dentro (aninhadas), apenas se o tipo for diferente do tipo da entidade de fora... se for do mesmo tipo, n茫o pega..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isEntidadeAninhada(a, b):\n",
    "    inicio1=a[0]\n",
    "    fim1=a[-1]\n",
    "    #print(inicio1, fim1)\n",
    "    inicio2=b[0]\n",
    "    fim2=b[-1]\n",
    "    #print(inicio2, fim2)\n",
    "    if inicio1 >=inicio2 and fim1 <=fim2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "isEntidadeAninhada([2], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento\n",
      "numErro1: 65\n",
      "numErro2: 43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.822222  0.685185  0.747475        54\n",
      "           O   0.000000  0.000000  0.000000        43\n",
      "    Problema   0.838384  0.734513  0.783019       113\n",
      "       Teste   0.860465  0.891566  0.875740        83\n",
      "  Tratamento   0.838710  0.812500  0.825397        64\n",
      "\n",
      "    accuracy                       0.689076       357\n",
      "   macro avg   0.671956  0.624753  0.646326       357\n",
      "weighted avg   0.740150  0.689076  0.712483       357\n",
      "\n",
      "[[37 16  1  0  0]\n",
      " [ 8  0 15 10 10]\n",
      " [ 0 29 83  1  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  1 52]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento')\n",
    "\n",
    "def posProcessamento(dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions_sentence.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = list()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeNer in listaEntidadesSpan:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            #print('entidadeSpan:', entidadeSpan)\n",
    "            indices = entidadeSpan[1]\n",
    "            #print('incluindo:', entidadeSpan)\n",
    "            # s贸 acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "            # ver pelo indice\n",
    "            isAninhada=False\n",
    "            isMesmoTipo=False\n",
    "            tipo=entidadeSpan[2]\n",
    "            for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                isAninhada=False\n",
    "                if indices!=indicesNer:\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # 茅 aninhada com alguma entidade?\n",
    "                if isAninhada and tipoNer == tipo:\n",
    "                    isMesmoTipo=True\n",
    "                    #print('-----mesmo tipo!!')\n",
    "                    #print('entidadeSpan:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    #print('indices:', indices)\n",
    "                    #print('indicesNer:', indicesNer)\n",
    "                    #print('indices!=indicesNer:', indices!=indicesNer)\n",
    "                    break\n",
    "            # ver pelo tipo tb\n",
    "            if not (isAninhada and isMesmoTipo):\n",
    "                #print('N茫o 茅 aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                listaEntidadesNer.append(entidadeSpan)\n",
    "            else:\n",
    "                #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                #print('tipoNer:', tipoNer)\n",
    "                #print('tipo:', tipo)\n",
    "                pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos processamento + dic_predictions (primeiro nivel -> ner)\n",
      "numErro1: 53\n",
      "numErro2: 46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.826087  0.703704  0.760000        54\n",
      "           O   0.000000  0.000000  0.000000        46\n",
      "    Problema   0.830357  0.823009  0.826667       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.716667       360\n",
      "   macro avg   0.671031  0.649281  0.659149       360\n",
      "weighted avg   0.732494  0.716667  0.723812       360\n",
      "\n",
      "[[38 13  1  1  1]\n",
      " [ 8  0 18 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "print('pos processamento + dic_predictions (primeiro nivel -> ner)')\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # s贸 acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # 茅 aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('N茫o 茅 aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = posProcessamento(dic_predictions_sentence)\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_all)\n",
    "#dic_predictions_all\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juntando sentence + ner, com pos processamento\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# juntar all_predictions\n",
    "print('juntando sentence + ner, com pos processamento')\n",
    "#dic_predictions = f.load_obj('dic_predictions_results_ner_'+str(BATCH))\n",
    "#dic_predictions_all = f.getDicPredictionsAll(combinacaoEntidadesAll_pred, dic_predictions)\n",
    "\n",
    "def juntaPredictons(dic_predictions, dic_predictions_sentence):\n",
    "    dic_predictions_all = {}\n",
    "    for key, value in dic_predictions.items():\n",
    "        #dic_predictions_all[key] = value.copy()\n",
    "        tokens = value[0].copy()\n",
    "        listaEntidadesNer = value[1].copy()\n",
    "        listaIndicesNer=list()\n",
    "        listaTipoNer=list()\n",
    "        for entidadeNer in listaEntidadesNer:\n",
    "            listaIndicesNer.append(entidadeNer[1])\n",
    "            listaTipoNer.append(entidadeNer[2])\n",
    "        #print('listaIndicesNer:', listaIndicesNer)\n",
    "        listaEntidadesSpan = dic_predictions_sentence[key][1]\n",
    "        for entidadeSpan in listaEntidadesSpan:\n",
    "            indices = entidadeSpan[1]\n",
    "            if indices in listaIndicesNer:\n",
    "                #print('ja tem, pulando')\n",
    "                pass\n",
    "            else:\n",
    "                #print('incluindo:', entidadeSpan)\n",
    "                # s贸 acrescenta se nao tem entidade mais externa do mesmo tipo\n",
    "                # ver pelo indice\n",
    "                isAninhada=False\n",
    "                isMesmoTipo=False\n",
    "                tipo=entidadeSpan[2]\n",
    "                for indicesNer, tipoNer in zip(listaIndicesNer, listaTipoNer):\n",
    "                    isAninhada = isEntidadeAninhada(indices, indicesNer) # 茅 aninhada com alguma entidade?\n",
    "                    if isAninhada and tipoNer == tipo:\n",
    "                        isMesmoTipo=True\n",
    "                        #print('-----mesmo tipo!!')\n",
    "                        #print('entidadeSpan:', entidadeSpan)\n",
    "                        #print('tipoNer:', tipoNer)\n",
    "                        #print('tipo:', tipo)\n",
    "                        break\n",
    "                # ver pelo tipo tb\n",
    "                if not (isAninhada and isMesmoTipo):\n",
    "                    #print('N茫o 茅 aninhada ou tipos sao diferentes, incluindo:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    listaEntidadesNer.append(entidadeSpan)\n",
    "                else:\n",
    "                    #print('aninhada com mesmo tipo, nao entra:', entidadeSpan)\n",
    "                    #print('tipoNer:', tipoNer)\n",
    "                    #print('tipo:', tipo)\n",
    "                    pass               \n",
    "        dic_predictions_all[key] = [tokens, listaEntidadesNer]\n",
    "        #if key>15:\n",
    "        #    break\n",
    "    return dic_predictions_all\n",
    "\n",
    "dic_predictions_all = juntaPredictons(dic_predictions, dic_predictions_sentence)\n",
    "#dic_predictions_all\n",
    "#region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "#print(confusion_matrix(region_true_list, region_pred_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turva莽茫o', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turva莽茫o visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema'],\n",
       "  ['MMII', [5], 'Anatomia']]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numErro1: 66\n",
      "numErro2: 130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Anatomia   0.766667  0.425926  0.547619        54\n",
      "           O   0.561290  0.669231  0.610526       130\n",
      "    Problema   0.845455  0.823009  0.834081       113\n",
      "       Teste   0.870588  0.891566  0.880952        83\n",
      "  Tratamento   0.828125  0.828125  0.828125        64\n",
      "\n",
      "    accuracy                       0.743243       444\n",
      "   macro avg   0.774425  0.727571  0.740261       444\n",
      "weighted avg   0.754871  0.743243  0.741689       444\n",
      "\n",
      "[[23 28  1  1  1]\n",
      " [ 7 87 16 10 10]\n",
      " [ 0 20 93  0  0]\n",
      " [ 0  9  0 74  0]\n",
      " [ 0 11  0  0 53]]\n"
     ]
    }
   ],
   "source": [
    "# ERRO, OLD com O\n",
    "region_true_list, region_pred_list = f.AvalFinal(dicSentences_new_test, dic_predictions_all, BATCH)\n",
    "print(confusion_matrix(region_true_list, region_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbumin煤ria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['altera莽玫es', 26],\n",
       "  ['tr贸ficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepic谩rdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['altera莽茫o', 41],\n",
       "  ['de', 42],\n",
       "  ['repolariza莽茫o', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['n茫o', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['aus锚ncia', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diast贸lica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medica莽茫o', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbumin煤ria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['altera莽玫es tr贸ficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepic谩rdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['altera莽茫o de repolariza莽茫o ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['RA', [87], 'Problema'],\n",
       "  ['sopros', [94], 'Problema'],\n",
       "  ['hipofonese de bulhas', [97, 98, 99], 'Problema'],\n",
       "  ['abdome', [102], 'Anatomia'],\n",
       "  ['massas', [111], 'Problema'],\n",
       "  ['visceromegalias', [113], 'Problema'],\n",
       "  ['MMII', [114], 'Anatomia'],\n",
       "  ['edema', [116], 'Problema'],\n",
       "  ['ICC diast贸lica classe III', [120, 121, 122, 123], 'Problema'],\n",
       "  ['HAS', [125], 'Problema'],\n",
       "  ['DMII', [127], 'Problema'],\n",
       "  ['Dislipidemia', [129], 'Problema'],\n",
       "  ['medica莽茫o', [134], 'Tratamento'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_all[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Rsultado', 0],\n",
       "  ['de', 1],\n",
       "  ['exames', 2],\n",
       "  [':', 3],\n",
       "  ['1', 4],\n",
       "  ['-', 5],\n",
       "  ['Microalbumin煤ria', 6],\n",
       "  ['12', 7],\n",
       "  ['/', 8],\n",
       "  ['04', 9],\n",
       "  [':', 10],\n",
       "  ['10', 11],\n",
       "  ['.', 12],\n",
       "  ['64', 13],\n",
       "  ['/', 14],\n",
       "  ['G', 15],\n",
       "  ['de', 16],\n",
       "  ['creatinina', 17],\n",
       "  ['2', 18],\n",
       "  ['-', 19],\n",
       "  ['US', 20],\n",
       "  ['abdome', 21],\n",
       "  ['total', 22],\n",
       "  [':', 23],\n",
       "  ['rins', 24],\n",
       "  ['com', 25],\n",
       "  ['altera莽玫es', 26],\n",
       "  ['tr贸ficas', 27],\n",
       "  ['3', 28],\n",
       "  ['-', 29],\n",
       "  ['ECG', 30],\n",
       "  ['19', 31],\n",
       "  ['/', 32],\n",
       "  ['03', 33],\n",
       "  ['/', 34],\n",
       "  ['14', 35],\n",
       "  [':', 36],\n",
       "  ['isquemia', 37],\n",
       "  ['subepic谩rdica', 38],\n",
       "  ['anterior', 39],\n",
       "  [',', 40],\n",
       "  ['altera莽茫o', 41],\n",
       "  ['de', 42],\n",
       "  ['repolariza莽茫o', 43],\n",
       "  ['ventricular', 44],\n",
       "  ['infero', 45],\n",
       "  ['-', 46],\n",
       "  ['lateral', 47],\n",
       "  ['4', 48],\n",
       "  ['-', 49],\n",
       "  ['Laboratoriais', 50],\n",
       "  ['12', 51],\n",
       "  ['/', 52],\n",
       "  ['04', 53],\n",
       "  ['/', 54],\n",
       "  ['14', 55],\n",
       "  [':', 56],\n",
       "  ['K', 57],\n",
       "  ['5', 58],\n",
       "  ['.', 59],\n",
       "  ['1', 60],\n",
       "  [',', 61],\n",
       "  ['clearence', 62],\n",
       "  ['creatinina', 63],\n",
       "  ['57', 64],\n",
       "  ['Ao', 65],\n",
       "  ['EF', 66],\n",
       "  [':', 67],\n",
       "  ['PA', 68],\n",
       "  ['170x100mmHg', 69],\n",
       "  ['(', 70],\n",
       "  ['relata', 71],\n",
       "  ['PA', 72],\n",
       "  ['normal', 73],\n",
       "  ['quando', 74],\n",
       "  ['em', 75],\n",
       "  ['ambiente', 76],\n",
       "  ['n茫o', 77],\n",
       "  ['hospitalar', 78],\n",
       "  [')', 79],\n",
       "  [',', 80],\n",
       "  ['FC', 81],\n",
       "  ['63bpm', 82],\n",
       "  ['AR', 83],\n",
       "  [':', 84],\n",
       "  ['mv', 85],\n",
       "  ['sem', 86],\n",
       "  ['RA', 87],\n",
       "  ['ACV', 88],\n",
       "  [':', 89],\n",
       "  ['rcr', 90],\n",
       "  ['em', 91],\n",
       "  ['2T', 92],\n",
       "  ['sem', 93],\n",
       "  ['sopros', 94],\n",
       "  [',', 95],\n",
       "  ['com', 96],\n",
       "  ['hipofonese', 97],\n",
       "  ['de', 98],\n",
       "  ['bulhas', 99],\n",
       "  ['AD', 100],\n",
       "  [':', 101],\n",
       "  ['abdome', 102],\n",
       "  ['globos', 103],\n",
       "  [',', 104],\n",
       "  ['normotenso', 105],\n",
       "  [',', 106],\n",
       "  ['indolor', 107],\n",
       "  [',', 108],\n",
       "  ['aus锚ncia', 109],\n",
       "  ['de', 110],\n",
       "  ['massas', 111],\n",
       "  ['ou', 112],\n",
       "  ['visceromegalias', 113],\n",
       "  ['MMII', 114],\n",
       "  ['sem', 115],\n",
       "  ['edema', 116],\n",
       "  ['HD', 117],\n",
       "  [':', 118],\n",
       "  ['-', 119],\n",
       "  ['ICC', 120],\n",
       "  ['diast贸lica', 121],\n",
       "  ['classe', 122],\n",
       "  ['III', 123],\n",
       "  ['-', 124],\n",
       "  ['HAS', 125],\n",
       "  ['-', 126],\n",
       "  ['DMII', 127],\n",
       "  ['-', 128],\n",
       "  ['Dislipidemia', 129],\n",
       "  ['CD', 130],\n",
       "  [':', 131],\n",
       "  ['-', 132],\n",
       "  ['mantenho', 133],\n",
       "  ['medica莽茫o', 134],\n",
       "  ['-', 135],\n",
       "  ['retorno', 136],\n",
       "  ['em', 137],\n",
       "  ['6', 138],\n",
       "  ['m', 139],\n",
       "  ['com', 140],\n",
       "  ['exames', 141],\n",
       "  ['laboratoriais', 142],\n",
       "  ['+', 143],\n",
       "  ['ECG', 144],\n",
       "  ['.', 145]],\n",
       " [['exames', [2], 'Teste'],\n",
       "  ['Microalbumin煤ria', [6], 'Teste'],\n",
       "  ['creatinina', [17], 'Teste'],\n",
       "  ['US abdome total', [20, 21, 22], 'Teste'],\n",
       "  ['altera莽玫es tr贸ficas', [26, 27], 'Problema'],\n",
       "  ['ECG', [30], 'Teste'],\n",
       "  ['isquemia subepic谩rdica anterior', [37, 38, 39], 'Problema'],\n",
       "  ['altera莽茫o de repolariza莽茫o ventricular infero - lateral',\n",
       "   [41, 42, 43, 44, 45, 46, 47],\n",
       "   'Problema'],\n",
       "  ['Laboratoriais', [50], 'Teste'],\n",
       "  ['K', [57], 'Teste'],\n",
       "  ['clearence creatinina', [62, 63], 'Teste'],\n",
       "  ['EF', [66], 'Teste'],\n",
       "  ['PA', [68], 'Teste'],\n",
       "  ['PA', [72], 'Teste'],\n",
       "  ['FC', [81], 'Teste'],\n",
       "  ['sopros', [94], 'Teste'],\n",
       "  ['medica莽茫o', [134], 'Teste'],\n",
       "  ['exames laboratoriais', [141, 142], 'Teste'],\n",
       "  ['ECG', [144], 'Teste'],\n",
       "  ['laboratoriais', [142], 'Teste'],\n",
       "  ['tr贸ficas', [27], 'Problema'],\n",
       "  ['ventricular', [44], 'Teste']]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions_sentence[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Paciente', 0],\n",
       "  ['refere', 1],\n",
       "  ['fraqueza', 2],\n",
       "  ['intermitente', 3],\n",
       "  ['em', 4],\n",
       "  ['MMII', 5],\n",
       "  [',', 6],\n",
       "  ['associada', 7],\n",
       "  ['a', 8],\n",
       "  ['tontura', 9],\n",
       "  [',', 10],\n",
       "  ['turva莽茫o', 11],\n",
       "  ['visual', 12],\n",
       "  ['e', 13],\n",
       "  ['epigastralgia', 14],\n",
       "  ['.', 15]],\n",
       " [['fraqueza intermitente em MMII', [2, 3, 4, 5], 'Problema'],\n",
       "  ['tontura', [9], 'Problema'],\n",
       "  ['turva莽茫o visual', [11, 12], 'Problema'],\n",
       "  ['epigastralgia', [14], 'Problema']]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_predictions[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aumento moderado de 谩trio esquerdo',\n",
       "  'aumento moderado de 谩trio esquerdo .',\n",
       "  [0, 1, 2, 3, 4],\n",
       "  1,\n",
       "  1],\n",
       " ['de', 'aumento moderado de 谩trio esquerdo .', [2], 0, 0],\n",
       " ['谩trio esquerdo', 'aumento moderado de 谩trio esquerdo .', [3, 4], 0, 4],\n",
       " ['moderado de 谩trio',\n",
       "  'aumento moderado de 谩trio esquerdo .',\n",
       "  [1, 2, 3],\n",
       "  0,\n",
       "  0],\n",
       " ['esquerdo', 'aumento moderado de 谩trio esquerdo .', [4], 0, 4],\n",
       " ['moderado', 'aumento moderado de 谩trio esquerdo .', [1], 0, 0],\n",
       " ['谩trio', 'aumento moderado de 谩trio esquerdo .', [3], 0, 0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesAll[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
